{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example making grid search for Tab Net\n",
    "\n",
    "Author: Hartorn\n",
    "\n",
    "File from [github.com](https://github.com/dreamquark-ai/tabnet/issues/80#issuecomment-623954898).\n",
    "\n",
    "> Note: In this notebook, I am studying the article mentioned above. Some changes may have been made to the code during its implementation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests black nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force:\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_VALUE = [\"Unkn0wnV@lue\"]\n",
    "\n",
    "\n",
    "class SafeLabelEncoder(LabelEncoder):\n",
    "    \"\"\"\n",
    "    Safe label encoder, encoding every unknown value as Unkn0wnV@lue.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Fit the label encoder, by casting the numpy array as a string, then adding the code for unknown.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        SafeLabelEncoder\n",
    "            itself, fitted\n",
    "        \"\"\"\n",
    "        return super().fit(np.concatenate((y.astype(\"str\"), UNKNOWN_VALUE)))\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        \"\"\"\n",
    "        Fit the encoder, then transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        self.fit(y)\n",
    "        return super().transform(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        \"\"\"\n",
    "        Transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        return super().transform(\n",
    "            np.where(\n",
    "                np.isin(y.astype(\"str\"), self.classes_), y.astype(\"str\"), UNKNOWN_VALUE\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n",
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "dataset_name = \"census-income\"\n",
    "out = Path(os.getcwd() + \"/datasets/\" + dataset_name + \".csv\")\n",
    "out_test = Path(os.getcwd() + \"/datasets/\" + dataset_name + \"_test.csv\")\n",
    "\n",
    "download(url, out, force=False)\n",
    "download(url_test, out_test, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"target\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(out, names=cols)\n",
    "test = pd.read_csv(out_test, names=cols, skiprows=2)\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = train[target].str.strip()\n",
    "# Test has . in label, let's clean it\n",
    "test[target] = test[target].str.strip().str.strip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   age          workclass  fnlwgt   education  education-num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital-status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital-gain  capital-loss  hours-per-week  native-country target  \n0          2174             0              40   United-States  <=50K  \n1             0             0              13   United-States  <=50K  \n2             0             0              40   United-States  <=50K  \n3             0             0              40   United-States  <=50K  \n4             0             0              40            Cuba  <=50K  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  target          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n0   38     Private   89814        HS-grad              9   Married-civ-spouse   \n1   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n2   44     Private  160323   Some-college             10   Married-civ-spouse   \n3   18           ?  103497   Some-college             10        Never-married   \n4   34     Private  198693           10th              6        Never-married   \n\n           occupation    relationship    race      sex  capital-gain  \\\n0     Farming-fishing         Husband   White     Male             0   \n1     Protective-serv         Husband   White     Male             0   \n2   Machine-op-inspct         Husband   Black     Male          7688   \n3                   ?       Own-child   White   Female             0   \n4       Other-service   Not-in-family   White     Male             0   \n\n   capital-loss  hours-per-week  native-country target  \n0             0              50   United-States  <=50K  \n1             0              40   United-States   >50K  \n2             0              40   United-States   >50K  \n3             0              30   United-States  <=50K  \n4             0              30   United-States  <=50K  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>89814</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Farming-fishing</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28</td>\n      <td>Local-gov</td>\n      <td>336951</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Married-civ-spouse</td>\n      <td>Protective-serv</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>160323</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Married-civ-spouse</td>\n      <td>Machine-op-inspct</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>7688</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>?</td>\n      <td>103497</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Never-married</td>\n      <td>?</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34</td>\n      <td>Private</td>\n      <td>198693</td>\n      <td>10th</td>\n      <td>6</td>\n      <td>Never-married</td>\n      <td>Other-service</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16280 entries, 0 to 16279\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             16280 non-null  int64 \n",
      " 1   workclass       16280 non-null  object\n",
      " 2   fnlwgt          16280 non-null  int64 \n",
      " 3   education       16280 non-null  object\n",
      " 4   education-num   16280 non-null  int64 \n",
      " 5   marital-status  16280 non-null  object\n",
      " 6   occupation      16280 non-null  object\n",
      " 7   relationship    16280 non-null  object\n",
      " 8   race            16280 non-null  object\n",
      " 9   sex             16280 non-null  object\n",
      " 10  capital-gain    16280 non-null  int64 \n",
      " 11  capital-loss    16280 non-null  int64 \n",
      " 12  hours-per-week  16280 non-null  int64 \n",
      " 13  native-country  16280 non-null  object\n",
      " 14  target          16280 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice(\n",
    "        [\"train\", \"valid\"], p=[0.8, 0.2], size=(train.shape[0],)\n",
    "    )\n",
    "\n",
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['capital-gain',\n 'native-country',\n 'capital-loss',\n 'relationship',\n 'education-num',\n 'hours-per-week',\n 'education',\n 'age',\n 'sex',\n 'workclass',\n 'fnlwgt',\n 'race',\n 'occupation',\n 'marital-status']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_columns = list(set(train.columns.tolist()) - set([target]) - set([\"Set\"]))\n",
    "used_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "Label encode categorical features and fill empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['capital-gain', 'native-country', 'capital-loss', 'relationship',\n",
      "       'education-num', 'hours-per-week', 'education', 'age', 'sex',\n",
      "       'workclass', 'race', 'occupation', 'marital-status'],\n",
      "      dtype='object')\n",
      "Index(['fnlwgt'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nunique = train[used_columns].nunique()\n",
    "types = train[used_columns].dtypes\n",
    "\n",
    "cat_cols = train[used_columns].columns[(nunique < 200) | (types == \"object\")]\n",
    "other_cols = train[used_columns].columns[~train[used_columns].columns.isin(cat_cols)]\n",
    "print(cat_cols)\n",
    "print(other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nunique[\"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna\n",
    "train[cat_cols] = train[cat_cols].astype(\"str\")\n",
    "train[other_cols] = train[other_cols].fillna(train[other_cols].mean())\n",
    "\n",
    "test[cat_cols] = test[cat_cols].astype(\"str\")\n",
    "test[other_cols] = test[other_cols].fillna(train[other_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  object\n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  object\n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  object\n",
      " 11  capital-loss    32561 non-null  object\n",
      " 12  hours-per-week  32561 non-null  object\n",
      " 13  native-country  32561 non-null  object\n",
      " 14  target          32561 non-null  object\n",
      " 15  Set             32561 non-null  object\n",
      "dtypes: int64(1), object(15)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16280 entries, 0 to 16279\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             16280 non-null  object\n",
      " 1   workclass       16280 non-null  object\n",
      " 2   fnlwgt          16280 non-null  int64 \n",
      " 3   education       16280 non-null  object\n",
      " 4   education-num   16280 non-null  object\n",
      " 5   marital-status  16280 non-null  object\n",
      " 6   occupation      16280 non-null  object\n",
      " 7   relationship    16280 non-null  object\n",
      " 8   race            16280 non-null  object\n",
      " 9   sex             16280 non-null  object\n",
      " 10  capital-gain    16280 non-null  object\n",
      " 11  capital-loss    16280 non-null  object\n",
      " 12  hours-per-week  16280 non-null  object\n",
      " 13  native-country  16280 non-null  object\n",
      " 14  target          16280 non-null  object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'capital-gain': SafeLabelEncoder(),\n 'native-country': SafeLabelEncoder(),\n 'capital-loss': SafeLabelEncoder(),\n 'relationship': SafeLabelEncoder(),\n 'education-num': SafeLabelEncoder(),\n 'hours-per-week': SafeLabelEncoder(),\n 'education': SafeLabelEncoder(),\n 'age': SafeLabelEncoder(),\n 'sex': SafeLabelEncoder(),\n 'workclass': SafeLabelEncoder(),\n 'race': SafeLabelEncoder(),\n 'occupation': SafeLabelEncoder(),\n 'marital-status': SafeLabelEncoder(),\n 'target': SafeLabelEncoder()}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = {}\n",
    "for col in cat_cols:\n",
    "    label_enc = SafeLabelEncoder()\n",
    "    enc[col] = label_enc\n",
    "    train[col] = label_enc.fit_transform(train[col])\n",
    "    test[col] = label_enc.transform(test[col])\n",
    "enc[target] = SafeLabelEncoder()\n",
    "train[target] = enc[target].fit_transform(train[target])\n",
    "test[target] = enc[target].transform(test[target])\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13]\n",
      "[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "unused_feat = [\"Set\"]\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(used_columns) if f in cat_cols]\n",
    "cat_dims = [len(enc[f].classes_) for f in used_columns if f in cat_cols]\n",
    "print(cat_idxs)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[used_columns].values\n",
    "y = train[target].values\n",
    "\n",
    "X_test = test[used_columns].values\n",
    "y_test = test[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('int64')"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('int64')"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].dtype"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[:,0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X idx:0 - unique:(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118]), array([29849,    43,    25,     6,    12,     4,     1,     6,     8,\n",
      "           2,     3,    27,    41,     7,     3,    26,     1,     7,\n",
      "           5,   347,    15,     6,     1,     7,     7,     6,     2,\n",
      "          37,     3,     4,     5,     2,     9,    48,    23,    16,\n",
      "           1,     5,     5,     6,     6,    11,     1,    19,     8,\n",
      "          11,     4,    11,     1,    12,    20,    11,     5,    34,\n",
      "          31,    24,    11,     3,     3,     9,     8,     2,    97,\n",
      "          37,     6,    53,     5,    24,     5,     4,     2,    23,\n",
      "           8,    14,    12,     7,     6,    32,    14,     2,    42,\n",
      "          20,     2,    70,    12,    12,    41,     3,    23,    17,\n",
      "           1,     7,    69,     1,    97,    11,     5,     3,    34,\n",
      "           1,     3,     9,    11,     5,     2,     5,    27,   246,\n",
      "           9,     5,   284,     3,     1,    55,     8,    22,     4,\n",
      "           5,   159]))\n",
      "\n",
      "\n",
      "X idx:1 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41]), array([  583,    19,   121,    75,    59,    95,    70,    28,   106,\n",
      "          90,    29,   137,    29,    64,    44,     1,    13,    20,\n",
      "          13,   100,    43,    24,    73,    81,    62,    18,   643,\n",
      "          34,    14,    31,   198,    60,    37,   114,    12,    80,\n",
      "          51,    18,    19, 29170,    67,    16]))\n",
      "\n",
      "\n",
      "X idx:2 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91]), array([31042,     7,     2,     4,     7,     7,    21,     1,    51,\n",
      "          18,     1,     1,    25,     6,    20,    40,     8,    47,\n",
      "           9,    15,     2,     9,     4,    24,    34,    22,    18,\n",
      "           4,     2,    42,    24,     2,    14,     2,     4,     1,\n",
      "          51,    39,   159,   202,     1,    18,   168,    23,    24,\n",
      "          21,     9,    21,     6,     1,     3,     4,     2,     1,\n",
      "           7,    15,     1,     9,     6,     3,     2,     6,    25,\n",
      "           3,     1,    17,     2,    20,     9,    49,    12,     3,\n",
      "           1,     1,     1,     4,    12,     5,     2,    10,     2,\n",
      "           3,     2,     2,     2,     3,     3,    12,     3,     2,\n",
      "           6,     2]))\n",
      "\n",
      "\n",
      "X idx:3 - unique:(array([0, 1, 2, 3, 4, 5]), array([13193,  8305,   981,  5068,  3446,  1568]))\n",
      "\n",
      "\n",
      "X idx:4 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([   51,  7291,  1382,  1067,  5355,  1723,   576,   413,   168,\n",
      "         333,   646,   514,   933,  1175,   433, 10501]))\n",
      "\n",
      "\n",
      "X idx:5 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91, 92, 93]), array([   20,   278,    11,   173,    23,    34,   404,   205,    29,\n",
      "          75,    14,    32,  1224,    24,    44,    21,   252,   674,\n",
      "          30,    30,    86,     7,    39,  1149,     5,   266,    39,\n",
      "          28,  1297,   220,   149,   476,    38,    54, 15217,    36,\n",
      "         219,   151,   212,  1824,    82,    49,   517,    29,    60,\n",
      "        2819,    13,   138,    25,    41,   694,    97,    17,    28,\n",
      "           5,    64,  1475,     2,    18,    10,    14,   244,    17,\n",
      "           4,    12,    26,   291,    71,     2,     1,    66,     3,\n",
      "           6,     8,   145,   133,     3,     1,    45,    13,     2,\n",
      "           1,     2,     2,    18,    29,     3,     1,     1,     2,\n",
      "           5,     2,    11,    85]))\n",
      "\n",
      "\n",
      "X idx:6 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([  933,  1175,   433,   168,   333,   646,   514,  1067,  1382,\n",
      "        5355,   413, 10501,  1723,    51,   576,  7291]))\n",
      "\n",
      "\n",
      "X idx:7 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72]), array([395, 550, 712, 753, 720, 765, 877, 798, 841, 785, 835, 867, 813,\n",
      "       861, 888, 828, 875, 886, 876, 898, 858, 827, 816, 794, 808, 780,\n",
      "       770, 724, 734, 737, 708, 543, 577, 602, 595, 478, 464, 415, 419,\n",
      "       366, 358, 366, 355, 312, 300, 258, 230, 208, 178, 150, 151, 120,\n",
      "       108,  89,  72,  67,  64,  51,  45,  46,  29,  23,  22,  22,  20,\n",
      "        12,   6,  10,   3,   1,   1,   3,  43]))\n",
      "\n",
      "\n",
      "X idx:8 - unique:(array([0, 1]), array([10771, 21790]))\n",
      "\n",
      "\n",
      "X idx:9 - unique:(array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([ 1836,   960,  2093,     7, 22696,  1116,  2541,  1298,    14]))\n",
      "\n",
      "\n",
      "X idx:11 - unique:(array([0, 1, 2, 3, 4]), array([  311,  1039,  3124,   271, 27816]))\n",
      "\n",
      "\n",
      "X idx:12 - unique:(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]), array([1843, 3770,    9, 4099, 4066,  994, 1370, 2002, 3295,  149, 4140,\n",
      "        649, 3650,  928, 1597]))\n",
      "\n",
      "\n",
      "X idx:13 - unique:(array([0, 1, 2, 3, 4, 5, 6]), array([ 4443,    23, 14976,   418, 10683,  1025,   993]))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idxs in cat_idxs:\n",
    "    print('X idx:{} - unique:{}\\n\\n'.format(idxs, np.unique(X[:,idxs], return_counts=True)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = os.cpu_count() if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetTuner(TabNetClassifier):\n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        # Dirty trick => would be better to add n_d in grid, or fix it in __init__ of tuner\n",
    "        self.n_d = self.n_a\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=0, shuffle=True, stratify=y\n",
    "        )\n",
    "        return super().fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            patience=20,\n",
    "            # X_valid=X_valid,\n",
    "            # y_valid=y_valid,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            num_workers=num_workers,\n",
    "            max_epochs=1000,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetTuner(cat_idxs=cat_idxs, cat_dims=cat_dims, device_name='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4],\n [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4],\n [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4],\n [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's generate embedding size based on cat dims\n",
    "cat_emb_dim_list = []\n",
    "for max_dim in [1, 5, 10, 20, 50]:\n",
    "    cat_emb_dim_list.append([min(nb // 2, max_dim) for nb in cat_dims])\n",
    "cat_emb_dim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"n_a\": [3, 5, 8, 13, 21],\n",
    "    # \"n_d\": [8], #\n",
    "    \"cat_emb_dim\": cat_emb_dim_list,\n",
    "    \"n_independent\": [0, 1, 2, 5],\n",
    "    \"n_shared\": [0, 1, 2],\n",
    "    \"n_steps\": [1, 3, 5, 8],\n",
    "    \"clip_value\": [1],\n",
    "    \"gamma\": [0.5, 1.3, 3],\n",
    "    \"momentum\": [0.1, 0.05, 0.02, 0.005],\n",
    "    \"lambda_sparse\": [0.1, 0.01, 0.001],\n",
    "    # \"lr\": [0.1, 0.02, 0.001],\n",
    "    \"optimizer_params\": [\n",
    "        {'lr': 0.01},\n",
    "        {'lr': 0.02},\n",
    "        {'lr': 0.001}],\n",
    "    \"verbose\": [1]\n",
    "    # optimizer_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    grid,\n",
    "    n_iter=90,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=1,\n",
    "    # iid=False,\n",
    "    refit=False,\n",
    "    cv=[(train_indices, valid_indices)],\n",
    "    verbose=1,\n",
    "    pre_dispatch=0,\n",
    "    random_state=0,\n",
    "    return_train_score=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[used_columns].values\n",
    "y = train[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(32561, 14)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 90 candidates, totalling 90 fits\n",
      "epoch 0  | loss: 0.59811 | val_0_auc: 0.70382 |  0:00:02s\n",
      "epoch 1  | loss: 0.43932 | val_0_auc: 0.78439 |  0:00:03s\n",
      "epoch 2  | loss: 0.38792 | val_0_auc: 0.82831 |  0:00:04s\n",
      "epoch 3  | loss: 0.35251 | val_0_auc: 0.84834 |  0:00:05s\n",
      "epoch 4  | loss: 0.33348 | val_0_auc: 0.86224 |  0:00:06s\n",
      "epoch 5  | loss: 0.31918 | val_0_auc: 0.86639 |  0:00:07s\n",
      "epoch 6  | loss: 0.30787 | val_0_auc: 0.87346 |  0:00:07s\n",
      "epoch 7  | loss: 0.29866 | val_0_auc: 0.87826 |  0:00:08s\n",
      "epoch 8  | loss: 0.28791 | val_0_auc: 0.87967 |  0:00:09s\n",
      "epoch 9  | loss: 0.28056 | val_0_auc: 0.8815  |  0:00:10s\n",
      "epoch 10 | loss: 0.27559 | val_0_auc: 0.88471 |  0:00:11s\n",
      "epoch 11 | loss: 0.27035 | val_0_auc: 0.8879  |  0:00:12s\n",
      "epoch 12 | loss: 0.26574 | val_0_auc: 0.89339 |  0:00:13s\n",
      "epoch 13 | loss: 0.26266 | val_0_auc: 0.89503 |  0:00:14s\n",
      "epoch 14 | loss: 0.25873 | val_0_auc: 0.89554 |  0:00:15s\n",
      "epoch 15 | loss: 0.25333 | val_0_auc: 0.89381 |  0:00:16s\n",
      "epoch 16 | loss: 0.25212 | val_0_auc: 0.89492 |  0:00:17s\n",
      "epoch 17 | loss: 0.24767 | val_0_auc: 0.89477 |  0:00:18s\n",
      "epoch 18 | loss: 0.24403 | val_0_auc: 0.89573 |  0:00:18s\n",
      "epoch 19 | loss: 0.242   | val_0_auc: 0.89551 |  0:00:19s\n",
      "epoch 20 | loss: 0.23817 | val_0_auc: 0.89693 |  0:00:20s\n",
      "epoch 21 | loss: 0.23389 | val_0_auc: 0.89321 |  0:00:21s\n",
      "epoch 22 | loss: 0.23348 | val_0_auc: 0.89894 |  0:00:22s\n",
      "epoch 23 | loss: 0.22899 | val_0_auc: 0.89604 |  0:00:23s\n",
      "epoch 24 | loss: 0.22891 | val_0_auc: 0.89416 |  0:00:24s\n",
      "epoch 25 | loss: 0.22647 | val_0_auc: 0.89768 |  0:00:25s\n",
      "epoch 26 | loss: 0.22712 | val_0_auc: 0.89391 |  0:00:26s\n",
      "epoch 27 | loss: 0.22302 | val_0_auc: 0.89796 |  0:00:27s\n",
      "epoch 28 | loss: 0.22115 | val_0_auc: 0.89379 |  0:00:27s\n",
      "epoch 29 | loss: 0.22117 | val_0_auc: 0.89598 |  0:00:28s\n",
      "epoch 30 | loss: 0.21945 | val_0_auc: 0.8962  |  0:00:29s\n",
      "epoch 31 | loss: 0.20909 | val_0_auc: 0.89595 |  0:00:30s\n",
      "epoch 32 | loss: 0.21052 | val_0_auc: 0.89521 |  0:00:31s\n",
      "epoch 33 | loss: 0.20953 | val_0_auc: 0.8927  |  0:00:31s\n",
      "epoch 34 | loss: 0.20804 | val_0_auc: 0.89088 |  0:00:32s\n",
      "epoch 35 | loss: 0.20508 | val_0_auc: 0.89313 |  0:00:33s\n",
      "epoch 36 | loss: 0.20523 | val_0_auc: 0.8916  |  0:00:34s\n",
      "epoch 37 | loss: 0.20371 | val_0_auc: 0.89116 |  0:00:35s\n",
      "epoch 38 | loss: 0.20332 | val_0_auc: 0.88899 |  0:00:35s\n",
      "epoch 39 | loss: 0.20229 | val_0_auc: 0.88826 |  0:00:36s\n",
      "epoch 40 | loss: 0.20339 | val_0_auc: 0.88821 |  0:00:37s\n",
      "epoch 41 | loss: 0.19931 | val_0_auc: 0.88875 |  0:00:38s\n",
      "epoch 42 | loss: 0.19519 | val_0_auc: 0.89198 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.89894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6916  | val_0_auc: 0.62325 |  0:00:01s\n",
      "epoch 1  | loss: 0.54812 | val_0_auc: 0.66734 |  0:00:03s\n",
      "epoch 2  | loss: 0.53206 | val_0_auc: 0.7289  |  0:00:06s\n",
      "epoch 3  | loss: 0.49039 | val_0_auc: 0.77537 |  0:00:08s\n",
      "epoch 4  | loss: 0.47283 | val_0_auc: 0.77575 |  0:00:10s\n",
      "epoch 5  | loss: 0.47535 | val_0_auc: 0.77961 |  0:00:11s\n",
      "epoch 6  | loss: 0.45526 | val_0_auc: 0.79757 |  0:00:13s\n",
      "epoch 7  | loss: 0.44773 | val_0_auc: 0.79994 |  0:00:15s\n",
      "epoch 8  | loss: 0.44362 | val_0_auc: 0.79878 |  0:00:16s\n",
      "epoch 9  | loss: 0.43448 | val_0_auc: 0.82269 |  0:00:18s\n",
      "epoch 10 | loss: 0.41718 | val_0_auc: 0.83518 |  0:00:20s\n",
      "epoch 11 | loss: 0.41083 | val_0_auc: 0.83882 |  0:00:21s\n",
      "epoch 12 | loss: 0.40075 | val_0_auc: 0.84101 |  0:00:23s\n",
      "epoch 13 | loss: 0.39254 | val_0_auc: 0.8485  |  0:00:24s\n",
      "epoch 14 | loss: 0.38992 | val_0_auc: 0.85714 |  0:00:26s\n",
      "epoch 15 | loss: 0.38083 | val_0_auc: 0.86024 |  0:00:27s\n",
      "epoch 16 | loss: 0.3799  | val_0_auc: 0.85881 |  0:00:29s\n",
      "epoch 17 | loss: 0.37668 | val_0_auc: 0.80494 |  0:00:30s\n",
      "epoch 18 | loss: 0.37428 | val_0_auc: 0.86573 |  0:00:32s\n",
      "epoch 19 | loss: 0.3663  | val_0_auc: 0.86774 |  0:00:34s\n",
      "epoch 20 | loss: 0.36326 | val_0_auc: 0.86885 |  0:00:35s\n",
      "epoch 21 | loss: 0.36325 | val_0_auc: 0.86715 |  0:00:37s\n",
      "epoch 22 | loss: 0.36343 | val_0_auc: 0.86293 |  0:00:39s\n",
      "epoch 23 | loss: 0.36904 | val_0_auc: 0.86089 |  0:00:40s\n",
      "epoch 24 | loss: 0.36383 | val_0_auc: 0.86115 |  0:00:42s\n",
      "epoch 25 | loss: 0.35338 | val_0_auc: 0.84047 |  0:00:43s\n",
      "epoch 26 | loss: 0.34885 | val_0_auc: 0.84038 |  0:00:45s\n",
      "epoch 27 | loss: 0.34342 | val_0_auc: 0.86995 |  0:00:46s\n",
      "epoch 28 | loss: 0.34181 | val_0_auc: 0.86519 |  0:00:48s\n",
      "epoch 29 | loss: 0.33635 | val_0_auc: 0.85153 |  0:00:49s\n",
      "epoch 30 | loss: 0.33426 | val_0_auc: 0.8539  |  0:00:51s\n",
      "epoch 31 | loss: 0.33516 | val_0_auc: 0.86619 |  0:00:52s\n",
      "epoch 32 | loss: 0.33188 | val_0_auc: 0.88039 |  0:00:54s\n",
      "epoch 33 | loss: 0.32934 | val_0_auc: 0.88479 |  0:00:56s\n",
      "epoch 34 | loss: 0.32714 | val_0_auc: 0.88592 |  0:00:57s\n",
      "epoch 35 | loss: 0.32648 | val_0_auc: 0.88766 |  0:00:59s\n",
      "epoch 36 | loss: 0.32214 | val_0_auc: 0.88796 |  0:01:00s\n",
      "epoch 37 | loss: 0.32405 | val_0_auc: 0.88835 |  0:01:02s\n",
      "epoch 38 | loss: 0.3254  | val_0_auc: 0.88568 |  0:01:03s\n",
      "epoch 39 | loss: 0.32245 | val_0_auc: 0.88722 |  0:01:05s\n",
      "epoch 40 | loss: 0.32297 | val_0_auc: 0.88625 |  0:01:06s\n",
      "epoch 41 | loss: 0.32575 | val_0_auc: 0.88698 |  0:01:08s\n",
      "epoch 42 | loss: 0.31994 | val_0_auc: 0.88659 |  0:01:10s\n",
      "epoch 43 | loss: 0.31652 | val_0_auc: 0.88617 |  0:01:11s\n",
      "epoch 44 | loss: 0.31611 | val_0_auc: 0.88961 |  0:01:13s\n",
      "epoch 45 | loss: 0.31634 | val_0_auc: 0.88996 |  0:01:14s\n",
      "epoch 46 | loss: 0.31588 | val_0_auc: 0.88766 |  0:01:16s\n",
      "epoch 47 | loss: 0.31558 | val_0_auc: 0.88877 |  0:01:17s\n",
      "epoch 48 | loss: 0.31561 | val_0_auc: 0.88343 |  0:01:19s\n",
      "epoch 49 | loss: 0.31483 | val_0_auc: 0.88077 |  0:01:20s\n",
      "epoch 50 | loss: 0.31571 | val_0_auc: 0.88981 |  0:01:22s\n",
      "epoch 51 | loss: 0.31645 | val_0_auc: 0.88098 |  0:01:23s\n",
      "epoch 52 | loss: 0.31447 | val_0_auc: 0.89099 |  0:01:25s\n",
      "epoch 53 | loss: 0.31389 | val_0_auc: 0.89278 |  0:01:27s\n",
      "epoch 54 | loss: 0.31121 | val_0_auc: 0.89287 |  0:01:28s\n",
      "epoch 55 | loss: 0.31344 | val_0_auc: 0.88344 |  0:01:30s\n",
      "epoch 56 | loss: 0.31457 | val_0_auc: 0.88982 |  0:01:31s\n",
      "epoch 57 | loss: 0.31222 | val_0_auc: 0.89226 |  0:01:33s\n",
      "epoch 58 | loss: 0.31316 | val_0_auc: 0.89268 |  0:01:34s\n",
      "epoch 59 | loss: 0.31112 | val_0_auc: 0.8927  |  0:01:36s\n",
      "epoch 60 | loss: 0.31234 | val_0_auc: 0.8932  |  0:01:37s\n",
      "epoch 61 | loss: 0.31035 | val_0_auc: 0.88478 |  0:01:39s\n",
      "epoch 62 | loss: 0.30774 | val_0_auc: 0.88324 |  0:01:40s\n",
      "epoch 63 | loss: 0.3065  | val_0_auc: 0.89296 |  0:01:42s\n",
      "epoch 64 | loss: 0.3095  | val_0_auc: 0.89133 |  0:01:43s\n",
      "epoch 65 | loss: 0.31165 | val_0_auc: 0.89265 |  0:01:45s\n",
      "epoch 66 | loss: 0.31133 | val_0_auc: 0.89545 |  0:01:47s\n",
      "epoch 67 | loss: 0.31145 | val_0_auc: 0.89626 |  0:01:48s\n",
      "epoch 68 | loss: 0.30903 | val_0_auc: 0.89526 |  0:01:50s\n",
      "epoch 69 | loss: 0.30816 | val_0_auc: 0.89677 |  0:01:51s\n",
      "epoch 70 | loss: 0.30761 | val_0_auc: 0.89756 |  0:01:53s\n",
      "epoch 71 | loss: 0.30574 | val_0_auc: 0.89784 |  0:01:54s\n",
      "epoch 72 | loss: 0.30424 | val_0_auc: 0.89874 |  0:01:56s\n",
      "epoch 73 | loss: 0.30388 | val_0_auc: 0.89819 |  0:01:57s\n",
      "epoch 74 | loss: 0.30684 | val_0_auc: 0.89885 |  0:01:59s\n",
      "epoch 75 | loss: 0.30577 | val_0_auc: 0.89796 |  0:02:00s\n",
      "epoch 76 | loss: 0.30659 | val_0_auc: 0.89825 |  0:02:02s\n",
      "epoch 77 | loss: 0.30995 | val_0_auc: 0.8944  |  0:02:03s\n",
      "epoch 78 | loss: 0.30942 | val_0_auc: 0.89051 |  0:02:05s\n",
      "epoch 79 | loss: 0.30769 | val_0_auc: 0.89507 |  0:02:06s\n",
      "epoch 80 | loss: 0.30749 | val_0_auc: 0.89645 |  0:02:08s\n",
      "epoch 81 | loss: 0.30727 | val_0_auc: 0.89397 |  0:02:10s\n",
      "epoch 82 | loss: 0.30946 | val_0_auc: 0.89239 |  0:02:12s\n",
      "epoch 83 | loss: 0.30757 | val_0_auc: 0.89562 |  0:02:14s\n",
      "epoch 84 | loss: 0.30525 | val_0_auc: 0.8972  |  0:02:15s\n",
      "epoch 85 | loss: 0.30426 | val_0_auc: 0.89609 |  0:02:17s\n",
      "epoch 86 | loss: 0.30591 | val_0_auc: 0.89668 |  0:02:18s\n",
      "epoch 87 | loss: 0.30747 | val_0_auc: 0.89699 |  0:02:20s\n",
      "epoch 88 | loss: 0.30472 | val_0_auc: 0.89796 |  0:02:21s\n",
      "epoch 89 | loss: 0.30359 | val_0_auc: 0.89911 |  0:02:23s\n",
      "epoch 90 | loss: 0.3115  | val_0_auc: 0.89708 |  0:02:24s\n",
      "epoch 91 | loss: 0.33109 | val_0_auc: 0.87344 |  0:02:26s\n",
      "epoch 92 | loss: 0.33571 | val_0_auc: 0.87255 |  0:02:28s\n",
      "epoch 93 | loss: 0.33225 | val_0_auc: 0.89556 |  0:02:29s\n",
      "epoch 94 | loss: 0.31197 | val_0_auc: 0.87074 |  0:02:31s\n",
      "epoch 95 | loss: 0.30845 | val_0_auc: 0.89916 |  0:02:33s\n",
      "epoch 96 | loss: 0.30968 | val_0_auc: 0.8981  |  0:02:34s\n",
      "epoch 97 | loss: 0.30913 | val_0_auc: 0.89784 |  0:02:36s\n",
      "epoch 98 | loss: 0.30705 | val_0_auc: 0.89734 |  0:02:37s\n",
      "epoch 99 | loss: 0.30554 | val_0_auc: 0.8971  |  0:02:39s\n",
      "epoch 100| loss: 0.32135 | val_0_auc: 0.88409 |  0:02:40s\n",
      "epoch 101| loss: 0.32647 | val_0_auc: 0.8767  |  0:02:42s\n",
      "epoch 102| loss: 0.32002 | val_0_auc: 0.87904 |  0:02:43s\n",
      "epoch 103| loss: 0.31875 | val_0_auc: 0.87941 |  0:02:45s\n",
      "epoch 104| loss: 0.31747 | val_0_auc: 0.89421 |  0:02:47s\n",
      "epoch 105| loss: 0.31688 | val_0_auc: 0.89419 |  0:02:48s\n",
      "epoch 106| loss: 0.31583 | val_0_auc: 0.89681 |  0:02:50s\n",
      "epoch 107| loss: 0.31133 | val_0_auc: 0.89096 |  0:02:51s\n",
      "epoch 108| loss: 0.30981 | val_0_auc: 0.89283 |  0:02:53s\n",
      "epoch 109| loss: 0.31091 | val_0_auc: 0.89218 |  0:02:55s\n",
      "epoch 110| loss: 0.31442 | val_0_auc: 0.89226 |  0:02:56s\n",
      "epoch 111| loss: 0.31419 | val_0_auc: 0.89127 |  0:02:58s\n",
      "epoch 112| loss: 0.31055 | val_0_auc: 0.89315 |  0:03:00s\n",
      "epoch 113| loss: 0.30763 | val_0_auc: 0.89416 |  0:03:01s\n",
      "epoch 114| loss: 0.30637 | val_0_auc: 0.89546 |  0:03:03s\n",
      "epoch 115| loss: 0.30609 | val_0_auc: 0.89756 |  0:03:04s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 95 and best_val_0_auc = 0.89916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.53135 | val_0_auc: 0.67106 |  0:00:01s\n",
      "epoch 1  | loss: 0.42633 | val_0_auc: 0.79781 |  0:00:02s\n",
      "epoch 2  | loss: 0.39527 | val_0_auc: 0.82905 |  0:00:03s\n",
      "epoch 3  | loss: 0.37935 | val_0_auc: 0.85093 |  0:00:04s\n",
      "epoch 4  | loss: 0.3445  | val_0_auc: 0.87008 |  0:00:06s\n",
      "epoch 5  | loss: 0.32508 | val_0_auc: 0.88878 |  0:00:07s\n",
      "epoch 6  | loss: 0.30894 | val_0_auc: 0.8956  |  0:00:08s\n",
      "epoch 7  | loss: 0.30401 | val_0_auc: 0.90196 |  0:00:09s\n",
      "epoch 8  | loss: 0.29488 | val_0_auc: 0.90476 |  0:00:10s\n",
      "epoch 9  | loss: 0.28607 | val_0_auc: 0.90926 |  0:00:11s\n",
      "epoch 10 | loss: 0.28422 | val_0_auc: 0.91087 |  0:00:12s\n",
      "epoch 11 | loss: 0.28203 | val_0_auc: 0.91282 |  0:00:14s\n",
      "epoch 12 | loss: 0.27896 | val_0_auc: 0.91351 |  0:00:15s\n",
      "epoch 13 | loss: 0.27622 | val_0_auc: 0.9099  |  0:00:16s\n",
      "epoch 14 | loss: 0.2772  | val_0_auc: 0.91113 |  0:00:18s\n",
      "epoch 15 | loss: 0.27641 | val_0_auc: 0.90989 |  0:00:19s\n",
      "epoch 16 | loss: 0.27313 | val_0_auc: 0.91347 |  0:00:20s\n",
      "epoch 17 | loss: 0.27213 | val_0_auc: 0.91353 |  0:00:21s\n",
      "epoch 18 | loss: 0.269   | val_0_auc: 0.91382 |  0:00:22s\n",
      "epoch 19 | loss: 0.26958 | val_0_auc: 0.91399 |  0:00:24s\n",
      "epoch 20 | loss: 0.26793 | val_0_auc: 0.91581 |  0:00:25s\n",
      "epoch 21 | loss: 0.26629 | val_0_auc: 0.9168  |  0:00:26s\n",
      "epoch 22 | loss: 0.26478 | val_0_auc: 0.91421 |  0:00:28s\n",
      "epoch 23 | loss: 0.26045 | val_0_auc: 0.91446 |  0:00:29s\n",
      "epoch 24 | loss: 0.26218 | val_0_auc: 0.91623 |  0:00:30s\n",
      "epoch 25 | loss: 0.2611  | val_0_auc: 0.91519 |  0:00:31s\n",
      "epoch 26 | loss: 0.25941 | val_0_auc: 0.91683 |  0:00:32s\n",
      "epoch 27 | loss: 0.25956 | val_0_auc: 0.914   |  0:00:33s\n",
      "epoch 28 | loss: 0.25984 | val_0_auc: 0.91574 |  0:00:35s\n",
      "epoch 29 | loss: 0.25851 | val_0_auc: 0.91328 |  0:00:36s\n",
      "epoch 30 | loss: 0.25614 | val_0_auc: 0.9132  |  0:00:37s\n",
      "epoch 31 | loss: 0.25431 | val_0_auc: 0.91148 |  0:00:38s\n",
      "epoch 32 | loss: 0.25659 | val_0_auc: 0.91572 |  0:00:40s\n",
      "epoch 33 | loss: 0.2572  | val_0_auc: 0.91626 |  0:00:41s\n",
      "epoch 34 | loss: 0.25312 | val_0_auc: 0.91491 |  0:00:42s\n",
      "epoch 35 | loss: 0.25544 | val_0_auc: 0.91329 |  0:00:44s\n",
      "epoch 36 | loss: 0.25609 | val_0_auc: 0.91458 |  0:00:45s\n",
      "epoch 37 | loss: 0.25113 | val_0_auc: 0.91394 |  0:00:46s\n",
      "epoch 38 | loss: 0.25082 | val_0_auc: 0.91464 |  0:00:47s\n",
      "epoch 39 | loss: 0.24996 | val_0_auc: 0.91288 |  0:00:49s\n",
      "epoch 40 | loss: 0.25018 | val_0_auc: 0.91195 |  0:00:50s\n",
      "epoch 41 | loss: 0.25011 | val_0_auc: 0.91278 |  0:00:51s\n",
      "epoch 42 | loss: 0.24828 | val_0_auc: 0.91101 |  0:00:52s\n",
      "epoch 43 | loss: 0.24739 | val_0_auc: 0.91146 |  0:00:53s\n",
      "epoch 44 | loss: 0.24645 | val_0_auc: 0.91375 |  0:00:54s\n",
      "epoch 45 | loss: 0.24718 | val_0_auc: 0.91312 |  0:00:55s\n",
      "epoch 46 | loss: 0.246   | val_0_auc: 0.90828 |  0:00:56s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_auc = 0.91683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.63397 | val_0_auc: 0.72383 |  0:00:02s\n",
      "epoch 1  | loss: 0.43726 | val_0_auc: 0.79065 |  0:00:04s\n",
      "epoch 2  | loss: 0.38869 | val_0_auc: 0.82497 |  0:00:07s\n",
      "epoch 3  | loss: 0.36657 | val_0_auc: 0.83727 |  0:00:09s\n",
      "epoch 4  | loss: 0.35059 | val_0_auc: 0.85044 |  0:00:11s\n",
      "epoch 5  | loss: 0.33875 | val_0_auc: 0.86712 |  0:00:14s\n",
      "epoch 6  | loss: 0.33095 | val_0_auc: 0.87696 |  0:00:17s\n",
      "epoch 7  | loss: 0.32389 | val_0_auc: 0.88051 |  0:00:19s\n",
      "epoch 8  | loss: 0.32423 | val_0_auc: 0.87868 |  0:00:22s\n",
      "epoch 9  | loss: 0.31931 | val_0_auc: 0.88491 |  0:00:25s\n",
      "epoch 10 | loss: 0.31463 | val_0_auc: 0.89123 |  0:00:27s\n",
      "epoch 11 | loss: 0.30717 | val_0_auc: 0.89528 |  0:00:30s\n",
      "epoch 12 | loss: 0.30264 | val_0_auc: 0.89913 |  0:00:32s\n",
      "epoch 13 | loss: 0.29732 | val_0_auc: 0.90099 |  0:00:35s\n",
      "epoch 14 | loss: 0.29279 | val_0_auc: 0.90389 |  0:00:37s\n",
      "epoch 15 | loss: 0.29267 | val_0_auc: 0.9023  |  0:00:40s\n",
      "epoch 16 | loss: 0.2941  | val_0_auc: 0.89528 |  0:00:42s\n",
      "epoch 17 | loss: 0.29226 | val_0_auc: 0.89988 |  0:00:44s\n",
      "epoch 18 | loss: 0.28818 | val_0_auc: 0.89854 |  0:00:46s\n",
      "epoch 19 | loss: 0.28271 | val_0_auc: 0.90061 |  0:00:49s\n",
      "epoch 20 | loss: 0.27875 | val_0_auc: 0.9009  |  0:00:51s\n",
      "epoch 21 | loss: 0.27729 | val_0_auc: 0.90245 |  0:00:54s\n",
      "epoch 22 | loss: 0.27278 | val_0_auc: 0.90086 |  0:00:56s\n",
      "epoch 23 | loss: 0.27387 | val_0_auc: 0.90151 |  0:00:58s\n",
      "epoch 24 | loss: 0.2682  | val_0_auc: 0.90435 |  0:01:01s\n",
      "epoch 25 | loss: 0.26158 | val_0_auc: 0.90423 |  0:01:03s\n",
      "epoch 26 | loss: 0.26324 | val_0_auc: 0.90292 |  0:01:06s\n",
      "epoch 27 | loss: 0.26077 | val_0_auc: 0.9016  |  0:01:08s\n",
      "epoch 28 | loss: 0.25908 | val_0_auc: 0.90045 |  0:01:10s\n",
      "epoch 29 | loss: 0.25794 | val_0_auc: 0.90254 |  0:01:13s\n",
      "epoch 30 | loss: 0.2579  | val_0_auc: 0.90278 |  0:01:15s\n",
      "epoch 31 | loss: 0.25885 | val_0_auc: 0.89898 |  0:01:17s\n",
      "epoch 32 | loss: 0.25634 | val_0_auc: 0.90183 |  0:01:19s\n",
      "epoch 33 | loss: 0.25246 | val_0_auc: 0.90202 |  0:01:22s\n",
      "epoch 34 | loss: 0.24987 | val_0_auc: 0.90179 |  0:01:24s\n",
      "epoch 35 | loss: 0.24636 | val_0_auc: 0.90142 |  0:01:26s\n",
      "epoch 36 | loss: 0.24545 | val_0_auc: 0.89781 |  0:01:28s\n",
      "epoch 37 | loss: 0.24263 | val_0_auc: 0.89939 |  0:01:31s\n",
      "epoch 38 | loss: 0.24037 | val_0_auc: 0.89799 |  0:01:33s\n",
      "epoch 39 | loss: 0.24008 | val_0_auc: 0.89192 |  0:01:35s\n",
      "epoch 40 | loss: 0.24492 | val_0_auc: 0.88751 |  0:01:37s\n",
      "epoch 41 | loss: 0.25193 | val_0_auc: 0.89419 |  0:01:40s\n",
      "epoch 42 | loss: 0.24937 | val_0_auc: 0.89096 |  0:01:42s\n",
      "epoch 43 | loss: 0.24998 | val_0_auc: 0.89603 |  0:01:44s\n",
      "epoch 44 | loss: 0.24779 | val_0_auc: 0.8983  |  0:01:47s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_auc = 0.90435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6929  | val_0_auc: 0.5708  |  0:00:01s\n",
      "epoch 1  | loss: 0.59058 | val_0_auc: 0.70224 |  0:00:03s\n",
      "epoch 2  | loss: 0.54912 | val_0_auc: 0.73002 |  0:00:05s\n",
      "epoch 3  | loss: 0.51153 | val_0_auc: 0.77299 |  0:00:07s\n",
      "epoch 4  | loss: 0.48059 | val_0_auc: 0.79959 |  0:00:09s\n",
      "epoch 5  | loss: 0.4667  | val_0_auc: 0.80181 |  0:00:10s\n",
      "epoch 6  | loss: 0.4699  | val_0_auc: 0.77898 |  0:00:12s\n",
      "epoch 7  | loss: 0.46353 | val_0_auc: 0.81665 |  0:00:14s\n",
      "epoch 8  | loss: 0.45063 | val_0_auc: 0.82893 |  0:00:16s\n",
      "epoch 9  | loss: 0.44078 | val_0_auc: 0.83138 |  0:00:18s\n",
      "epoch 10 | loss: 0.42832 | val_0_auc: 0.83667 |  0:00:20s\n",
      "epoch 11 | loss: 0.42791 | val_0_auc: 0.836   |  0:00:22s\n",
      "epoch 12 | loss: 0.42115 | val_0_auc: 0.84385 |  0:00:24s\n",
      "epoch 13 | loss: 0.41073 | val_0_auc: 0.8526  |  0:00:25s\n",
      "epoch 14 | loss: 0.40933 | val_0_auc: 0.85289 |  0:00:27s\n",
      "epoch 15 | loss: 0.40384 | val_0_auc: 0.85381 |  0:00:29s\n",
      "epoch 16 | loss: 0.39551 | val_0_auc: 0.85671 |  0:00:31s\n",
      "epoch 17 | loss: 0.393   | val_0_auc: 0.86112 |  0:00:33s\n",
      "epoch 18 | loss: 0.38547 | val_0_auc: 0.86531 |  0:00:35s\n",
      "epoch 19 | loss: 0.3785  | val_0_auc: 0.86453 |  0:00:37s\n",
      "epoch 20 | loss: 0.37442 | val_0_auc: 0.86528 |  0:00:39s\n",
      "epoch 21 | loss: 0.3748  | val_0_auc: 0.86928 |  0:00:40s\n",
      "epoch 22 | loss: 0.3699  | val_0_auc: 0.87182 |  0:00:42s\n",
      "epoch 23 | loss: 0.37074 | val_0_auc: 0.87323 |  0:00:44s\n",
      "epoch 24 | loss: 0.36574 | val_0_auc: 0.87294 |  0:00:45s\n",
      "epoch 25 | loss: 0.36058 | val_0_auc: 0.87446 |  0:00:47s\n",
      "epoch 26 | loss: 0.35908 | val_0_auc: 0.87593 |  0:00:49s\n",
      "epoch 27 | loss: 0.35853 | val_0_auc: 0.87774 |  0:00:51s\n",
      "epoch 28 | loss: 0.3593  | val_0_auc: 0.87749 |  0:00:52s\n",
      "epoch 29 | loss: 0.35669 | val_0_auc: 0.87351 |  0:00:54s\n",
      "epoch 30 | loss: 0.35443 | val_0_auc: 0.87552 |  0:00:56s\n",
      "epoch 31 | loss: 0.35178 | val_0_auc: 0.87707 |  0:00:58s\n",
      "epoch 32 | loss: 0.34812 | val_0_auc: 0.87587 |  0:01:00s\n",
      "epoch 33 | loss: 0.35107 | val_0_auc: 0.87909 |  0:01:02s\n",
      "epoch 34 | loss: 0.34498 | val_0_auc: 0.88089 |  0:01:04s\n",
      "epoch 35 | loss: 0.34305 | val_0_auc: 0.87855 |  0:01:06s\n",
      "epoch 36 | loss: 0.34314 | val_0_auc: 0.87955 |  0:01:08s\n",
      "epoch 37 | loss: 0.34085 | val_0_auc: 0.87997 |  0:01:10s\n",
      "epoch 38 | loss: 0.34168 | val_0_auc: 0.87725 |  0:01:12s\n",
      "epoch 39 | loss: 0.33994 | val_0_auc: 0.88159 |  0:01:14s\n",
      "epoch 40 | loss: 0.33594 | val_0_auc: 0.88503 |  0:01:16s\n",
      "epoch 41 | loss: 0.33278 | val_0_auc: 0.88634 |  0:01:18s\n",
      "epoch 42 | loss: 0.33476 | val_0_auc: 0.88321 |  0:01:20s\n",
      "epoch 43 | loss: 0.33304 | val_0_auc: 0.88719 |  0:01:22s\n",
      "epoch 44 | loss: 0.33102 | val_0_auc: 0.88659 |  0:01:23s\n",
      "epoch 45 | loss: 0.32824 | val_0_auc: 0.8856  |  0:01:25s\n",
      "epoch 46 | loss: 0.3308  | val_0_auc: 0.88551 |  0:01:27s\n",
      "epoch 47 | loss: 0.33097 | val_0_auc: 0.88447 |  0:01:29s\n",
      "epoch 48 | loss: 0.32993 | val_0_auc: 0.8857  |  0:01:30s\n",
      "epoch 49 | loss: 0.32846 | val_0_auc: 0.88526 |  0:01:32s\n",
      "epoch 50 | loss: 0.32972 | val_0_auc: 0.88809 |  0:01:34s\n",
      "epoch 51 | loss: 0.32704 | val_0_auc: 0.88961 |  0:01:36s\n",
      "epoch 52 | loss: 0.32117 | val_0_auc: 0.88892 |  0:01:37s\n",
      "epoch 53 | loss: 0.32198 | val_0_auc: 0.8903  |  0:01:39s\n",
      "epoch 54 | loss: 0.32289 | val_0_auc: 0.88873 |  0:01:41s\n",
      "epoch 55 | loss: 0.32257 | val_0_auc: 0.89092 |  0:01:43s\n",
      "epoch 56 | loss: 0.32172 | val_0_auc: 0.89285 |  0:01:44s\n",
      "epoch 57 | loss: 0.32129 | val_0_auc: 0.89227 |  0:01:46s\n",
      "epoch 58 | loss: 0.31938 | val_0_auc: 0.89319 |  0:01:48s\n",
      "epoch 59 | loss: 0.3178  | val_0_auc: 0.89505 |  0:01:50s\n",
      "epoch 60 | loss: 0.31948 | val_0_auc: 0.89383 |  0:01:51s\n",
      "epoch 61 | loss: 0.31647 | val_0_auc: 0.89363 |  0:01:53s\n",
      "epoch 62 | loss: 0.31801 | val_0_auc: 0.89356 |  0:01:55s\n",
      "epoch 63 | loss: 0.316   | val_0_auc: 0.89427 |  0:01:57s\n",
      "epoch 64 | loss: 0.31351 | val_0_auc: 0.89549 |  0:01:59s\n",
      "epoch 65 | loss: 0.3143  | val_0_auc: 0.89585 |  0:02:01s\n",
      "epoch 66 | loss: 0.31072 | val_0_auc: 0.89471 |  0:02:02s\n",
      "epoch 67 | loss: 0.31092 | val_0_auc: 0.89025 |  0:02:04s\n",
      "epoch 68 | loss: 0.31679 | val_0_auc: 0.89275 |  0:02:06s\n",
      "epoch 69 | loss: 0.31818 | val_0_auc: 0.89097 |  0:02:07s\n",
      "epoch 70 | loss: 0.31515 | val_0_auc: 0.89232 |  0:02:09s\n",
      "epoch 71 | loss: 0.31731 | val_0_auc: 0.89085 |  0:02:11s\n",
      "epoch 72 | loss: 0.31744 | val_0_auc: 0.89163 |  0:02:13s\n",
      "epoch 73 | loss: 0.31691 | val_0_auc: 0.89242 |  0:02:15s\n",
      "epoch 74 | loss: 0.31433 | val_0_auc: 0.89307 |  0:02:17s\n",
      "epoch 75 | loss: 0.31569 | val_0_auc: 0.89326 |  0:02:19s\n",
      "epoch 76 | loss: 0.31491 | val_0_auc: 0.89262 |  0:02:20s\n",
      "epoch 77 | loss: 0.31348 | val_0_auc: 0.8958  |  0:02:22s\n",
      "epoch 78 | loss: 0.3099  | val_0_auc: 0.89488 |  0:02:24s\n",
      "epoch 79 | loss: 0.30683 | val_0_auc: 0.89504 |  0:02:26s\n",
      "epoch 80 | loss: 0.30796 | val_0_auc: 0.89709 |  0:02:28s\n",
      "epoch 81 | loss: 0.31035 | val_0_auc: 0.89655 |  0:02:30s\n",
      "epoch 82 | loss: 0.30871 | val_0_auc: 0.89639 |  0:02:31s\n",
      "epoch 83 | loss: 0.309   | val_0_auc: 0.89673 |  0:02:33s\n",
      "epoch 84 | loss: 0.30773 | val_0_auc: 0.89792 |  0:02:35s\n",
      "epoch 85 | loss: 0.30802 | val_0_auc: 0.89681 |  0:02:36s\n",
      "epoch 86 | loss: 0.30917 | val_0_auc: 0.89488 |  0:02:38s\n",
      "epoch 87 | loss: 0.31004 | val_0_auc: 0.8953  |  0:02:40s\n",
      "epoch 88 | loss: 0.30486 | val_0_auc: 0.89873 |  0:02:42s\n",
      "epoch 89 | loss: 0.30614 | val_0_auc: 0.89815 |  0:02:43s\n",
      "epoch 90 | loss: 0.30492 | val_0_auc: 0.8997  |  0:02:45s\n",
      "epoch 91 | loss: 0.30839 | val_0_auc: 0.8983  |  0:02:47s\n",
      "epoch 92 | loss: 0.30742 | val_0_auc: 0.89942 |  0:02:48s\n",
      "epoch 93 | loss: 0.30542 | val_0_auc: 0.89877 |  0:02:50s\n",
      "epoch 94 | loss: 0.30591 | val_0_auc: 0.89621 |  0:02:52s\n",
      "epoch 95 | loss: 0.30508 | val_0_auc: 0.89837 |  0:02:53s\n",
      "epoch 96 | loss: 0.30558 | val_0_auc: 0.89991 |  0:02:55s\n",
      "epoch 97 | loss: 0.30343 | val_0_auc: 0.89981 |  0:02:57s\n",
      "epoch 98 | loss: 0.30421 | val_0_auc: 0.8992  |  0:02:59s\n",
      "epoch 99 | loss: 0.30194 | val_0_auc: 0.89996 |  0:03:01s\n",
      "epoch 100| loss: 0.30119 | val_0_auc: 0.90196 |  0:03:02s\n",
      "epoch 101| loss: 0.29995 | val_0_auc: 0.90174 |  0:03:04s\n",
      "epoch 102| loss: 0.30136 | val_0_auc: 0.90135 |  0:03:06s\n",
      "epoch 103| loss: 0.29929 | val_0_auc: 0.90145 |  0:03:08s\n",
      "epoch 104| loss: 0.30013 | val_0_auc: 0.90203 |  0:03:09s\n",
      "epoch 105| loss: 0.29999 | val_0_auc: 0.90143 |  0:03:11s\n",
      "epoch 106| loss: 0.30013 | val_0_auc: 0.90161 |  0:03:13s\n",
      "epoch 107| loss: 0.29902 | val_0_auc: 0.90214 |  0:03:15s\n",
      "epoch 108| loss: 0.29857 | val_0_auc: 0.90266 |  0:03:18s\n",
      "epoch 109| loss: 0.29965 | val_0_auc: 0.90175 |  0:03:21s\n",
      "epoch 110| loss: 0.29914 | val_0_auc: 0.90327 |  0:03:22s\n",
      "epoch 111| loss: 0.2972  | val_0_auc: 0.90278 |  0:03:24s\n",
      "epoch 112| loss: 0.29732 | val_0_auc: 0.90188 |  0:03:26s\n",
      "epoch 113| loss: 0.30107 | val_0_auc: 0.89984 |  0:03:27s\n",
      "epoch 114| loss: 0.30143 | val_0_auc: 0.90122 |  0:03:29s\n",
      "epoch 115| loss: 0.29989 | val_0_auc: 0.90189 |  0:03:31s\n",
      "epoch 116| loss: 0.29793 | val_0_auc: 0.90438 |  0:03:32s\n",
      "epoch 117| loss: 0.29797 | val_0_auc: 0.90349 |  0:03:34s\n",
      "epoch 118| loss: 0.29819 | val_0_auc: 0.90433 |  0:03:36s\n",
      "epoch 119| loss: 0.30012 | val_0_auc: 0.90244 |  0:03:37s\n",
      "epoch 120| loss: 0.29988 | val_0_auc: 0.90351 |  0:03:39s\n",
      "epoch 121| loss: 0.30219 | val_0_auc: 0.90431 |  0:03:41s\n",
      "epoch 122| loss: 0.30003 | val_0_auc: 0.90233 |  0:03:42s\n",
      "epoch 123| loss: 0.3063  | val_0_auc: 0.89911 |  0:03:44s\n",
      "epoch 124| loss: 0.30544 | val_0_auc: 0.90236 |  0:03:46s\n",
      "epoch 125| loss: 0.30207 | val_0_auc: 0.9031  |  0:03:48s\n",
      "epoch 126| loss: 0.29999 | val_0_auc: 0.90217 |  0:03:50s\n",
      "epoch 127| loss: 0.30069 | val_0_auc: 0.90197 |  0:03:52s\n",
      "epoch 128| loss: 0.3001  | val_0_auc: 0.90225 |  0:03:54s\n",
      "epoch 129| loss: 0.30362 | val_0_auc: 0.89911 |  0:03:56s\n",
      "epoch 130| loss: 0.30634 | val_0_auc: 0.89768 |  0:03:59s\n",
      "epoch 131| loss: 0.30749 | val_0_auc: 0.90008 |  0:04:00s\n",
      "epoch 132| loss: 0.30329 | val_0_auc: 0.90164 |  0:04:02s\n",
      "epoch 133| loss: 0.30254 | val_0_auc: 0.90137 |  0:04:04s\n",
      "epoch 134| loss: 0.30543 | val_0_auc: 0.90071 |  0:04:06s\n",
      "epoch 135| loss: 0.30397 | val_0_auc: 0.90189 |  0:04:09s\n",
      "epoch 136| loss: 0.30548 | val_0_auc: 0.90041 |  0:04:11s\n",
      "\n",
      "Early stopping occurred at epoch 136 with best_epoch = 116 and best_val_0_auc = 0.90438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.34947 | val_0_auc: 0.5865  |  0:00:02s\n",
      "epoch 1  | loss: 0.59759 | val_0_auc: 0.59167 |  0:00:03s\n",
      "epoch 2  | loss: 0.50825 | val_0_auc: 0.70153 |  0:00:04s\n",
      "epoch 3  | loss: 0.45483 | val_0_auc: 0.77026 |  0:00:05s\n",
      "epoch 4  | loss: 0.43806 | val_0_auc: 0.78572 |  0:00:06s\n",
      "epoch 5  | loss: 0.43279 | val_0_auc: 0.79497 |  0:00:08s\n",
      "epoch 6  | loss: 0.4267  | val_0_auc: 0.80024 |  0:00:09s\n",
      "epoch 7  | loss: 0.42412 | val_0_auc: 0.81125 |  0:00:10s\n",
      "epoch 8  | loss: 0.42179 | val_0_auc: 0.81122 |  0:00:11s\n",
      "epoch 9  | loss: 0.41821 | val_0_auc: 0.82035 |  0:00:12s\n",
      "epoch 10 | loss: 0.41201 | val_0_auc: 0.83061 |  0:00:13s\n",
      "epoch 11 | loss: 0.40855 | val_0_auc: 0.82317 |  0:00:14s\n",
      "epoch 12 | loss: 0.40718 | val_0_auc: 0.8332  |  0:00:15s\n",
      "epoch 13 | loss: 0.39915 | val_0_auc: 0.83872 |  0:00:16s\n",
      "epoch 14 | loss: 0.39496 | val_0_auc: 0.84454 |  0:00:17s\n",
      "epoch 15 | loss: 0.38927 | val_0_auc: 0.84397 |  0:00:19s\n",
      "epoch 16 | loss: 0.3913  | val_0_auc: 0.8444  |  0:00:20s\n",
      "epoch 17 | loss: 0.38805 | val_0_auc: 0.84638 |  0:00:21s\n",
      "epoch 18 | loss: 0.38627 | val_0_auc: 0.8471  |  0:00:22s\n",
      "epoch 19 | loss: 0.3814  | val_0_auc: 0.8497  |  0:00:24s\n",
      "epoch 20 | loss: 0.37789 | val_0_auc: 0.8513  |  0:00:25s\n",
      "epoch 21 | loss: 0.37563 | val_0_auc: 0.85245 |  0:00:26s\n",
      "epoch 22 | loss: 0.37451 | val_0_auc: 0.85585 |  0:00:27s\n",
      "epoch 23 | loss: 0.37124 | val_0_auc: 0.86027 |  0:00:28s\n",
      "epoch 24 | loss: 0.36888 | val_0_auc: 0.86407 |  0:00:30s\n",
      "epoch 25 | loss: 0.36312 | val_0_auc: 0.86806 |  0:00:31s\n",
      "epoch 26 | loss: 0.36082 | val_0_auc: 0.85651 |  0:00:32s\n",
      "epoch 27 | loss: 0.36389 | val_0_auc: 0.86381 |  0:00:34s\n",
      "epoch 28 | loss: 0.35561 | val_0_auc: 0.86749 |  0:00:35s\n",
      "epoch 29 | loss: 0.35182 | val_0_auc: 0.87005 |  0:00:36s\n",
      "epoch 30 | loss: 0.34819 | val_0_auc: 0.87225 |  0:00:37s\n",
      "epoch 31 | loss: 0.34708 | val_0_auc: 0.87427 |  0:00:39s\n",
      "epoch 32 | loss: 0.34481 | val_0_auc: 0.87345 |  0:00:40s\n",
      "epoch 33 | loss: 0.34288 | val_0_auc: 0.87625 |  0:00:41s\n",
      "epoch 34 | loss: 0.34238 | val_0_auc: 0.87502 |  0:00:42s\n",
      "epoch 35 | loss: 0.33701 | val_0_auc: 0.88044 |  0:00:43s\n",
      "epoch 36 | loss: 0.33207 | val_0_auc: 0.88294 |  0:00:44s\n",
      "epoch 37 | loss: 0.32893 | val_0_auc: 0.88392 |  0:00:45s\n",
      "epoch 38 | loss: 0.32553 | val_0_auc: 0.88626 |  0:00:46s\n",
      "epoch 39 | loss: 0.32467 | val_0_auc: 0.8883  |  0:00:47s\n",
      "epoch 40 | loss: 0.32307 | val_0_auc: 0.88809 |  0:00:49s\n",
      "epoch 41 | loss: 0.32237 | val_0_auc: 0.88909 |  0:00:50s\n",
      "epoch 42 | loss: 0.32045 | val_0_auc: 0.89084 |  0:00:51s\n",
      "epoch 43 | loss: 0.31614 | val_0_auc: 0.89244 |  0:00:52s\n",
      "epoch 44 | loss: 0.31443 | val_0_auc: 0.89432 |  0:00:53s\n",
      "epoch 45 | loss: 0.31368 | val_0_auc: 0.89341 |  0:00:54s\n",
      "epoch 46 | loss: 0.31064 | val_0_auc: 0.89468 |  0:00:55s\n",
      "epoch 47 | loss: 0.30798 | val_0_auc: 0.89621 |  0:00:56s\n",
      "epoch 48 | loss: 0.31053 | val_0_auc: 0.89364 |  0:00:58s\n",
      "epoch 49 | loss: 0.31003 | val_0_auc: 0.89589 |  0:00:59s\n",
      "epoch 50 | loss: 0.30746 | val_0_auc: 0.89678 |  0:01:00s\n",
      "epoch 51 | loss: 0.30824 | val_0_auc: 0.89893 |  0:01:01s\n",
      "epoch 52 | loss: 0.30508 | val_0_auc: 0.90029 |  0:01:02s\n",
      "epoch 53 | loss: 0.30375 | val_0_auc: 0.90173 |  0:01:03s\n",
      "epoch 54 | loss: 0.30165 | val_0_auc: 0.90127 |  0:01:04s\n",
      "epoch 55 | loss: 0.30321 | val_0_auc: 0.90179 |  0:01:06s\n",
      "epoch 56 | loss: 0.30191 | val_0_auc: 0.90107 |  0:01:07s\n",
      "epoch 57 | loss: 0.30163 | val_0_auc: 0.90245 |  0:01:08s\n",
      "epoch 58 | loss: 0.29831 | val_0_auc: 0.90304 |  0:01:09s\n",
      "epoch 59 | loss: 0.29948 | val_0_auc: 0.90387 |  0:01:10s\n",
      "epoch 60 | loss: 0.2977  | val_0_auc: 0.9028  |  0:01:11s\n",
      "epoch 61 | loss: 0.30086 | val_0_auc: 0.90364 |  0:01:12s\n",
      "epoch 62 | loss: 0.30104 | val_0_auc: 0.90316 |  0:01:13s\n",
      "epoch 63 | loss: 0.30077 | val_0_auc: 0.90481 |  0:01:14s\n",
      "epoch 64 | loss: 0.29946 | val_0_auc: 0.90498 |  0:01:16s\n",
      "epoch 65 | loss: 0.29897 | val_0_auc: 0.90629 |  0:01:17s\n",
      "epoch 66 | loss: 0.29521 | val_0_auc: 0.90649 |  0:01:18s\n",
      "epoch 67 | loss: 0.29381 | val_0_auc: 0.90717 |  0:01:19s\n",
      "epoch 68 | loss: 0.29564 | val_0_auc: 0.90299 |  0:01:20s\n",
      "epoch 69 | loss: 0.29445 | val_0_auc: 0.90495 |  0:01:21s\n",
      "epoch 70 | loss: 0.29297 | val_0_auc: 0.90875 |  0:01:23s\n",
      "epoch 71 | loss: 0.2922  | val_0_auc: 0.90922 |  0:01:24s\n",
      "epoch 72 | loss: 0.29131 | val_0_auc: 0.90906 |  0:01:25s\n",
      "epoch 73 | loss: 0.28931 | val_0_auc: 0.90858 |  0:01:26s\n",
      "epoch 74 | loss: 0.29347 | val_0_auc: 0.90841 |  0:01:27s\n",
      "epoch 75 | loss: 0.29034 | val_0_auc: 0.90823 |  0:01:29s\n",
      "epoch 76 | loss: 0.29141 | val_0_auc: 0.90853 |  0:01:30s\n",
      "epoch 77 | loss: 0.29005 | val_0_auc: 0.90921 |  0:01:31s\n",
      "epoch 78 | loss: 0.29048 | val_0_auc: 0.90989 |  0:01:32s\n",
      "epoch 79 | loss: 0.2888  | val_0_auc: 0.90854 |  0:01:33s\n",
      "epoch 80 | loss: 0.28893 | val_0_auc: 0.90846 |  0:01:34s\n",
      "epoch 81 | loss: 0.28912 | val_0_auc: 0.90936 |  0:01:35s\n",
      "epoch 82 | loss: 0.29061 | val_0_auc: 0.91023 |  0:01:36s\n",
      "epoch 83 | loss: 0.28747 | val_0_auc: 0.90848 |  0:01:37s\n",
      "epoch 84 | loss: 0.2855  | val_0_auc: 0.91011 |  0:01:38s\n",
      "epoch 85 | loss: 0.28527 | val_0_auc: 0.91058 |  0:01:39s\n",
      "epoch 86 | loss: 0.28606 | val_0_auc: 0.91156 |  0:01:40s\n",
      "epoch 87 | loss: 0.28775 | val_0_auc: 0.91089 |  0:01:42s\n",
      "epoch 88 | loss: 0.28694 | val_0_auc: 0.91169 |  0:01:43s\n",
      "epoch 89 | loss: 0.28607 | val_0_auc: 0.91186 |  0:01:44s\n",
      "epoch 90 | loss: 0.28458 | val_0_auc: 0.91191 |  0:01:46s\n",
      "epoch 91 | loss: 0.28562 | val_0_auc: 0.91187 |  0:01:47s\n",
      "epoch 92 | loss: 0.28273 | val_0_auc: 0.91118 |  0:01:49s\n",
      "epoch 93 | loss: 0.2834  | val_0_auc: 0.91092 |  0:01:51s\n",
      "epoch 94 | loss: 0.28521 | val_0_auc: 0.91181 |  0:01:53s\n",
      "epoch 95 | loss: 0.28333 | val_0_auc: 0.91145 |  0:01:54s\n",
      "epoch 96 | loss: 0.28421 | val_0_auc: 0.91105 |  0:01:55s\n",
      "epoch 97 | loss: 0.28356 | val_0_auc: 0.91258 |  0:01:56s\n",
      "epoch 98 | loss: 0.28549 | val_0_auc: 0.91271 |  0:01:57s\n",
      "epoch 99 | loss: 0.28325 | val_0_auc: 0.9125  |  0:01:58s\n",
      "epoch 100| loss: 0.28263 | val_0_auc: 0.91237 |  0:01:59s\n",
      "epoch 101| loss: 0.28065 | val_0_auc: 0.9129  |  0:02:00s\n",
      "epoch 102| loss: 0.2813  | val_0_auc: 0.91291 |  0:02:01s\n",
      "epoch 103| loss: 0.28248 | val_0_auc: 0.91311 |  0:02:02s\n",
      "epoch 104| loss: 0.27878 | val_0_auc: 0.91313 |  0:02:03s\n",
      "epoch 105| loss: 0.28167 | val_0_auc: 0.91151 |  0:02:05s\n",
      "epoch 106| loss: 0.28051 | val_0_auc: 0.91167 |  0:02:06s\n",
      "epoch 107| loss: 0.28085 | val_0_auc: 0.91228 |  0:02:07s\n",
      "epoch 108| loss: 0.28049 | val_0_auc: 0.9122  |  0:02:08s\n",
      "epoch 109| loss: 0.28211 | val_0_auc: 0.91194 |  0:02:09s\n",
      "epoch 110| loss: 0.28029 | val_0_auc: 0.91228 |  0:02:10s\n",
      "epoch 111| loss: 0.28107 | val_0_auc: 0.91298 |  0:02:11s\n",
      "epoch 112| loss: 0.27865 | val_0_auc: 0.91313 |  0:02:12s\n",
      "epoch 113| loss: 0.27842 | val_0_auc: 0.91278 |  0:02:13s\n",
      "epoch 114| loss: 0.27857 | val_0_auc: 0.91402 |  0:02:14s\n",
      "epoch 115| loss: 0.27923 | val_0_auc: 0.91307 |  0:02:15s\n",
      "epoch 116| loss: 0.27814 | val_0_auc: 0.91395 |  0:02:16s\n",
      "epoch 117| loss: 0.27744 | val_0_auc: 0.9144  |  0:02:17s\n",
      "epoch 118| loss: 0.27812 | val_0_auc: 0.91284 |  0:02:18s\n",
      "epoch 119| loss: 0.28465 | val_0_auc: 0.91286 |  0:02:19s\n",
      "epoch 120| loss: 0.28334 | val_0_auc: 0.91262 |  0:02:20s\n",
      "epoch 121| loss: 0.28034 | val_0_auc: 0.91084 |  0:02:22s\n",
      "epoch 122| loss: 0.28113 | val_0_auc: 0.91233 |  0:02:23s\n",
      "epoch 123| loss: 0.28162 | val_0_auc: 0.91322 |  0:02:24s\n",
      "epoch 124| loss: 0.27839 | val_0_auc: 0.91442 |  0:02:25s\n",
      "epoch 125| loss: 0.27883 | val_0_auc: 0.91278 |  0:02:26s\n",
      "epoch 126| loss: 0.27843 | val_0_auc: 0.91479 |  0:02:27s\n",
      "epoch 127| loss: 0.27753 | val_0_auc: 0.91586 |  0:02:28s\n",
      "epoch 128| loss: 0.27918 | val_0_auc: 0.91594 |  0:02:29s\n",
      "epoch 129| loss: 0.27883 | val_0_auc: 0.91529 |  0:02:30s\n",
      "epoch 130| loss: 0.27913 | val_0_auc: 0.91656 |  0:02:31s\n",
      "epoch 131| loss: 0.27598 | val_0_auc: 0.91748 |  0:02:32s\n",
      "epoch 132| loss: 0.27619 | val_0_auc: 0.91595 |  0:02:33s\n",
      "epoch 133| loss: 0.27881 | val_0_auc: 0.9173  |  0:02:34s\n",
      "epoch 134| loss: 0.27541 | val_0_auc: 0.9183  |  0:02:35s\n",
      "epoch 135| loss: 0.27696 | val_0_auc: 0.91893 |  0:02:36s\n",
      "epoch 136| loss: 0.27284 | val_0_auc: 0.91884 |  0:02:37s\n",
      "epoch 137| loss: 0.27566 | val_0_auc: 0.91797 |  0:02:38s\n",
      "epoch 138| loss: 0.27715 | val_0_auc: 0.91663 |  0:02:39s\n",
      "epoch 139| loss: 0.27979 | val_0_auc: 0.91825 |  0:02:40s\n",
      "epoch 140| loss: 0.27671 | val_0_auc: 0.91792 |  0:02:41s\n",
      "epoch 141| loss: 0.27812 | val_0_auc: 0.91861 |  0:02:42s\n",
      "epoch 142| loss: 0.27478 | val_0_auc: 0.9191  |  0:02:43s\n",
      "epoch 143| loss: 0.27612 | val_0_auc: 0.91906 |  0:02:44s\n",
      "epoch 144| loss: 0.27765 | val_0_auc: 0.9155  |  0:02:45s\n",
      "epoch 145| loss: 0.28056 | val_0_auc: 0.91653 |  0:02:46s\n",
      "epoch 146| loss: 0.27743 | val_0_auc: 0.91841 |  0:02:47s\n",
      "epoch 147| loss: 0.2761  | val_0_auc: 0.91823 |  0:02:48s\n",
      "epoch 148| loss: 0.27518 | val_0_auc: 0.91755 |  0:02:49s\n",
      "epoch 149| loss: 0.27594 | val_0_auc: 0.91606 |  0:02:51s\n",
      "epoch 150| loss: 0.27537 | val_0_auc: 0.91678 |  0:02:52s\n",
      "epoch 151| loss: 0.27569 | val_0_auc: 0.91591 |  0:02:53s\n",
      "epoch 152| loss: 0.27876 | val_0_auc: 0.91811 |  0:02:54s\n",
      "epoch 153| loss: 0.27298 | val_0_auc: 0.91777 |  0:02:55s\n",
      "epoch 154| loss: 0.27499 | val_0_auc: 0.9167  |  0:02:56s\n",
      "epoch 155| loss: 0.27266 | val_0_auc: 0.91759 |  0:02:57s\n",
      "epoch 156| loss: 0.27611 | val_0_auc: 0.91777 |  0:02:58s\n",
      "epoch 157| loss: 0.27445 | val_0_auc: 0.91735 |  0:02:59s\n",
      "epoch 158| loss: 0.27494 | val_0_auc: 0.91901 |  0:03:00s\n",
      "epoch 159| loss: 0.27308 | val_0_auc: 0.91892 |  0:03:01s\n",
      "epoch 160| loss: 0.27293 | val_0_auc: 0.91975 |  0:03:02s\n",
      "epoch 161| loss: 0.27425 | val_0_auc: 0.91719 |  0:03:03s\n",
      "epoch 162| loss: 0.27431 | val_0_auc: 0.91693 |  0:03:05s\n",
      "epoch 163| loss: 0.27476 | val_0_auc: 0.91502 |  0:03:06s\n",
      "epoch 164| loss: 0.27299 | val_0_auc: 0.91851 |  0:03:07s\n",
      "epoch 165| loss: 0.27078 | val_0_auc: 0.91832 |  0:03:08s\n",
      "epoch 166| loss: 0.27082 | val_0_auc: 0.91824 |  0:03:09s\n",
      "epoch 167| loss: 0.27064 | val_0_auc: 0.91878 |  0:03:10s\n",
      "epoch 168| loss: 0.26961 | val_0_auc: 0.91834 |  0:03:11s\n",
      "epoch 169| loss: 0.26975 | val_0_auc: 0.91804 |  0:03:12s\n",
      "epoch 170| loss: 0.27035 | val_0_auc: 0.91864 |  0:03:14s\n",
      "epoch 171| loss: 0.26828 | val_0_auc: 0.91799 |  0:03:16s\n",
      "epoch 172| loss: 0.26738 | val_0_auc: 0.91809 |  0:03:17s\n",
      "epoch 173| loss: 0.26934 | val_0_auc: 0.91779 |  0:03:18s\n",
      "epoch 174| loss: 0.26769 | val_0_auc: 0.91862 |  0:03:20s\n",
      "epoch 175| loss: 0.26981 | val_0_auc: 0.9143  |  0:03:21s\n",
      "epoch 176| loss: 0.27099 | val_0_auc: 0.91752 |  0:03:22s\n",
      "epoch 177| loss: 0.2668  | val_0_auc: 0.91843 |  0:03:24s\n",
      "epoch 178| loss: 0.26722 | val_0_auc: 0.91915 |  0:03:25s\n",
      "epoch 179| loss: 0.26779 | val_0_auc: 0.91969 |  0:03:27s\n",
      "epoch 180| loss: 0.26881 | val_0_auc: 0.92005 |  0:03:28s\n",
      "epoch 181| loss: 0.26673 | val_0_auc: 0.91906 |  0:03:29s\n",
      "epoch 182| loss: 0.26692 | val_0_auc: 0.91926 |  0:03:31s\n",
      "epoch 183| loss: 0.26661 | val_0_auc: 0.91927 |  0:03:33s\n",
      "epoch 184| loss: 0.26834 | val_0_auc: 0.91812 |  0:03:34s\n",
      "epoch 185| loss: 0.26731 | val_0_auc: 0.91931 |  0:03:35s\n",
      "epoch 186| loss: 0.26593 | val_0_auc: 0.91858 |  0:03:37s\n",
      "epoch 187| loss: 0.26566 | val_0_auc: 0.91899 |  0:03:38s\n",
      "epoch 188| loss: 0.26587 | val_0_auc: 0.91988 |  0:03:39s\n",
      "epoch 189| loss: 0.26871 | val_0_auc: 0.91759 |  0:03:40s\n",
      "epoch 190| loss: 0.27006 | val_0_auc: 0.91924 |  0:03:41s\n",
      "epoch 191| loss: 0.26676 | val_0_auc: 0.91914 |  0:03:42s\n",
      "epoch 192| loss: 0.2679  | val_0_auc: 0.91796 |  0:03:43s\n",
      "epoch 193| loss: 0.26776 | val_0_auc: 0.91881 |  0:03:44s\n",
      "epoch 194| loss: 0.26736 | val_0_auc: 0.91859 |  0:03:45s\n",
      "epoch 195| loss: 0.26579 | val_0_auc: 0.91829 |  0:03:47s\n",
      "epoch 196| loss: 0.26572 | val_0_auc: 0.91852 |  0:03:48s\n",
      "epoch 197| loss: 0.26661 | val_0_auc: 0.91875 |  0:03:49s\n",
      "epoch 198| loss: 0.26583 | val_0_auc: 0.91904 |  0:03:50s\n",
      "epoch 199| loss: 0.26414 | val_0_auc: 0.9184  |  0:03:51s\n",
      "epoch 200| loss: 0.26454 | val_0_auc: 0.91558 |  0:03:52s\n",
      "\n",
      "Early stopping occurred at epoch 200 with best_epoch = 180 and best_val_0_auc = 0.92005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.98839 | val_0_auc: 0.50235 |  0:00:04s\n",
      "epoch 1  | loss: 0.97351 | val_0_auc: 0.50342 |  0:00:08s\n",
      "epoch 2  | loss: 0.95647 | val_0_auc: 0.51008 |  0:00:12s\n",
      "epoch 3  | loss: 0.95587 | val_0_auc: 0.51594 |  0:00:17s\n",
      "epoch 4  | loss: 0.93887 | val_0_auc: 0.52231 |  0:00:21s\n",
      "epoch 5  | loss: 0.92761 | val_0_auc: 0.51354 |  0:00:26s\n",
      "epoch 6  | loss: 0.93799 | val_0_auc: 0.51445 |  0:00:30s\n",
      "epoch 7  | loss: 0.90871 | val_0_auc: 0.51735 |  0:00:34s\n",
      "epoch 8  | loss: 0.90749 | val_0_auc: 0.51877 |  0:00:38s\n",
      "epoch 9  | loss: 0.86537 | val_0_auc: 0.5159  |  0:00:41s\n",
      "epoch 10 | loss: 0.83822 | val_0_auc: 0.51968 |  0:00:45s\n",
      "epoch 11 | loss: 0.84041 | val_0_auc: 0.53443 |  0:00:49s\n",
      "epoch 12 | loss: 0.83432 | val_0_auc: 0.5378  |  0:00:52s\n",
      "epoch 13 | loss: 0.81391 | val_0_auc: 0.53494 |  0:00:57s\n",
      "epoch 14 | loss: 0.80203 | val_0_auc: 0.53625 |  0:01:00s\n",
      "epoch 15 | loss: 0.78542 | val_0_auc: 0.53886 |  0:01:04s\n",
      "epoch 16 | loss: 0.77448 | val_0_auc: 0.5332  |  0:01:07s\n",
      "epoch 17 | loss: 0.76277 | val_0_auc: 0.54088 |  0:01:12s\n",
      "epoch 18 | loss: 0.7614  | val_0_auc: 0.531   |  0:01:15s\n",
      "epoch 19 | loss: 0.75812 | val_0_auc: 0.54662 |  0:01:19s\n",
      "epoch 20 | loss: 0.76094 | val_0_auc: 0.54769 |  0:01:22s\n",
      "epoch 21 | loss: 0.74138 | val_0_auc: 0.55599 |  0:01:26s\n",
      "epoch 22 | loss: 0.72082 | val_0_auc: 0.55611 |  0:01:31s\n",
      "epoch 23 | loss: 0.71731 | val_0_auc: 0.5558  |  0:01:34s\n",
      "epoch 24 | loss: 0.70744 | val_0_auc: 0.55022 |  0:01:38s\n",
      "epoch 25 | loss: 0.69843 | val_0_auc: 0.56089 |  0:01:43s\n",
      "epoch 26 | loss: 0.6986  | val_0_auc: 0.55781 |  0:01:47s\n",
      "epoch 27 | loss: 0.68897 | val_0_auc: 0.56513 |  0:01:51s\n",
      "epoch 28 | loss: 0.68217 | val_0_auc: 0.56962 |  0:01:55s\n",
      "epoch 29 | loss: 0.69065 | val_0_auc: 0.56435 |  0:01:59s\n",
      "epoch 30 | loss: 0.6907  | val_0_auc: 0.56654 |  0:02:04s\n",
      "epoch 31 | loss: 0.6793  | val_0_auc: 0.58002 |  0:02:07s\n",
      "epoch 32 | loss: 0.66322 | val_0_auc: 0.57929 |  0:02:11s\n",
      "epoch 33 | loss: 0.66503 | val_0_auc: 0.58014 |  0:02:14s\n",
      "epoch 34 | loss: 0.65717 | val_0_auc: 0.5876  |  0:02:17s\n",
      "epoch 35 | loss: 0.65089 | val_0_auc: 0.57214 |  0:02:21s\n",
      "epoch 36 | loss: 0.64823 | val_0_auc: 0.58669 |  0:02:25s\n",
      "epoch 37 | loss: 0.6498  | val_0_auc: 0.58003 |  0:02:28s\n",
      "epoch 38 | loss: 0.6487  | val_0_auc: 0.59777 |  0:02:32s\n",
      "epoch 39 | loss: 0.64006 | val_0_auc: 0.57279 |  0:02:37s\n",
      "epoch 40 | loss: 0.63465 | val_0_auc: 0.60169 |  0:02:41s\n",
      "epoch 41 | loss: 0.63713 | val_0_auc: 0.58947 |  0:02:47s\n",
      "epoch 42 | loss: 0.62614 | val_0_auc: 0.58171 |  0:02:52s\n",
      "epoch 43 | loss: 0.62649 | val_0_auc: 0.60292 |  0:02:55s\n",
      "epoch 44 | loss: 0.6285  | val_0_auc: 0.59082 |  0:02:59s\n",
      "epoch 45 | loss: 0.62251 | val_0_auc: 0.59052 |  0:03:03s\n",
      "epoch 46 | loss: 0.62509 | val_0_auc: 0.6076  |  0:03:06s\n",
      "epoch 47 | loss: 0.61539 | val_0_auc: 0.59861 |  0:03:10s\n",
      "epoch 48 | loss: 0.61741 | val_0_auc: 0.59444 |  0:03:14s\n",
      "epoch 49 | loss: 0.61626 | val_0_auc: 0.60397 |  0:03:18s\n",
      "epoch 50 | loss: 0.60976 | val_0_auc: 0.60009 |  0:03:22s\n",
      "epoch 51 | loss: 0.60364 | val_0_auc: 0.61112 |  0:03:25s\n",
      "epoch 52 | loss: 0.60859 | val_0_auc: 0.60665 |  0:03:29s\n",
      "epoch 53 | loss: 0.59807 | val_0_auc: 0.60772 |  0:03:33s\n",
      "epoch 54 | loss: 0.60402 | val_0_auc: 0.62129 |  0:03:36s\n",
      "epoch 55 | loss: 0.59987 | val_0_auc: 0.61918 |  0:03:40s\n",
      "epoch 56 | loss: 0.59967 | val_0_auc: 0.59939 |  0:03:43s\n",
      "epoch 57 | loss: 0.60076 | val_0_auc: 0.61416 |  0:03:47s\n",
      "epoch 58 | loss: 0.59342 | val_0_auc: 0.59561 |  0:03:50s\n",
      "epoch 59 | loss: 0.59508 | val_0_auc: 0.61741 |  0:03:54s\n",
      "epoch 60 | loss: 0.59242 | val_0_auc: 0.61637 |  0:03:57s\n",
      "epoch 61 | loss: 0.59145 | val_0_auc: 0.61113 |  0:04:02s\n",
      "epoch 62 | loss: 0.5915  | val_0_auc: 0.64254 |  0:04:05s\n",
      "epoch 63 | loss: 0.5881  | val_0_auc: 0.62484 |  0:04:09s\n",
      "epoch 64 | loss: 0.58955 | val_0_auc: 0.6308  |  0:04:12s\n",
      "epoch 65 | loss: 0.58904 | val_0_auc: 0.63371 |  0:04:16s\n",
      "epoch 66 | loss: 0.58738 | val_0_auc: 0.64835 |  0:04:19s\n",
      "epoch 67 | loss: 0.58549 | val_0_auc: 0.63288 |  0:04:23s\n",
      "epoch 68 | loss: 0.58065 | val_0_auc: 0.6355  |  0:04:26s\n",
      "epoch 69 | loss: 0.58359 | val_0_auc: 0.63812 |  0:04:30s\n",
      "epoch 70 | loss: 0.58015 | val_0_auc: 0.65018 |  0:04:33s\n",
      "epoch 71 | loss: 0.57695 | val_0_auc: 0.65099 |  0:04:36s\n",
      "epoch 72 | loss: 0.57647 | val_0_auc: 0.66723 |  0:04:40s\n",
      "epoch 73 | loss: 0.5779  | val_0_auc: 0.67548 |  0:04:43s\n",
      "epoch 74 | loss: 0.57683 | val_0_auc: 0.67053 |  0:04:46s\n",
      "epoch 75 | loss: 0.57227 | val_0_auc: 0.68074 |  0:04:49s\n",
      "epoch 76 | loss: 0.57115 | val_0_auc: 0.66197 |  0:04:53s\n",
      "epoch 77 | loss: 0.56891 | val_0_auc: 0.67614 |  0:04:56s\n",
      "epoch 78 | loss: 0.56894 | val_0_auc: 0.6716  |  0:05:00s\n",
      "epoch 79 | loss: 0.56703 | val_0_auc: 0.68169 |  0:05:03s\n",
      "epoch 80 | loss: 0.56943 | val_0_auc: 0.66785 |  0:05:06s\n",
      "epoch 81 | loss: 0.57048 | val_0_auc: 0.70143 |  0:05:09s\n",
      "epoch 82 | loss: 0.56998 | val_0_auc: 0.66811 |  0:05:14s\n",
      "epoch 83 | loss: 0.57042 | val_0_auc: 0.67356 |  0:05:17s\n",
      "epoch 84 | loss: 0.57096 | val_0_auc: 0.67389 |  0:05:20s\n",
      "epoch 85 | loss: 0.56903 | val_0_auc: 0.69934 |  0:05:24s\n",
      "epoch 86 | loss: 0.56705 | val_0_auc: 0.70514 |  0:05:27s\n",
      "epoch 87 | loss: 0.56766 | val_0_auc: 0.70578 |  0:05:30s\n",
      "epoch 88 | loss: 0.56266 | val_0_auc: 0.68998 |  0:05:33s\n",
      "epoch 89 | loss: 0.56314 | val_0_auc: 0.69098 |  0:05:37s\n",
      "epoch 90 | loss: 0.56346 | val_0_auc: 0.70048 |  0:05:41s\n",
      "epoch 91 | loss: 0.56032 | val_0_auc: 0.69526 |  0:05:44s\n",
      "epoch 92 | loss: 0.56044 | val_0_auc: 0.69447 |  0:05:47s\n",
      "epoch 93 | loss: 0.55956 | val_0_auc: 0.70175 |  0:05:51s\n",
      "epoch 94 | loss: 0.56378 | val_0_auc: 0.70256 |  0:05:54s\n",
      "epoch 95 | loss: 0.55846 | val_0_auc: 0.72264 |  0:05:58s\n",
      "epoch 96 | loss: 0.5604  | val_0_auc: 0.71847 |  0:06:01s\n",
      "epoch 97 | loss: 0.55858 | val_0_auc: 0.6983  |  0:06:05s\n",
      "epoch 98 | loss: 0.55778 | val_0_auc: 0.6931  |  0:06:08s\n",
      "epoch 99 | loss: 0.55695 | val_0_auc: 0.71967 |  0:06:12s\n",
      "epoch 100| loss: 0.55973 | val_0_auc: 0.70089 |  0:06:16s\n",
      "epoch 101| loss: 0.55611 | val_0_auc: 0.69844 |  0:06:19s\n",
      "epoch 102| loss: 0.55477 | val_0_auc: 0.70596 |  0:06:23s\n",
      "epoch 103| loss: 0.55414 | val_0_auc: 0.71457 |  0:06:26s\n",
      "epoch 104| loss: 0.55602 | val_0_auc: 0.70632 |  0:06:31s\n",
      "epoch 105| loss: 0.55337 | val_0_auc: 0.7013  |  0:06:35s\n",
      "epoch 106| loss: 0.55447 | val_0_auc: 0.72136 |  0:06:38s\n",
      "epoch 107| loss: 0.55525 | val_0_auc: 0.69871 |  0:06:42s\n",
      "epoch 108| loss: 0.55703 | val_0_auc: 0.71525 |  0:06:45s\n",
      "epoch 109| loss: 0.55716 | val_0_auc: 0.71049 |  0:06:49s\n",
      "epoch 110| loss: 0.55328 | val_0_auc: 0.70879 |  0:06:52s\n",
      "epoch 111| loss: 0.55543 | val_0_auc: 0.70848 |  0:06:56s\n",
      "epoch 112| loss: 0.55461 | val_0_auc: 0.72308 |  0:07:01s\n",
      "epoch 113| loss: 0.55394 | val_0_auc: 0.71651 |  0:07:06s\n",
      "epoch 114| loss: 0.55161 | val_0_auc: 0.71425 |  0:07:10s\n",
      "epoch 115| loss: 0.55242 | val_0_auc: 0.73244 |  0:07:14s\n",
      "epoch 116| loss: 0.55164 | val_0_auc: 0.71907 |  0:07:18s\n",
      "epoch 117| loss: 0.55196 | val_0_auc: 0.71401 |  0:07:21s\n",
      "epoch 118| loss: 0.55002 | val_0_auc: 0.71133 |  0:07:24s\n",
      "epoch 119| loss: 0.55078 | val_0_auc: 0.72091 |  0:07:28s\n",
      "epoch 120| loss: 0.55107 | val_0_auc: 0.72196 |  0:07:32s\n",
      "epoch 121| loss: 0.55199 | val_0_auc: 0.72472 |  0:07:35s\n",
      "epoch 122| loss: 0.55516 | val_0_auc: 0.70691 |  0:07:39s\n",
      "epoch 123| loss: 0.55148 | val_0_auc: 0.72344 |  0:07:42s\n",
      "epoch 124| loss: 0.54948 | val_0_auc: 0.71855 |  0:07:46s\n",
      "epoch 125| loss: 0.55337 | val_0_auc: 0.71253 |  0:07:49s\n",
      "epoch 126| loss: 0.5518  | val_0_auc: 0.71467 |  0:07:53s\n",
      "epoch 127| loss: 0.55059 | val_0_auc: 0.70752 |  0:07:57s\n",
      "epoch 128| loss: 0.55136 | val_0_auc: 0.71389 |  0:08:01s\n",
      "epoch 129| loss: 0.54723 | val_0_auc: 0.71992 |  0:08:05s\n",
      "epoch 130| loss: 0.54854 | val_0_auc: 0.72882 |  0:08:11s\n",
      "epoch 131| loss: 0.54987 | val_0_auc: 0.71767 |  0:08:16s\n",
      "epoch 132| loss: 0.54613 | val_0_auc: 0.72843 |  0:08:24s\n",
      "epoch 133| loss: 0.54537 | val_0_auc: 0.72657 |  0:08:30s\n",
      "epoch 134| loss: 0.54386 | val_0_auc: 0.71803 |  0:08:36s\n",
      "epoch 135| loss: 0.54584 | val_0_auc: 0.72546 |  0:08:39s\n",
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 115 and best_val_0_auc = 0.73244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6571  | val_0_auc: 0.48986 |  0:00:03s\n",
      "epoch 1  | loss: 0.63144 | val_0_auc: 0.52063 |  0:00:05s\n",
      "epoch 2  | loss: 0.6002  | val_0_auc: 0.54653 |  0:00:06s\n",
      "epoch 3  | loss: 0.58224 | val_0_auc: 0.57009 |  0:00:08s\n",
      "epoch 4  | loss: 0.56427 | val_0_auc: 0.60184 |  0:00:10s\n",
      "epoch 5  | loss: 0.55461 | val_0_auc: 0.62778 |  0:00:11s\n",
      "epoch 6  | loss: 0.53513 | val_0_auc: 0.65105 |  0:00:13s\n",
      "epoch 7  | loss: 0.52787 | val_0_auc: 0.67637 |  0:00:15s\n",
      "epoch 8  | loss: 0.51685 | val_0_auc: 0.69639 |  0:00:16s\n",
      "epoch 9  | loss: 0.50561 | val_0_auc: 0.71872 |  0:00:18s\n",
      "epoch 10 | loss: 0.49821 | val_0_auc: 0.73262 |  0:00:19s\n",
      "epoch 11 | loss: 0.49233 | val_0_auc: 0.74228 |  0:00:21s\n",
      "epoch 12 | loss: 0.48104 | val_0_auc: 0.75438 |  0:00:22s\n",
      "epoch 13 | loss: 0.47442 | val_0_auc: 0.7668  |  0:00:24s\n",
      "epoch 14 | loss: 0.4699  | val_0_auc: 0.77546 |  0:00:26s\n",
      "epoch 15 | loss: 0.46293 | val_0_auc: 0.77839 |  0:00:30s\n",
      "epoch 16 | loss: 0.46081 | val_0_auc: 0.785   |  0:00:32s\n",
      "epoch 17 | loss: 0.45508 | val_0_auc: 0.78983 |  0:00:34s\n",
      "epoch 18 | loss: 0.44967 | val_0_auc: 0.79062 |  0:00:36s\n",
      "epoch 19 | loss: 0.4456  | val_0_auc: 0.79403 |  0:00:37s\n",
      "epoch 20 | loss: 0.44007 | val_0_auc: 0.79589 |  0:00:38s\n",
      "epoch 21 | loss: 0.43872 | val_0_auc: 0.79994 |  0:00:40s\n",
      "epoch 22 | loss: 0.43587 | val_0_auc: 0.80335 |  0:00:42s\n",
      "epoch 23 | loss: 0.43152 | val_0_auc: 0.80687 |  0:00:43s\n",
      "epoch 24 | loss: 0.43151 | val_0_auc: 0.80787 |  0:00:45s\n",
      "epoch 25 | loss: 0.42751 | val_0_auc: 0.80944 |  0:00:47s\n",
      "epoch 26 | loss: 0.42496 | val_0_auc: 0.81043 |  0:00:49s\n",
      "epoch 27 | loss: 0.42514 | val_0_auc: 0.81112 |  0:00:53s\n",
      "epoch 28 | loss: 0.42205 | val_0_auc: 0.81321 |  0:00:56s\n",
      "epoch 29 | loss: 0.41624 | val_0_auc: 0.8192  |  0:00:58s\n",
      "epoch 30 | loss: 0.41829 | val_0_auc: 0.82015 |  0:01:00s\n",
      "epoch 31 | loss: 0.4122  | val_0_auc: 0.82137 |  0:01:02s\n",
      "epoch 32 | loss: 0.41143 | val_0_auc: 0.8244  |  0:01:04s\n",
      "epoch 33 | loss: 0.4098  | val_0_auc: 0.82482 |  0:01:05s\n",
      "epoch 34 | loss: 0.40612 | val_0_auc: 0.82629 |  0:01:06s\n",
      "epoch 35 | loss: 0.40845 | val_0_auc: 0.82677 |  0:01:08s\n",
      "epoch 36 | loss: 0.40633 | val_0_auc: 0.82945 |  0:01:09s\n",
      "epoch 37 | loss: 0.40199 | val_0_auc: 0.82996 |  0:01:11s\n",
      "epoch 38 | loss: 0.40176 | val_0_auc: 0.83087 |  0:01:12s\n",
      "epoch 39 | loss: 0.39963 | val_0_auc: 0.8334  |  0:01:14s\n",
      "epoch 40 | loss: 0.39917 | val_0_auc: 0.83451 |  0:01:15s\n",
      "epoch 41 | loss: 0.40034 | val_0_auc: 0.83474 |  0:01:18s\n",
      "epoch 42 | loss: 0.39854 | val_0_auc: 0.83554 |  0:01:19s\n",
      "epoch 43 | loss: 0.39734 | val_0_auc: 0.83677 |  0:01:21s\n",
      "epoch 44 | loss: 0.39675 | val_0_auc: 0.83835 |  0:01:23s\n",
      "epoch 45 | loss: 0.39349 | val_0_auc: 0.83862 |  0:01:25s\n",
      "epoch 46 | loss: 0.39727 | val_0_auc: 0.83919 |  0:01:27s\n",
      "epoch 47 | loss: 0.39034 | val_0_auc: 0.83958 |  0:01:29s\n",
      "epoch 48 | loss: 0.39188 | val_0_auc: 0.84041 |  0:01:32s\n",
      "epoch 49 | loss: 0.38943 | val_0_auc: 0.84154 |  0:01:35s\n",
      "epoch 50 | loss: 0.389   | val_0_auc: 0.8424  |  0:01:38s\n",
      "epoch 51 | loss: 0.38994 | val_0_auc: 0.84303 |  0:01:40s\n",
      "epoch 52 | loss: 0.38849 | val_0_auc: 0.84394 |  0:01:42s\n",
      "epoch 53 | loss: 0.38689 | val_0_auc: 0.84401 |  0:01:48s\n",
      "epoch 54 | loss: 0.38724 | val_0_auc: 0.84428 |  0:01:54s\n",
      "epoch 55 | loss: 0.38454 | val_0_auc: 0.8456  |  0:01:56s\n",
      "epoch 56 | loss: 0.38674 | val_0_auc: 0.84619 |  0:01:58s\n",
      "epoch 57 | loss: 0.38452 | val_0_auc: 0.84681 |  0:02:00s\n",
      "epoch 58 | loss: 0.38371 | val_0_auc: 0.84843 |  0:02:03s\n",
      "epoch 59 | loss: 0.3848  | val_0_auc: 0.84897 |  0:02:05s\n",
      "epoch 60 | loss: 0.38063 | val_0_auc: 0.8506  |  0:02:06s\n",
      "epoch 61 | loss: 0.38249 | val_0_auc: 0.85112 |  0:02:08s\n",
      "epoch 62 | loss: 0.3803  | val_0_auc: 0.84996 |  0:02:10s\n",
      "epoch 63 | loss: 0.3767  | val_0_auc: 0.85127 |  0:02:12s\n",
      "epoch 64 | loss: 0.37837 | val_0_auc: 0.85081 |  0:02:14s\n",
      "epoch 65 | loss: 0.37646 | val_0_auc: 0.8503  |  0:02:16s\n",
      "epoch 66 | loss: 0.37595 | val_0_auc: 0.84985 |  0:02:18s\n",
      "epoch 67 | loss: 0.37559 | val_0_auc: 0.85072 |  0:02:19s\n",
      "epoch 68 | loss: 0.37355 | val_0_auc: 0.85168 |  0:02:22s\n",
      "epoch 69 | loss: 0.37542 | val_0_auc: 0.84965 |  0:02:23s\n",
      "epoch 70 | loss: 0.37384 | val_0_auc: 0.84976 |  0:02:25s\n",
      "epoch 71 | loss: 0.37582 | val_0_auc: 0.84992 |  0:02:26s\n",
      "epoch 72 | loss: 0.37198 | val_0_auc: 0.85138 |  0:02:28s\n",
      "epoch 73 | loss: 0.3737  | val_0_auc: 0.85238 |  0:02:30s\n",
      "epoch 74 | loss: 0.37154 | val_0_auc: 0.85331 |  0:02:32s\n",
      "epoch 75 | loss: 0.37025 | val_0_auc: 0.85458 |  0:02:35s\n",
      "epoch 76 | loss: 0.37027 | val_0_auc: 0.85491 |  0:02:37s\n",
      "epoch 77 | loss: 0.37333 | val_0_auc: 0.85518 |  0:02:40s\n",
      "epoch 78 | loss: 0.37064 | val_0_auc: 0.85614 |  0:02:42s\n",
      "epoch 79 | loss: 0.36907 | val_0_auc: 0.85596 |  0:02:44s\n",
      "epoch 80 | loss: 0.36793 | val_0_auc: 0.85649 |  0:02:46s\n",
      "epoch 81 | loss: 0.36831 | val_0_auc: 0.85628 |  0:02:47s\n",
      "epoch 82 | loss: 0.36789 | val_0_auc: 0.85672 |  0:02:49s\n",
      "epoch 83 | loss: 0.36755 | val_0_auc: 0.85646 |  0:02:50s\n",
      "epoch 84 | loss: 0.36717 | val_0_auc: 0.85685 |  0:02:52s\n",
      "epoch 85 | loss: 0.3669  | val_0_auc: 0.85719 |  0:02:54s\n",
      "epoch 86 | loss: 0.36468 | val_0_auc: 0.85741 |  0:02:55s\n",
      "epoch 87 | loss: 0.36727 | val_0_auc: 0.85842 |  0:02:56s\n",
      "epoch 88 | loss: 0.36493 | val_0_auc: 0.85818 |  0:02:58s\n",
      "epoch 89 | loss: 0.36285 | val_0_auc: 0.85852 |  0:02:59s\n",
      "epoch 90 | loss: 0.36489 | val_0_auc: 0.8594  |  0:03:01s\n",
      "epoch 91 | loss: 0.36378 | val_0_auc: 0.86003 |  0:03:02s\n",
      "epoch 92 | loss: 0.3618  | val_0_auc: 0.86124 |  0:03:03s\n",
      "epoch 93 | loss: 0.36043 | val_0_auc: 0.86088 |  0:03:05s\n",
      "epoch 94 | loss: 0.35889 | val_0_auc: 0.86052 |  0:03:09s\n",
      "epoch 95 | loss: 0.36239 | val_0_auc: 0.8613  |  0:03:12s\n",
      "epoch 96 | loss: 0.36232 | val_0_auc: 0.8619  |  0:03:16s\n",
      "epoch 97 | loss: 0.36048 | val_0_auc: 0.86273 |  0:03:18s\n",
      "epoch 98 | loss: 0.35873 | val_0_auc: 0.86288 |  0:03:21s\n",
      "epoch 99 | loss: 0.35847 | val_0_auc: 0.86198 |  0:03:23s\n",
      "epoch 100| loss: 0.35793 | val_0_auc: 0.86091 |  0:03:25s\n",
      "epoch 101| loss: 0.35852 | val_0_auc: 0.86172 |  0:03:26s\n",
      "epoch 102| loss: 0.35738 | val_0_auc: 0.8614  |  0:03:27s\n",
      "epoch 103| loss: 0.35859 | val_0_auc: 0.86101 |  0:03:29s\n",
      "epoch 104| loss: 0.35654 | val_0_auc: 0.86251 |  0:03:30s\n",
      "epoch 105| loss: 0.3554  | val_0_auc: 0.86339 |  0:03:33s\n",
      "epoch 106| loss: 0.35661 | val_0_auc: 0.86342 |  0:03:35s\n",
      "epoch 107| loss: 0.35343 | val_0_auc: 0.86352 |  0:03:37s\n",
      "epoch 108| loss: 0.35448 | val_0_auc: 0.86384 |  0:03:39s\n",
      "epoch 109| loss: 0.35391 | val_0_auc: 0.86338 |  0:03:41s\n",
      "epoch 110| loss: 0.35438 | val_0_auc: 0.86313 |  0:03:43s\n",
      "epoch 111| loss: 0.35324 | val_0_auc: 0.86317 |  0:03:44s\n",
      "epoch 112| loss: 0.35362 | val_0_auc: 0.86457 |  0:03:45s\n",
      "epoch 113| loss: 0.35318 | val_0_auc: 0.86387 |  0:03:47s\n",
      "epoch 114| loss: 0.35378 | val_0_auc: 0.86388 |  0:03:49s\n",
      "epoch 115| loss: 0.3521  | val_0_auc: 0.86431 |  0:03:50s\n",
      "epoch 116| loss: 0.35226 | val_0_auc: 0.86561 |  0:03:53s\n",
      "epoch 117| loss: 0.35209 | val_0_auc: 0.86607 |  0:03:55s\n",
      "epoch 118| loss: 0.35091 | val_0_auc: 0.86674 |  0:03:58s\n",
      "epoch 119| loss: 0.35135 | val_0_auc: 0.86679 |  0:04:00s\n",
      "epoch 120| loss: 0.35095 | val_0_auc: 0.86645 |  0:04:03s\n",
      "epoch 121| loss: 0.35056 | val_0_auc: 0.86725 |  0:04:05s\n",
      "epoch 122| loss: 0.34913 | val_0_auc: 0.8678  |  0:04:07s\n",
      "epoch 123| loss: 0.35035 | val_0_auc: 0.86815 |  0:04:09s\n",
      "epoch 124| loss: 0.34687 | val_0_auc: 0.86761 |  0:04:11s\n",
      "epoch 125| loss: 0.34625 | val_0_auc: 0.86794 |  0:04:12s\n",
      "epoch 126| loss: 0.34626 | val_0_auc: 0.86736 |  0:04:14s\n",
      "epoch 127| loss: 0.34909 | val_0_auc: 0.8673  |  0:04:16s\n",
      "epoch 128| loss: 0.3476  | val_0_auc: 0.86658 |  0:04:18s\n",
      "epoch 129| loss: 0.34746 | val_0_auc: 0.8674  |  0:04:19s\n",
      "epoch 130| loss: 0.34925 | val_0_auc: 0.86729 |  0:04:21s\n",
      "epoch 131| loss: 0.34502 | val_0_auc: 0.86764 |  0:04:23s\n",
      "epoch 132| loss: 0.34625 | val_0_auc: 0.86783 |  0:04:25s\n",
      "epoch 133| loss: 0.34454 | val_0_auc: 0.86771 |  0:04:27s\n",
      "epoch 134| loss: 0.34517 | val_0_auc: 0.86775 |  0:04:28s\n",
      "epoch 135| loss: 0.34583 | val_0_auc: 0.86838 |  0:04:30s\n",
      "epoch 136| loss: 0.34382 | val_0_auc: 0.86811 |  0:04:33s\n",
      "epoch 137| loss: 0.34383 | val_0_auc: 0.86803 |  0:04:35s\n",
      "epoch 138| loss: 0.34273 | val_0_auc: 0.8679  |  0:04:36s\n",
      "epoch 139| loss: 0.34316 | val_0_auc: 0.86752 |  0:04:37s\n",
      "epoch 140| loss: 0.34411 | val_0_auc: 0.86772 |  0:04:39s\n",
      "epoch 141| loss: 0.34276 | val_0_auc: 0.86709 |  0:04:40s\n",
      "epoch 142| loss: 0.34226 | val_0_auc: 0.86705 |  0:04:42s\n",
      "epoch 143| loss: 0.34178 | val_0_auc: 0.86704 |  0:04:43s\n",
      "epoch 144| loss: 0.34113 | val_0_auc: 0.8674  |  0:04:45s\n",
      "epoch 145| loss: 0.34057 | val_0_auc: 0.86779 |  0:04:47s\n",
      "epoch 146| loss: 0.34048 | val_0_auc: 0.86762 |  0:04:50s\n",
      "epoch 147| loss: 0.34061 | val_0_auc: 0.86694 |  0:04:51s\n",
      "epoch 148| loss: 0.33906 | val_0_auc: 0.86669 |  0:04:53s\n",
      "epoch 149| loss: 0.34005 | val_0_auc: 0.86727 |  0:04:55s\n",
      "epoch 150| loss: 0.33817 | val_0_auc: 0.86758 |  0:04:56s\n",
      "epoch 151| loss: 0.33837 | val_0_auc: 0.8678  |  0:04:57s\n",
      "epoch 152| loss: 0.33871 | val_0_auc: 0.86744 |  0:04:59s\n",
      "epoch 153| loss: 0.33649 | val_0_auc: 0.86772 |  0:05:00s\n",
      "epoch 154| loss: 0.33738 | val_0_auc: 0.86672 |  0:05:03s\n",
      "epoch 155| loss: 0.33784 | val_0_auc: 0.86847 |  0:05:04s\n",
      "epoch 156| loss: 0.33675 | val_0_auc: 0.8694  |  0:05:07s\n",
      "epoch 157| loss: 0.3387  | val_0_auc: 0.86875 |  0:05:09s\n",
      "epoch 158| loss: 0.3353  | val_0_auc: 0.86801 |  0:05:13s\n",
      "epoch 159| loss: 0.33894 | val_0_auc: 0.86924 |  0:05:15s\n",
      "epoch 160| loss: 0.33659 | val_0_auc: 0.86927 |  0:05:17s\n",
      "epoch 161| loss: 0.33363 | val_0_auc: 0.86939 |  0:05:18s\n",
      "epoch 162| loss: 0.33515 | val_0_auc: 0.8694  |  0:05:20s\n",
      "epoch 163| loss: 0.33623 | val_0_auc: 0.86893 |  0:05:21s\n",
      "epoch 164| loss: 0.3325  | val_0_auc: 0.86942 |  0:05:23s\n",
      "epoch 165| loss: 0.33361 | val_0_auc: 0.86965 |  0:05:25s\n",
      "epoch 166| loss: 0.33659 | val_0_auc: 0.87002 |  0:05:27s\n",
      "epoch 167| loss: 0.33501 | val_0_auc: 0.87059 |  0:05:29s\n",
      "epoch 168| loss: 0.3325  | val_0_auc: 0.87046 |  0:05:31s\n",
      "epoch 169| loss: 0.3326  | val_0_auc: 0.87015 |  0:05:32s\n",
      "epoch 170| loss: 0.33233 | val_0_auc: 0.86998 |  0:05:34s\n",
      "epoch 171| loss: 0.33233 | val_0_auc: 0.87076 |  0:05:36s\n",
      "epoch 172| loss: 0.33302 | val_0_auc: 0.87202 |  0:05:37s\n",
      "epoch 173| loss: 0.33341 | val_0_auc: 0.87044 |  0:05:39s\n",
      "epoch 174| loss: 0.33294 | val_0_auc: 0.87096 |  0:05:41s\n",
      "epoch 175| loss: 0.32998 | val_0_auc: 0.87226 |  0:05:43s\n",
      "epoch 176| loss: 0.32914 | val_0_auc: 0.8717  |  0:05:48s\n",
      "epoch 177| loss: 0.3325  | val_0_auc: 0.87196 |  0:05:50s\n",
      "epoch 178| loss: 0.33056 | val_0_auc: 0.87164 |  0:05:51s\n",
      "epoch 179| loss: 0.33095 | val_0_auc: 0.87129 |  0:05:53s\n",
      "epoch 180| loss: 0.32943 | val_0_auc: 0.87148 |  0:05:54s\n",
      "epoch 181| loss: 0.33153 | val_0_auc: 0.87143 |  0:05:56s\n",
      "epoch 182| loss: 0.33055 | val_0_auc: 0.87196 |  0:05:57s\n",
      "epoch 183| loss: 0.33064 | val_0_auc: 0.87207 |  0:05:58s\n",
      "epoch 184| loss: 0.32879 | val_0_auc: 0.87241 |  0:06:00s\n",
      "epoch 185| loss: 0.33018 | val_0_auc: 0.87244 |  0:06:01s\n",
      "epoch 186| loss: 0.33002 | val_0_auc: 0.87243 |  0:06:02s\n",
      "epoch 187| loss: 0.32897 | val_0_auc: 0.87349 |  0:06:04s\n",
      "epoch 188| loss: 0.32908 | val_0_auc: 0.87369 |  0:06:05s\n",
      "epoch 189| loss: 0.32567 | val_0_auc: 0.87271 |  0:06:07s\n",
      "epoch 190| loss: 0.32591 | val_0_auc: 0.87235 |  0:06:08s\n",
      "epoch 191| loss: 0.32832 | val_0_auc: 0.87217 |  0:06:09s\n",
      "epoch 192| loss: 0.32546 | val_0_auc: 0.87234 |  0:06:11s\n",
      "epoch 193| loss: 0.32496 | val_0_auc: 0.8725  |  0:06:12s\n",
      "epoch 194| loss: 0.32494 | val_0_auc: 0.87293 |  0:06:13s\n",
      "epoch 195| loss: 0.32703 | val_0_auc: 0.87349 |  0:06:15s\n",
      "epoch 196| loss: 0.3242  | val_0_auc: 0.87327 |  0:06:16s\n",
      "epoch 197| loss: 0.32396 | val_0_auc: 0.87369 |  0:06:18s\n",
      "epoch 198| loss: 0.32586 | val_0_auc: 0.87373 |  0:06:19s\n",
      "epoch 199| loss: 0.3233  | val_0_auc: 0.87367 |  0:06:20s\n",
      "epoch 200| loss: 0.32472 | val_0_auc: 0.87417 |  0:06:22s\n",
      "epoch 201| loss: 0.32308 | val_0_auc: 0.87407 |  0:06:23s\n",
      "epoch 202| loss: 0.32293 | val_0_auc: 0.87375 |  0:06:24s\n",
      "epoch 203| loss: 0.3222  | val_0_auc: 0.87391 |  0:06:26s\n",
      "epoch 204| loss: 0.32326 | val_0_auc: 0.87432 |  0:06:27s\n",
      "epoch 205| loss: 0.3212  | val_0_auc: 0.8744  |  0:06:29s\n",
      "epoch 206| loss: 0.32385 | val_0_auc: 0.8743  |  0:06:31s\n",
      "epoch 207| loss: 0.32089 | val_0_auc: 0.8752  |  0:06:32s\n",
      "epoch 208| loss: 0.32242 | val_0_auc: 0.8749  |  0:06:34s\n",
      "epoch 209| loss: 0.32124 | val_0_auc: 0.87587 |  0:06:35s\n",
      "epoch 210| loss: 0.32179 | val_0_auc: 0.87563 |  0:06:36s\n",
      "epoch 211| loss: 0.32062 | val_0_auc: 0.87517 |  0:06:38s\n",
      "epoch 212| loss: 0.32096 | val_0_auc: 0.87537 |  0:06:39s\n",
      "epoch 213| loss: 0.32027 | val_0_auc: 0.87592 |  0:06:40s\n",
      "epoch 214| loss: 0.31883 | val_0_auc: 0.87598 |  0:06:42s\n",
      "epoch 215| loss: 0.31927 | val_0_auc: 0.87597 |  0:06:43s\n",
      "epoch 216| loss: 0.31906 | val_0_auc: 0.87649 |  0:06:45s\n",
      "epoch 217| loss: 0.31902 | val_0_auc: 0.87617 |  0:06:46s\n",
      "epoch 218| loss: 0.31976 | val_0_auc: 0.87656 |  0:06:47s\n",
      "epoch 219| loss: 0.31929 | val_0_auc: 0.87503 |  0:06:49s\n",
      "epoch 220| loss: 0.31883 | val_0_auc: 0.87575 |  0:06:50s\n",
      "epoch 221| loss: 0.32015 | val_0_auc: 0.87654 |  0:06:51s\n",
      "epoch 222| loss: 0.31887 | val_0_auc: 0.8766  |  0:06:53s\n",
      "epoch 223| loss: 0.31545 | val_0_auc: 0.87656 |  0:06:54s\n",
      "epoch 224| loss: 0.31821 | val_0_auc: 0.87597 |  0:06:56s\n",
      "epoch 225| loss: 0.31709 | val_0_auc: 0.87633 |  0:06:57s\n",
      "epoch 226| loss: 0.31548 | val_0_auc: 0.87623 |  0:06:58s\n",
      "epoch 227| loss: 0.31616 | val_0_auc: 0.87693 |  0:07:00s\n",
      "epoch 228| loss: 0.31708 | val_0_auc: 0.87846 |  0:07:01s\n",
      "epoch 229| loss: 0.31672 | val_0_auc: 0.87827 |  0:07:03s\n",
      "epoch 230| loss: 0.31507 | val_0_auc: 0.87856 |  0:07:04s\n",
      "epoch 231| loss: 0.31521 | val_0_auc: 0.87887 |  0:07:05s\n",
      "epoch 232| loss: 0.3142  | val_0_auc: 0.8783  |  0:07:07s\n",
      "epoch 233| loss: 0.31251 | val_0_auc: 0.87744 |  0:07:08s\n",
      "epoch 234| loss: 0.31391 | val_0_auc: 0.87887 |  0:07:10s\n",
      "epoch 235| loss: 0.31559 | val_0_auc: 0.87819 |  0:07:11s\n",
      "epoch 236| loss: 0.31301 | val_0_auc: 0.87834 |  0:07:12s\n",
      "epoch 237| loss: 0.31306 | val_0_auc: 0.87914 |  0:07:14s\n",
      "epoch 238| loss: 0.31135 | val_0_auc: 0.87923 |  0:07:15s\n",
      "epoch 239| loss: 0.31374 | val_0_auc: 0.87884 |  0:07:16s\n",
      "epoch 240| loss: 0.31064 | val_0_auc: 0.87989 |  0:07:18s\n",
      "epoch 241| loss: 0.31204 | val_0_auc: 0.8799  |  0:07:19s\n",
      "epoch 242| loss: 0.31409 | val_0_auc: 0.88025 |  0:07:20s\n",
      "epoch 243| loss: 0.31126 | val_0_auc: 0.88092 |  0:07:22s\n",
      "epoch 244| loss: 0.31172 | val_0_auc: 0.88054 |  0:07:23s\n",
      "epoch 245| loss: 0.31165 | val_0_auc: 0.88084 |  0:07:25s\n",
      "epoch 246| loss: 0.3115  | val_0_auc: 0.88135 |  0:07:26s\n",
      "epoch 247| loss: 0.31093 | val_0_auc: 0.88044 |  0:07:27s\n",
      "epoch 248| loss: 0.30979 | val_0_auc: 0.88106 |  0:07:29s\n",
      "epoch 249| loss: 0.3097  | val_0_auc: 0.882   |  0:07:30s\n",
      "epoch 250| loss: 0.31118 | val_0_auc: 0.88171 |  0:07:31s\n",
      "epoch 251| loss: 0.30923 | val_0_auc: 0.88014 |  0:07:33s\n",
      "epoch 252| loss: 0.31214 | val_0_auc: 0.88012 |  0:07:34s\n",
      "epoch 253| loss: 0.3103  | val_0_auc: 0.87992 |  0:07:35s\n",
      "epoch 254| loss: 0.31024 | val_0_auc: 0.88075 |  0:07:37s\n",
      "epoch 255| loss: 0.30635 | val_0_auc: 0.88025 |  0:07:38s\n",
      "epoch 256| loss: 0.30998 | val_0_auc: 0.88088 |  0:07:39s\n",
      "epoch 257| loss: 0.30878 | val_0_auc: 0.88135 |  0:07:41s\n",
      "epoch 258| loss: 0.3072  | val_0_auc: 0.88079 |  0:07:42s\n",
      "epoch 259| loss: 0.30833 | val_0_auc: 0.8814  |  0:07:44s\n",
      "epoch 260| loss: 0.3078  | val_0_auc: 0.88217 |  0:07:45s\n",
      "epoch 261| loss: 0.30629 | val_0_auc: 0.88193 |  0:07:46s\n",
      "epoch 262| loss: 0.30682 | val_0_auc: 0.88216 |  0:07:48s\n",
      "epoch 263| loss: 0.30497 | val_0_auc: 0.8823  |  0:07:49s\n",
      "epoch 264| loss: 0.30539 | val_0_auc: 0.8826  |  0:07:50s\n",
      "epoch 265| loss: 0.30836 | val_0_auc: 0.88199 |  0:07:52s\n",
      "epoch 266| loss: 0.30482 | val_0_auc: 0.88217 |  0:07:53s\n",
      "epoch 267| loss: 0.30652 | val_0_auc: 0.88184 |  0:07:54s\n",
      "epoch 268| loss: 0.30442 | val_0_auc: 0.88279 |  0:07:56s\n",
      "epoch 269| loss: 0.30347 | val_0_auc: 0.88261 |  0:07:57s\n",
      "epoch 270| loss: 0.30555 | val_0_auc: 0.88319 |  0:07:59s\n",
      "epoch 271| loss: 0.30611 | val_0_auc: 0.88311 |  0:08:00s\n",
      "epoch 272| loss: 0.30441 | val_0_auc: 0.88302 |  0:08:01s\n",
      "epoch 273| loss: 0.30492 | val_0_auc: 0.8835  |  0:08:03s\n",
      "epoch 274| loss: 0.30334 | val_0_auc: 0.88382 |  0:08:04s\n",
      "epoch 275| loss: 0.30331 | val_0_auc: 0.88355 |  0:08:05s\n",
      "epoch 276| loss: 0.30051 | val_0_auc: 0.88396 |  0:08:07s\n",
      "epoch 277| loss: 0.30094 | val_0_auc: 0.88351 |  0:08:08s\n",
      "epoch 278| loss: 0.30339 | val_0_auc: 0.88385 |  0:08:09s\n",
      "epoch 279| loss: 0.30236 | val_0_auc: 0.88363 |  0:08:11s\n",
      "epoch 280| loss: 0.30175 | val_0_auc: 0.88402 |  0:08:12s\n",
      "epoch 281| loss: 0.30147 | val_0_auc: 0.8844  |  0:08:14s\n",
      "epoch 282| loss: 0.30047 | val_0_auc: 0.88436 |  0:08:15s\n",
      "epoch 283| loss: 0.30313 | val_0_auc: 0.88397 |  0:08:16s\n",
      "epoch 284| loss: 0.30155 | val_0_auc: 0.88428 |  0:08:18s\n",
      "epoch 285| loss: 0.30101 | val_0_auc: 0.88418 |  0:08:19s\n",
      "epoch 286| loss: 0.30299 | val_0_auc: 0.88361 |  0:08:20s\n",
      "epoch 287| loss: 0.30112 | val_0_auc: 0.88361 |  0:08:22s\n",
      "epoch 288| loss: 0.30175 | val_0_auc: 0.88422 |  0:08:23s\n",
      "epoch 289| loss: 0.30074 | val_0_auc: 0.88515 |  0:08:24s\n",
      "epoch 290| loss: 0.303   | val_0_auc: 0.88421 |  0:08:26s\n",
      "epoch 291| loss: 0.29768 | val_0_auc: 0.88426 |  0:08:27s\n",
      "epoch 292| loss: 0.30014 | val_0_auc: 0.88458 |  0:08:28s\n",
      "epoch 293| loss: 0.30073 | val_0_auc: 0.88521 |  0:08:30s\n",
      "epoch 294| loss: 0.29835 | val_0_auc: 0.88539 |  0:08:31s\n",
      "epoch 295| loss: 0.29872 | val_0_auc: 0.88503 |  0:08:33s\n",
      "epoch 296| loss: 0.29659 | val_0_auc: 0.88428 |  0:08:34s\n",
      "epoch 297| loss: 0.29843 | val_0_auc: 0.88412 |  0:08:36s\n",
      "epoch 298| loss: 0.29717 | val_0_auc: 0.88485 |  0:08:37s\n",
      "epoch 299| loss: 0.29691 | val_0_auc: 0.88398 |  0:08:38s\n",
      "epoch 300| loss: 0.29706 | val_0_auc: 0.88466 |  0:08:40s\n",
      "epoch 301| loss: 0.29709 | val_0_auc: 0.88521 |  0:08:41s\n",
      "epoch 302| loss: 0.29633 | val_0_auc: 0.88556 |  0:08:42s\n",
      "epoch 303| loss: 0.29551 | val_0_auc: 0.88545 |  0:08:44s\n",
      "epoch 304| loss: 0.29551 | val_0_auc: 0.8855  |  0:08:45s\n",
      "epoch 305| loss: 0.29653 | val_0_auc: 0.88439 |  0:08:46s\n",
      "epoch 306| loss: 0.29459 | val_0_auc: 0.88436 |  0:08:48s\n",
      "epoch 307| loss: 0.29463 | val_0_auc: 0.88449 |  0:08:49s\n",
      "epoch 308| loss: 0.29591 | val_0_auc: 0.8849  |  0:08:51s\n",
      "epoch 309| loss: 0.2972  | val_0_auc: 0.88537 |  0:08:52s\n",
      "epoch 310| loss: 0.29561 | val_0_auc: 0.88521 |  0:08:53s\n",
      "epoch 311| loss: 0.29309 | val_0_auc: 0.88524 |  0:08:55s\n",
      "epoch 312| loss: 0.29373 | val_0_auc: 0.88586 |  0:08:56s\n",
      "epoch 313| loss: 0.29475 | val_0_auc: 0.88576 |  0:08:57s\n",
      "epoch 314| loss: 0.29395 | val_0_auc: 0.88574 |  0:08:59s\n",
      "epoch 315| loss: 0.2937  | val_0_auc: 0.88562 |  0:09:00s\n",
      "epoch 316| loss: 0.29511 | val_0_auc: 0.88631 |  0:09:01s\n",
      "epoch 317| loss: 0.29398 | val_0_auc: 0.886   |  0:09:03s\n",
      "epoch 318| loss: 0.29159 | val_0_auc: 0.8854  |  0:09:04s\n",
      "epoch 319| loss: 0.29317 | val_0_auc: 0.88543 |  0:09:05s\n",
      "epoch 320| loss: 0.29183 | val_0_auc: 0.88552 |  0:09:07s\n",
      "epoch 321| loss: 0.29392 | val_0_auc: 0.88612 |  0:09:08s\n",
      "epoch 322| loss: 0.28935 | val_0_auc: 0.88589 |  0:09:10s\n",
      "epoch 323| loss: 0.29349 | val_0_auc: 0.8869  |  0:09:11s\n",
      "epoch 324| loss: 0.29218 | val_0_auc: 0.88635 |  0:09:12s\n",
      "epoch 325| loss: 0.2909  | val_0_auc: 0.88669 |  0:09:14s\n",
      "epoch 326| loss: 0.29197 | val_0_auc: 0.88705 |  0:09:15s\n",
      "epoch 327| loss: 0.29293 | val_0_auc: 0.88671 |  0:09:16s\n",
      "epoch 328| loss: 0.2923  | val_0_auc: 0.88738 |  0:09:18s\n",
      "epoch 329| loss: 0.28955 | val_0_auc: 0.88794 |  0:09:19s\n",
      "epoch 330| loss: 0.29117 | val_0_auc: 0.88866 |  0:09:20s\n",
      "epoch 331| loss: 0.29217 | val_0_auc: 0.88858 |  0:09:22s\n",
      "epoch 332| loss: 0.29091 | val_0_auc: 0.88883 |  0:09:23s\n",
      "epoch 333| loss: 0.29141 | val_0_auc: 0.88904 |  0:09:24s\n",
      "epoch 334| loss: 0.29066 | val_0_auc: 0.88911 |  0:09:26s\n",
      "epoch 335| loss: 0.29099 | val_0_auc: 0.88824 |  0:09:27s\n",
      "epoch 336| loss: 0.28864 | val_0_auc: 0.88845 |  0:09:29s\n",
      "epoch 337| loss: 0.28903 | val_0_auc: 0.88883 |  0:09:30s\n",
      "epoch 338| loss: 0.28881 | val_0_auc: 0.88876 |  0:09:31s\n",
      "epoch 339| loss: 0.29025 | val_0_auc: 0.88788 |  0:09:33s\n",
      "epoch 340| loss: 0.28583 | val_0_auc: 0.88755 |  0:09:34s\n",
      "epoch 341| loss: 0.29038 | val_0_auc: 0.88849 |  0:09:35s\n",
      "epoch 342| loss: 0.28906 | val_0_auc: 0.88811 |  0:09:37s\n",
      "epoch 343| loss: 0.28793 | val_0_auc: 0.88808 |  0:09:38s\n",
      "epoch 344| loss: 0.28457 | val_0_auc: 0.88778 |  0:09:39s\n",
      "epoch 345| loss: 0.28738 | val_0_auc: 0.88875 |  0:09:41s\n",
      "epoch 346| loss: 0.28726 | val_0_auc: 0.88824 |  0:09:42s\n",
      "epoch 347| loss: 0.28843 | val_0_auc: 0.88837 |  0:09:43s\n",
      "epoch 348| loss: 0.28877 | val_0_auc: 0.88912 |  0:09:45s\n",
      "epoch 349| loss: 0.28597 | val_0_auc: 0.88964 |  0:09:46s\n",
      "epoch 350| loss: 0.28754 | val_0_auc: 0.8905  |  0:09:48s\n",
      "epoch 351| loss: 0.28663 | val_0_auc: 0.89022 |  0:09:49s\n",
      "epoch 352| loss: 0.28728 | val_0_auc: 0.89049 |  0:09:50s\n",
      "epoch 353| loss: 0.287   | val_0_auc: 0.88934 |  0:09:52s\n",
      "epoch 354| loss: 0.28609 | val_0_auc: 0.89029 |  0:09:53s\n",
      "epoch 355| loss: 0.28709 | val_0_auc: 0.88984 |  0:09:54s\n",
      "epoch 356| loss: 0.28566 | val_0_auc: 0.88951 |  0:09:56s\n",
      "epoch 357| loss: 0.28641 | val_0_auc: 0.88997 |  0:09:57s\n",
      "epoch 358| loss: 0.28542 | val_0_auc: 0.89132 |  0:09:58s\n",
      "epoch 359| loss: 0.28612 | val_0_auc: 0.89024 |  0:10:00s\n",
      "epoch 360| loss: 0.28572 | val_0_auc: 0.88993 |  0:10:01s\n",
      "epoch 361| loss: 0.28527 | val_0_auc: 0.88955 |  0:10:02s\n",
      "epoch 362| loss: 0.28531 | val_0_auc: 0.88973 |  0:10:04s\n",
      "epoch 363| loss: 0.28607 | val_0_auc: 0.88964 |  0:10:05s\n",
      "epoch 364| loss: 0.28444 | val_0_auc: 0.88959 |  0:10:07s\n",
      "epoch 365| loss: 0.28568 | val_0_auc: 0.89059 |  0:10:08s\n",
      "epoch 366| loss: 0.28562 | val_0_auc: 0.88972 |  0:10:09s\n",
      "epoch 367| loss: 0.28195 | val_0_auc: 0.88996 |  0:10:11s\n",
      "epoch 368| loss: 0.28417 | val_0_auc: 0.89    |  0:10:12s\n",
      "epoch 369| loss: 0.28407 | val_0_auc: 0.89026 |  0:10:13s\n",
      "epoch 370| loss: 0.28405 | val_0_auc: 0.89002 |  0:10:15s\n",
      "epoch 371| loss: 0.28394 | val_0_auc: 0.89025 |  0:10:16s\n",
      "epoch 372| loss: 0.2848  | val_0_auc: 0.89029 |  0:10:17s\n",
      "epoch 373| loss: 0.28282 | val_0_auc: 0.88996 |  0:10:19s\n",
      "epoch 374| loss: 0.2834  | val_0_auc: 0.88978 |  0:10:20s\n",
      "epoch 375| loss: 0.28247 | val_0_auc: 0.88917 |  0:10:21s\n",
      "epoch 376| loss: 0.28385 | val_0_auc: 0.88875 |  0:10:23s\n",
      "epoch 377| loss: 0.28447 | val_0_auc: 0.88889 |  0:10:24s\n",
      "epoch 378| loss: 0.28326 | val_0_auc: 0.88779 |  0:10:26s\n",
      "\n",
      "Early stopping occurred at epoch 378 with best_epoch = 358 and best_val_0_auc = 0.89132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.92111 | val_0_auc: 0.50222 |  0:00:05s\n",
      "epoch 1  | loss: 0.88013 | val_0_auc: 0.5082  |  0:00:10s\n",
      "epoch 2  | loss: 0.86179 | val_0_auc: 0.51296 |  0:00:15s\n",
      "epoch 3  | loss: 0.83507 | val_0_auc: 0.5071  |  0:00:20s\n",
      "epoch 4  | loss: 0.82846 | val_0_auc: 0.50085 |  0:00:25s\n",
      "epoch 5  | loss: 0.80172 | val_0_auc: 0.49439 |  0:00:30s\n",
      "epoch 6  | loss: 0.7896  | val_0_auc: 0.48846 |  0:00:36s\n",
      "epoch 7  | loss: 0.78876 | val_0_auc: 0.49953 |  0:00:41s\n",
      "epoch 8  | loss: 0.78427 | val_0_auc: 0.49581 |  0:00:46s\n",
      "epoch 9  | loss: 0.77046 | val_0_auc: 0.50174 |  0:00:51s\n",
      "epoch 10 | loss: 0.7496  | val_0_auc: 0.49663 |  0:00:56s\n",
      "epoch 11 | loss: 0.73611 | val_0_auc: 0.47701 |  0:01:01s\n",
      "epoch 12 | loss: 0.72805 | val_0_auc: 0.49158 |  0:01:07s\n",
      "epoch 13 | loss: 0.72103 | val_0_auc: 0.48269 |  0:01:12s\n",
      "epoch 14 | loss: 0.71655 | val_0_auc: 0.50889 |  0:01:17s\n",
      "epoch 15 | loss: 0.70748 | val_0_auc: 0.51102 |  0:01:22s\n",
      "epoch 16 | loss: 0.701   | val_0_auc: 0.50705 |  0:01:27s\n",
      "epoch 17 | loss: 0.69608 | val_0_auc: 0.50773 |  0:01:32s\n",
      "epoch 18 | loss: 0.68695 | val_0_auc: 0.53137 |  0:01:37s\n",
      "epoch 19 | loss: 0.67902 | val_0_auc: 0.51427 |  0:01:42s\n",
      "epoch 20 | loss: 0.67092 | val_0_auc: 0.5208  |  0:01:47s\n",
      "epoch 21 | loss: 0.66951 | val_0_auc: 0.52592 |  0:01:52s\n",
      "epoch 22 | loss: 0.66449 | val_0_auc: 0.5263  |  0:01:58s\n",
      "epoch 23 | loss: 0.66672 | val_0_auc: 0.52281 |  0:02:02s\n",
      "epoch 24 | loss: 0.66414 | val_0_auc: 0.53583 |  0:02:07s\n",
      "epoch 25 | loss: 0.65312 | val_0_auc: 0.51812 |  0:02:12s\n",
      "epoch 26 | loss: 0.65987 | val_0_auc: 0.51958 |  0:02:17s\n",
      "epoch 27 | loss: 0.64945 | val_0_auc: 0.53505 |  0:02:21s\n",
      "epoch 28 | loss: 0.65524 | val_0_auc: 0.51199 |  0:02:26s\n",
      "epoch 29 | loss: 0.65119 | val_0_auc: 0.52305 |  0:02:31s\n",
      "epoch 30 | loss: 0.64869 | val_0_auc: 0.54133 |  0:02:36s\n",
      "epoch 31 | loss: 0.64805 | val_0_auc: 0.56288 |  0:02:41s\n",
      "epoch 32 | loss: 0.64046 | val_0_auc: 0.58041 |  0:02:45s\n",
      "epoch 33 | loss: 0.64071 | val_0_auc: 0.56387 |  0:02:50s\n",
      "epoch 34 | loss: 0.63496 | val_0_auc: 0.56437 |  0:02:55s\n",
      "epoch 35 | loss: 0.6321  | val_0_auc: 0.56373 |  0:02:59s\n",
      "epoch 36 | loss: 0.6301  | val_0_auc: 0.55225 |  0:03:04s\n",
      "epoch 37 | loss: 0.6263  | val_0_auc: 0.5455  |  0:03:09s\n",
      "epoch 38 | loss: 0.62676 | val_0_auc: 0.56381 |  0:03:14s\n",
      "epoch 39 | loss: 0.62478 | val_0_auc: 0.56889 |  0:03:18s\n",
      "epoch 40 | loss: 0.62019 | val_0_auc: 0.5585  |  0:03:24s\n",
      "epoch 41 | loss: 0.6174  | val_0_auc: 0.55345 |  0:03:29s\n",
      "epoch 42 | loss: 0.61878 | val_0_auc: 0.56421 |  0:03:34s\n",
      "epoch 43 | loss: 0.6155  | val_0_auc: 0.56573 |  0:03:39s\n",
      "epoch 44 | loss: 0.61667 | val_0_auc: 0.55913 |  0:03:44s\n",
      "epoch 45 | loss: 0.61009 | val_0_auc: 0.56429 |  0:03:49s\n",
      "epoch 46 | loss: 0.61367 | val_0_auc: 0.56516 |  0:03:53s\n",
      "epoch 47 | loss: 0.61324 | val_0_auc: 0.57759 |  0:03:58s\n",
      "epoch 48 | loss: 0.61155 | val_0_auc: 0.57395 |  0:04:03s\n",
      "epoch 49 | loss: 0.60715 | val_0_auc: 0.58773 |  0:04:08s\n",
      "epoch 50 | loss: 0.60735 | val_0_auc: 0.58712 |  0:04:13s\n",
      "epoch 51 | loss: 0.60745 | val_0_auc: 0.5876  |  0:04:17s\n",
      "epoch 52 | loss: 0.60327 | val_0_auc: 0.6187  |  0:04:22s\n",
      "epoch 53 | loss: 0.60519 | val_0_auc: 0.57758 |  0:04:27s\n",
      "epoch 54 | loss: 0.5997  | val_0_auc: 0.60201 |  0:04:32s\n",
      "epoch 55 | loss: 0.60011 | val_0_auc: 0.60461 |  0:04:36s\n",
      "epoch 56 | loss: 0.60246 | val_0_auc: 0.61856 |  0:04:41s\n",
      "epoch 57 | loss: 0.59718 | val_0_auc: 0.60164 |  0:04:46s\n",
      "epoch 58 | loss: 0.60116 | val_0_auc: 0.60563 |  0:04:51s\n",
      "epoch 59 | loss: 0.59612 | val_0_auc: 0.60846 |  0:04:55s\n",
      "epoch 60 | loss: 0.59436 | val_0_auc: 0.61326 |  0:05:00s\n",
      "epoch 61 | loss: 0.59947 | val_0_auc: 0.61344 |  0:05:05s\n",
      "epoch 62 | loss: 0.59929 | val_0_auc: 0.62341 |  0:05:10s\n",
      "epoch 63 | loss: 0.59911 | val_0_auc: 0.61523 |  0:05:14s\n",
      "epoch 64 | loss: 0.59715 | val_0_auc: 0.61084 |  0:05:19s\n",
      "epoch 65 | loss: 0.59338 | val_0_auc: 0.60671 |  0:05:24s\n",
      "epoch 66 | loss: 0.59255 | val_0_auc: 0.62202 |  0:05:29s\n",
      "epoch 67 | loss: 0.59434 | val_0_auc: 0.62157 |  0:05:33s\n",
      "epoch 68 | loss: 0.5929  | val_0_auc: 0.63718 |  0:05:38s\n",
      "epoch 69 | loss: 0.59377 | val_0_auc: 0.63364 |  0:05:43s\n",
      "epoch 70 | loss: 0.59508 | val_0_auc: 0.61496 |  0:05:48s\n",
      "epoch 71 | loss: 0.5946  | val_0_auc: 0.63527 |  0:05:53s\n",
      "epoch 72 | loss: 0.59047 | val_0_auc: 0.63801 |  0:05:57s\n",
      "epoch 73 | loss: 0.58949 | val_0_auc: 0.62326 |  0:06:02s\n",
      "epoch 74 | loss: 0.59052 | val_0_auc: 0.63858 |  0:06:09s\n",
      "epoch 75 | loss: 0.58561 | val_0_auc: 0.63335 |  0:06:14s\n",
      "epoch 76 | loss: 0.58755 | val_0_auc: 0.64103 |  0:06:18s\n",
      "epoch 77 | loss: 0.58389 | val_0_auc: 0.63627 |  0:06:23s\n",
      "epoch 78 | loss: 0.58593 | val_0_auc: 0.63839 |  0:06:28s\n",
      "epoch 79 | loss: 0.58849 | val_0_auc: 0.64165 |  0:06:33s\n",
      "epoch 80 | loss: 0.58302 | val_0_auc: 0.63498 |  0:06:38s\n",
      "epoch 81 | loss: 0.58494 | val_0_auc: 0.62707 |  0:06:43s\n",
      "epoch 82 | loss: 0.58419 | val_0_auc: 0.63636 |  0:06:48s\n",
      "epoch 83 | loss: 0.58186 | val_0_auc: 0.6607  |  0:06:52s\n",
      "epoch 84 | loss: 0.58126 | val_0_auc: 0.64356 |  0:06:57s\n",
      "epoch 85 | loss: 0.58167 | val_0_auc: 0.63572 |  0:07:02s\n",
      "epoch 86 | loss: 0.57992 | val_0_auc: 0.63596 |  0:07:07s\n",
      "epoch 87 | loss: 0.57902 | val_0_auc: 0.64762 |  0:07:11s\n",
      "epoch 88 | loss: 0.57999 | val_0_auc: 0.65646 |  0:07:16s\n",
      "epoch 89 | loss: 0.58181 | val_0_auc: 0.63617 |  0:07:21s\n",
      "epoch 90 | loss: 0.57904 | val_0_auc: 0.649   |  0:07:26s\n",
      "epoch 91 | loss: 0.57845 | val_0_auc: 0.65755 |  0:07:31s\n",
      "epoch 92 | loss: 0.57802 | val_0_auc: 0.67289 |  0:07:35s\n",
      "epoch 93 | loss: 0.58171 | val_0_auc: 0.65229 |  0:07:40s\n",
      "epoch 94 | loss: 0.57858 | val_0_auc: 0.65049 |  0:07:45s\n",
      "epoch 95 | loss: 0.57774 | val_0_auc: 0.65026 |  0:07:50s\n",
      "epoch 96 | loss: 0.57485 | val_0_auc: 0.65786 |  0:07:55s\n",
      "epoch 97 | loss: 0.57703 | val_0_auc: 0.66064 |  0:07:59s\n",
      "epoch 98 | loss: 0.57853 | val_0_auc: 0.67208 |  0:08:04s\n",
      "epoch 99 | loss: 0.57626 | val_0_auc: 0.66272 |  0:08:09s\n",
      "epoch 100| loss: 0.57478 | val_0_auc: 0.66448 |  0:08:14s\n",
      "epoch 101| loss: 0.57744 | val_0_auc: 0.67325 |  0:08:19s\n",
      "epoch 102| loss: 0.57613 | val_0_auc: 0.65817 |  0:08:23s\n",
      "epoch 103| loss: 0.57423 | val_0_auc: 0.67096 |  0:08:28s\n",
      "epoch 104| loss: 0.57489 | val_0_auc: 0.67231 |  0:08:33s\n",
      "epoch 105| loss: 0.57547 | val_0_auc: 0.66954 |  0:08:38s\n",
      "epoch 106| loss: 0.57454 | val_0_auc: 0.66319 |  0:08:43s\n",
      "epoch 107| loss: 0.57604 | val_0_auc: 0.66132 |  0:08:49s\n",
      "epoch 108| loss: 0.57571 | val_0_auc: 0.66456 |  0:08:54s\n",
      "epoch 109| loss: 0.57234 | val_0_auc: 0.66089 |  0:08:59s\n",
      "epoch 110| loss: 0.57603 | val_0_auc: 0.64398 |  0:09:04s\n",
      "epoch 111| loss: 0.5725  | val_0_auc: 0.65234 |  0:09:09s\n",
      "epoch 112| loss: 0.57044 | val_0_auc: 0.6554  |  0:09:14s\n",
      "epoch 113| loss: 0.57083 | val_0_auc: 0.67268 |  0:09:19s\n",
      "epoch 114| loss: 0.57122 | val_0_auc: 0.6552  |  0:09:24s\n",
      "epoch 115| loss: 0.57048 | val_0_auc: 0.65288 |  0:09:29s\n",
      "epoch 116| loss: 0.56823 | val_0_auc: 0.66018 |  0:09:34s\n",
      "epoch 117| loss: 0.57004 | val_0_auc: 0.66784 |  0:09:41s\n",
      "epoch 118| loss: 0.5681  | val_0_auc: 0.66129 |  0:09:49s\n",
      "epoch 119| loss: 0.57296 | val_0_auc: 0.66651 |  0:09:58s\n",
      "epoch 120| loss: 0.56628 | val_0_auc: 0.66708 |  0:10:03s\n",
      "epoch 121| loss: 0.56638 | val_0_auc: 0.67292 |  0:10:08s\n",
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 101 and best_val_0_auc = 0.67325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.52691 | val_0_auc: 0.70795 |  0:00:00s\n",
      "epoch 1  | loss: 0.38215 | val_0_auc: 0.79956 |  0:00:01s\n",
      "epoch 2  | loss: 0.33394 | val_0_auc: 0.84295 |  0:00:03s\n",
      "epoch 3  | loss: 0.30937 | val_0_auc: 0.86486 |  0:00:04s\n",
      "epoch 4  | loss: 0.29492 | val_0_auc: 0.88524 |  0:00:06s\n",
      "epoch 5  | loss: 0.28493 | val_0_auc: 0.89185 |  0:00:07s\n",
      "epoch 6  | loss: 0.27555 | val_0_auc: 0.90042 |  0:00:09s\n",
      "epoch 7  | loss: 0.26553 | val_0_auc: 0.89833 |  0:00:12s\n",
      "epoch 8  | loss: 0.26082 | val_0_auc: 0.90423 |  0:00:14s\n",
      "epoch 9  | loss: 0.25734 | val_0_auc: 0.90598 |  0:00:17s\n",
      "epoch 10 | loss: 0.25143 | val_0_auc: 0.90733 |  0:00:19s\n",
      "epoch 11 | loss: 0.25083 | val_0_auc: 0.90316 |  0:00:21s\n",
      "epoch 12 | loss: 0.24656 | val_0_auc: 0.90667 |  0:00:22s\n",
      "epoch 13 | loss: 0.23878 | val_0_auc: 0.9057  |  0:00:23s\n",
      "epoch 14 | loss: 0.23863 | val_0_auc: 0.90261 |  0:00:24s\n",
      "epoch 15 | loss: 0.23586 | val_0_auc: 0.90423 |  0:00:25s\n",
      "epoch 16 | loss: 0.23013 | val_0_auc: 0.90158 |  0:00:26s\n",
      "epoch 17 | loss: 0.23067 | val_0_auc: 0.90123 |  0:00:27s\n",
      "epoch 18 | loss: 0.22826 | val_0_auc: 0.90035 |  0:00:29s\n",
      "epoch 19 | loss: 0.22829 | val_0_auc: 0.89913 |  0:00:30s\n",
      "epoch 20 | loss: 0.22428 | val_0_auc: 0.89481 |  0:00:31s\n",
      "epoch 21 | loss: 0.21785 | val_0_auc: 0.89568 |  0:00:32s\n",
      "epoch 22 | loss: 0.21632 | val_0_auc: 0.89504 |  0:00:34s\n",
      "epoch 23 | loss: 0.21455 | val_0_auc: 0.89678 |  0:00:35s\n",
      "epoch 24 | loss: 0.21094 | val_0_auc: 0.89381 |  0:00:36s\n",
      "epoch 25 | loss: 0.21029 | val_0_auc: 0.8989  |  0:00:37s\n",
      "epoch 26 | loss: 0.2122  | val_0_auc: 0.90039 |  0:00:38s\n",
      "epoch 27 | loss: 0.20939 | val_0_auc: 0.89485 |  0:00:40s\n",
      "epoch 28 | loss: 0.2087  | val_0_auc: 0.89607 |  0:00:41s\n",
      "epoch 29 | loss: 0.20734 | val_0_auc: 0.89363 |  0:00:42s\n",
      "epoch 30 | loss: 0.20606 | val_0_auc: 0.89586 |  0:00:43s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_0_auc = 0.90733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6814  | val_0_auc: 0.73825 |  0:00:02s\n",
      "epoch 1  | loss: 0.45167 | val_0_auc: 0.81524 |  0:00:04s\n",
      "epoch 2  | loss: 0.41365 | val_0_auc: 0.83953 |  0:00:06s\n",
      "epoch 3  | loss: 0.39251 | val_0_auc: 0.84622 |  0:00:08s\n",
      "epoch 4  | loss: 0.38501 | val_0_auc: 0.86376 |  0:00:10s\n",
      "epoch 5  | loss: 0.37549 | val_0_auc: 0.8665  |  0:00:12s\n",
      "epoch 6  | loss: 0.36887 | val_0_auc: 0.87317 |  0:00:14s\n",
      "epoch 7  | loss: 0.36311 | val_0_auc: 0.87496 |  0:00:15s\n",
      "epoch 8  | loss: 0.35798 | val_0_auc: 0.87818 |  0:00:17s\n",
      "epoch 9  | loss: 0.35178 | val_0_auc: 0.88352 |  0:00:19s\n",
      "epoch 10 | loss: 0.3517  | val_0_auc: 0.88497 |  0:00:21s\n",
      "epoch 11 | loss: 0.34494 | val_0_auc: 0.88645 |  0:00:23s\n",
      "epoch 12 | loss: 0.34352 | val_0_auc: 0.8891  |  0:00:25s\n",
      "epoch 13 | loss: 0.33551 | val_0_auc: 0.89176 |  0:00:26s\n",
      "epoch 14 | loss: 0.33469 | val_0_auc: 0.89226 |  0:00:28s\n",
      "epoch 15 | loss: 0.33047 | val_0_auc: 0.89635 |  0:00:30s\n",
      "epoch 16 | loss: 0.32659 | val_0_auc: 0.89782 |  0:00:31s\n",
      "epoch 17 | loss: 0.32255 | val_0_auc: 0.89831 |  0:00:33s\n",
      "epoch 18 | loss: 0.32097 | val_0_auc: 0.8999  |  0:00:35s\n",
      "epoch 19 | loss: 0.3158  | val_0_auc: 0.90085 |  0:00:37s\n",
      "epoch 20 | loss: 0.31183 | val_0_auc: 0.90464 |  0:00:38s\n",
      "epoch 21 | loss: 0.31099 | val_0_auc: 0.90543 |  0:00:40s\n",
      "epoch 22 | loss: 0.30776 | val_0_auc: 0.90627 |  0:00:42s\n",
      "epoch 23 | loss: 0.30418 | val_0_auc: 0.90848 |  0:00:44s\n",
      "epoch 24 | loss: 0.30273 | val_0_auc: 0.9079  |  0:00:45s\n",
      "epoch 25 | loss: 0.29966 | val_0_auc: 0.9073  |  0:00:47s\n",
      "epoch 26 | loss: 0.29705 | val_0_auc: 0.90647 |  0:00:49s\n",
      "epoch 27 | loss: 0.29518 | val_0_auc: 0.9071  |  0:00:51s\n",
      "epoch 28 | loss: 0.29279 | val_0_auc: 0.90826 |  0:00:52s\n",
      "epoch 29 | loss: 0.29027 | val_0_auc: 0.90884 |  0:00:54s\n",
      "epoch 30 | loss: 0.28847 | val_0_auc: 0.90999 |  0:00:56s\n",
      "epoch 31 | loss: 0.28988 | val_0_auc: 0.90888 |  0:00:57s\n",
      "epoch 32 | loss: 0.28454 | val_0_auc: 0.91029 |  0:00:59s\n",
      "epoch 33 | loss: 0.28586 | val_0_auc: 0.90879 |  0:01:01s\n",
      "epoch 34 | loss: 0.28479 | val_0_auc: 0.90965 |  0:01:04s\n",
      "epoch 35 | loss: 0.2805  | val_0_auc: 0.91167 |  0:01:06s\n",
      "epoch 36 | loss: 0.28003 | val_0_auc: 0.91063 |  0:01:08s\n",
      "epoch 37 | loss: 0.27717 | val_0_auc: 0.91265 |  0:01:10s\n",
      "epoch 38 | loss: 0.27513 | val_0_auc: 0.91193 |  0:01:12s\n",
      "epoch 39 | loss: 0.27545 | val_0_auc: 0.91256 |  0:01:14s\n",
      "epoch 40 | loss: 0.27558 | val_0_auc: 0.91328 |  0:01:15s\n",
      "epoch 41 | loss: 0.27265 | val_0_auc: 0.91091 |  0:01:17s\n",
      "epoch 42 | loss: 0.27163 | val_0_auc: 0.90958 |  0:01:19s\n",
      "epoch 43 | loss: 0.27229 | val_0_auc: 0.9102  |  0:01:21s\n",
      "epoch 44 | loss: 0.26983 | val_0_auc: 0.91039 |  0:01:22s\n",
      "epoch 45 | loss: 0.2664  | val_0_auc: 0.91086 |  0:01:24s\n",
      "epoch 46 | loss: 0.26449 | val_0_auc: 0.90846 |  0:01:26s\n",
      "epoch 47 | loss: 0.2631  | val_0_auc: 0.90927 |  0:01:28s\n",
      "epoch 48 | loss: 0.26268 | val_0_auc: 0.90994 |  0:01:30s\n",
      "epoch 49 | loss: 0.26042 | val_0_auc: 0.91005 |  0:01:31s\n",
      "epoch 50 | loss: 0.25959 | val_0_auc: 0.90929 |  0:01:33s\n",
      "epoch 51 | loss: 0.26026 | val_0_auc: 0.91134 |  0:01:35s\n",
      "epoch 52 | loss: 0.25735 | val_0_auc: 0.90994 |  0:01:37s\n",
      "epoch 53 | loss: 0.25607 | val_0_auc: 0.90753 |  0:01:39s\n",
      "epoch 54 | loss: 0.25252 | val_0_auc: 0.90864 |  0:01:41s\n",
      "epoch 55 | loss: 0.2534  | val_0_auc: 0.90811 |  0:01:42s\n",
      "epoch 56 | loss: 0.2533  | val_0_auc: 0.90665 |  0:01:44s\n",
      "epoch 57 | loss: 0.2518  | val_0_auc: 0.90379 |  0:01:46s\n",
      "epoch 58 | loss: 0.25067 | val_0_auc: 0.90591 |  0:01:48s\n",
      "epoch 59 | loss: 0.25068 | val_0_auc: 0.90573 |  0:01:49s\n",
      "epoch 60 | loss: 0.24915 | val_0_auc: 0.90552 |  0:01:51s\n",
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 40 and best_val_0_auc = 0.91328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78668 | val_0_auc: 0.57875 |  0:00:02s\n",
      "epoch 1  | loss: 0.61364 | val_0_auc: 0.62965 |  0:00:04s\n",
      "epoch 2  | loss: 0.52392 | val_0_auc: 0.64412 |  0:00:07s\n",
      "epoch 3  | loss: 0.46481 | val_0_auc: 0.78956 |  0:00:10s\n",
      "epoch 4  | loss: 0.43784 | val_0_auc: 0.82228 |  0:00:12s\n",
      "epoch 5  | loss: 0.42463 | val_0_auc: 0.82015 |  0:00:15s\n",
      "epoch 6  | loss: 0.41097 | val_0_auc: 0.8277  |  0:00:17s\n",
      "epoch 7  | loss: 0.3972  | val_0_auc: 0.84106 |  0:00:20s\n",
      "epoch 8  | loss: 0.37705 | val_0_auc: 0.85271 |  0:00:22s\n",
      "epoch 9  | loss: 0.36504 | val_0_auc: 0.86018 |  0:00:24s\n",
      "epoch 10 | loss: 0.35562 | val_0_auc: 0.86709 |  0:00:27s\n",
      "epoch 11 | loss: 0.35007 | val_0_auc: 0.87178 |  0:00:29s\n",
      "epoch 12 | loss: 0.34415 | val_0_auc: 0.87569 |  0:00:32s\n",
      "epoch 13 | loss: 0.34031 | val_0_auc: 0.87111 |  0:00:35s\n",
      "epoch 14 | loss: 0.34046 | val_0_auc: 0.87642 |  0:00:38s\n",
      "epoch 15 | loss: 0.34301 | val_0_auc: 0.88261 |  0:00:40s\n",
      "epoch 16 | loss: 0.33925 | val_0_auc: 0.88678 |  0:00:43s\n",
      "epoch 17 | loss: 0.32996 | val_0_auc: 0.88901 |  0:00:45s\n",
      "epoch 18 | loss: 0.32165 | val_0_auc: 0.89098 |  0:00:48s\n",
      "epoch 19 | loss: 0.31535 | val_0_auc: 0.89333 |  0:00:51s\n",
      "epoch 20 | loss: 0.31043 | val_0_auc: 0.89366 |  0:00:53s\n",
      "epoch 21 | loss: 0.30709 | val_0_auc: 0.89392 |  0:00:56s\n",
      "epoch 22 | loss: 0.30593 | val_0_auc: 0.89474 |  0:00:58s\n",
      "epoch 23 | loss: 0.30641 | val_0_auc: 0.89659 |  0:01:01s\n",
      "epoch 24 | loss: 0.30697 | val_0_auc: 0.90095 |  0:01:03s\n",
      "epoch 25 | loss: 0.30298 | val_0_auc: 0.89987 |  0:01:05s\n",
      "epoch 26 | loss: 0.30101 | val_0_auc: 0.90122 |  0:01:08s\n",
      "epoch 27 | loss: 0.30002 | val_0_auc: 0.9033  |  0:01:10s\n",
      "epoch 28 | loss: 0.29993 | val_0_auc: 0.90319 |  0:01:13s\n",
      "epoch 29 | loss: 0.29755 | val_0_auc: 0.90529 |  0:01:15s\n",
      "epoch 30 | loss: 0.2935  | val_0_auc: 0.90617 |  0:01:17s\n",
      "epoch 31 | loss: 0.29137 | val_0_auc: 0.90578 |  0:01:20s\n",
      "epoch 32 | loss: 0.29231 | val_0_auc: 0.90774 |  0:01:22s\n",
      "epoch 33 | loss: 0.29381 | val_0_auc: 0.9049  |  0:01:24s\n",
      "epoch 34 | loss: 0.28735 | val_0_auc: 0.90821 |  0:01:27s\n",
      "epoch 35 | loss: 0.29013 | val_0_auc: 0.9041  |  0:01:30s\n",
      "epoch 36 | loss: 0.28789 | val_0_auc: 0.90827 |  0:01:32s\n",
      "epoch 37 | loss: 0.28468 | val_0_auc: 0.90906 |  0:01:35s\n",
      "epoch 38 | loss: 0.28546 | val_0_auc: 0.90854 |  0:01:37s\n",
      "epoch 39 | loss: 0.28562 | val_0_auc: 0.90799 |  0:01:39s\n",
      "epoch 40 | loss: 0.28277 | val_0_auc: 0.90755 |  0:01:42s\n",
      "epoch 41 | loss: 0.28416 | val_0_auc: 0.90514 |  0:01:44s\n",
      "epoch 42 | loss: 0.28242 | val_0_auc: 0.90765 |  0:01:47s\n",
      "epoch 43 | loss: 0.28102 | val_0_auc: 0.9076  |  0:01:49s\n",
      "epoch 44 | loss: 0.28275 | val_0_auc: 0.91108 |  0:01:52s\n",
      "epoch 45 | loss: 0.2797  | val_0_auc: 0.91176 |  0:01:55s\n",
      "epoch 46 | loss: 0.27731 | val_0_auc: 0.91206 |  0:01:57s\n",
      "epoch 47 | loss: 0.27532 | val_0_auc: 0.91068 |  0:02:00s\n",
      "epoch 48 | loss: 0.27367 | val_0_auc: 0.91031 |  0:02:02s\n",
      "epoch 49 | loss: 0.27409 | val_0_auc: 0.91061 |  0:02:05s\n",
      "epoch 50 | loss: 0.27414 | val_0_auc: 0.91124 |  0:02:08s\n",
      "epoch 51 | loss: 0.27313 | val_0_auc: 0.90901 |  0:02:10s\n",
      "epoch 52 | loss: 0.27351 | val_0_auc: 0.9106  |  0:02:13s\n",
      "epoch 53 | loss: 0.27264 | val_0_auc: 0.91188 |  0:02:15s\n",
      "epoch 54 | loss: 0.27094 | val_0_auc: 0.9114  |  0:02:18s\n",
      "epoch 55 | loss: 0.27087 | val_0_auc: 0.9082  |  0:02:20s\n",
      "epoch 56 | loss: 0.27225 | val_0_auc: 0.91113 |  0:02:26s\n",
      "epoch 57 | loss: 0.26514 | val_0_auc: 0.9104  |  0:02:29s\n",
      "epoch 58 | loss: 0.26503 | val_0_auc: 0.91204 |  0:02:31s\n",
      "epoch 59 | loss: 0.26757 | val_0_auc: 0.913   |  0:02:34s\n",
      "epoch 60 | loss: 0.26713 | val_0_auc: 0.9102  |  0:02:36s\n",
      "epoch 61 | loss: 0.26296 | val_0_auc: 0.91148 |  0:02:39s\n",
      "epoch 62 | loss: 0.26445 | val_0_auc: 0.91236 |  0:02:41s\n",
      "epoch 63 | loss: 0.25959 | val_0_auc: 0.91155 |  0:02:44s\n",
      "epoch 64 | loss: 0.26146 | val_0_auc: 0.91291 |  0:02:46s\n",
      "epoch 65 | loss: 0.25832 | val_0_auc: 0.91474 |  0:02:49s\n",
      "epoch 66 | loss: 0.25857 | val_0_auc: 0.91162 |  0:02:51s\n",
      "epoch 67 | loss: 0.25758 | val_0_auc: 0.91296 |  0:02:53s\n",
      "epoch 68 | loss: 0.25533 | val_0_auc: 0.91067 |  0:02:56s\n",
      "epoch 69 | loss: 0.25505 | val_0_auc: 0.9139  |  0:02:58s\n",
      "epoch 70 | loss: 0.25423 | val_0_auc: 0.91237 |  0:03:00s\n",
      "epoch 71 | loss: 0.25329 | val_0_auc: 0.91406 |  0:03:03s\n",
      "epoch 72 | loss: 0.25446 | val_0_auc: 0.91404 |  0:03:06s\n",
      "epoch 73 | loss: 0.25301 | val_0_auc: 0.91192 |  0:03:08s\n",
      "epoch 74 | loss: 0.25082 | val_0_auc: 0.91199 |  0:03:11s\n",
      "epoch 75 | loss: 0.25145 | val_0_auc: 0.91125 |  0:03:14s\n",
      "epoch 76 | loss: 0.25135 | val_0_auc: 0.9099  |  0:03:17s\n",
      "epoch 77 | loss: 0.2506  | val_0_auc: 0.90998 |  0:03:20s\n",
      "epoch 78 | loss: 0.24873 | val_0_auc: 0.90808 |  0:03:22s\n",
      "epoch 79 | loss: 0.25088 | val_0_auc: 0.91117 |  0:03:25s\n",
      "epoch 80 | loss: 0.24731 | val_0_auc: 0.90997 |  0:03:27s\n",
      "epoch 81 | loss: 0.24603 | val_0_auc: 0.90863 |  0:03:29s\n",
      "epoch 82 | loss: 0.24573 | val_0_auc: 0.91041 |  0:03:32s\n",
      "epoch 83 | loss: 0.24374 | val_0_auc: 0.91012 |  0:03:34s\n",
      "epoch 84 | loss: 0.24655 | val_0_auc: 0.90846 |  0:03:37s\n",
      "epoch 85 | loss: 0.24456 | val_0_auc: 0.90633 |  0:03:41s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_val_0_auc = 0.91474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.85055 | val_0_auc: 0.49858 |  0:00:05s\n",
      "epoch 1  | loss: 1.28894 | val_0_auc: 0.50081 |  0:00:10s\n",
      "epoch 2  | loss: 1.1092  | val_0_auc: 0.55259 |  0:00:15s\n",
      "epoch 3  | loss: 1.03928 | val_0_auc: 0.61573 |  0:00:21s\n",
      "epoch 4  | loss: 0.98393 | val_0_auc: 0.69007 |  0:00:26s\n",
      "epoch 5  | loss: 0.93532 | val_0_auc: 0.74662 |  0:00:33s\n",
      "epoch 6  | loss: 0.88929 | val_0_auc: 0.78293 |  0:00:38s\n",
      "epoch 7  | loss: 0.8755  | val_0_auc: 0.81035 |  0:00:43s\n",
      "epoch 8  | loss: 0.83829 | val_0_auc: 0.82099 |  0:00:48s\n",
      "epoch 9  | loss: 0.83267 | val_0_auc: 0.83192 |  0:00:53s\n",
      "epoch 10 | loss: 0.8102  | val_0_auc: 0.83676 |  0:00:58s\n",
      "epoch 11 | loss: 0.80673 | val_0_auc: 0.83555 |  0:01:03s\n",
      "epoch 12 | loss: 0.7888  | val_0_auc: 0.84531 |  0:01:09s\n",
      "epoch 13 | loss: 0.77778 | val_0_auc: 0.8462  |  0:01:14s\n",
      "epoch 14 | loss: 0.7653  | val_0_auc: 0.84985 |  0:01:21s\n",
      "epoch 15 | loss: 0.75742 | val_0_auc: 0.85377 |  0:01:27s\n",
      "epoch 16 | loss: 0.75199 | val_0_auc: 0.85964 |  0:01:34s\n",
      "epoch 17 | loss: 0.74622 | val_0_auc: 0.85329 |  0:01:41s\n",
      "epoch 18 | loss: 0.73243 | val_0_auc: 0.86235 |  0:01:47s\n",
      "epoch 19 | loss: 0.72602 | val_0_auc: 0.86311 |  0:01:52s\n",
      "epoch 20 | loss: 0.71752 | val_0_auc: 0.86465 |  0:01:57s\n",
      "epoch 21 | loss: 0.7132  | val_0_auc: 0.86542 |  0:02:02s\n",
      "epoch 22 | loss: 0.70616 | val_0_auc: 0.87151 |  0:02:08s\n",
      "epoch 23 | loss: 0.69449 | val_0_auc: 0.86715 |  0:02:13s\n",
      "epoch 24 | loss: 0.69585 | val_0_auc: 0.87273 |  0:02:19s\n",
      "epoch 25 | loss: 0.69605 | val_0_auc: 0.87087 |  0:02:24s\n",
      "epoch 26 | loss: 0.68444 | val_0_auc: 0.87851 |  0:02:29s\n",
      "epoch 27 | loss: 0.68395 | val_0_auc: 0.88133 |  0:02:34s\n",
      "epoch 28 | loss: 0.67492 | val_0_auc: 0.88312 |  0:02:40s\n",
      "epoch 29 | loss: 0.67406 | val_0_auc: 0.88341 |  0:02:45s\n",
      "epoch 30 | loss: 0.66454 | val_0_auc: 0.88362 |  0:02:53s\n",
      "epoch 31 | loss: 0.66538 | val_0_auc: 0.88186 |  0:02:58s\n",
      "epoch 32 | loss: 0.66411 | val_0_auc: 0.88266 |  0:03:04s\n",
      "epoch 33 | loss: 0.66323 | val_0_auc: 0.88366 |  0:03:11s\n",
      "epoch 34 | loss: 0.65895 | val_0_auc: 0.88409 |  0:03:17s\n",
      "epoch 35 | loss: 0.65273 | val_0_auc: 0.88497 |  0:03:24s\n",
      "epoch 36 | loss: 0.6498  | val_0_auc: 0.88706 |  0:03:29s\n",
      "epoch 37 | loss: 0.64882 | val_0_auc: 0.8949  |  0:03:34s\n",
      "epoch 38 | loss: 0.6471  | val_0_auc: 0.89163 |  0:03:39s\n",
      "epoch 39 | loss: 0.64384 | val_0_auc: 0.89203 |  0:03:45s\n",
      "epoch 40 | loss: 0.64414 | val_0_auc: 0.89333 |  0:03:50s\n",
      "epoch 41 | loss: 0.64141 | val_0_auc: 0.89154 |  0:03:55s\n",
      "epoch 42 | loss: 0.63908 | val_0_auc: 0.89527 |  0:04:02s\n",
      "epoch 43 | loss: 0.63475 | val_0_auc: 0.89551 |  0:04:07s\n",
      "epoch 44 | loss: 0.63404 | val_0_auc: 0.89603 |  0:04:13s\n",
      "epoch 45 | loss: 0.63496 | val_0_auc: 0.89713 |  0:04:19s\n",
      "epoch 46 | loss: 0.62976 | val_0_auc: 0.89542 |  0:04:25s\n",
      "epoch 47 | loss: 0.62578 | val_0_auc: 0.89577 |  0:04:31s\n",
      "epoch 48 | loss: 0.62497 | val_0_auc: 0.89658 |  0:04:36s\n",
      "epoch 49 | loss: 0.62291 | val_0_auc: 0.8972  |  0:04:41s\n",
      "epoch 50 | loss: 0.62026 | val_0_auc: 0.89668 |  0:04:47s\n",
      "epoch 51 | loss: 0.62243 | val_0_auc: 0.89638 |  0:04:52s\n",
      "epoch 52 | loss: 0.62092 | val_0_auc: 0.89878 |  0:04:58s\n",
      "epoch 53 | loss: 0.61755 | val_0_auc: 0.89652 |  0:05:04s\n",
      "epoch 54 | loss: 0.61488 | val_0_auc: 0.89813 |  0:05:09s\n",
      "epoch 55 | loss: 0.61707 | val_0_auc: 0.89622 |  0:05:15s\n",
      "epoch 56 | loss: 0.61339 | val_0_auc: 0.89663 |  0:05:20s\n",
      "epoch 57 | loss: 0.61156 | val_0_auc: 0.89746 |  0:05:25s\n",
      "epoch 58 | loss: 0.60639 | val_0_auc: 0.89793 |  0:05:30s\n",
      "epoch 59 | loss: 0.60828 | val_0_auc: 0.89913 |  0:05:36s\n",
      "epoch 60 | loss: 0.60541 | val_0_auc: 0.90151 |  0:05:41s\n",
      "epoch 61 | loss: 0.60529 | val_0_auc: 0.90265 |  0:05:47s\n",
      "epoch 62 | loss: 0.60327 | val_0_auc: 0.89973 |  0:05:55s\n",
      "epoch 63 | loss: 0.60199 | val_0_auc: 0.89943 |  0:06:01s\n",
      "epoch 64 | loss: 0.59994 | val_0_auc: 0.90079 |  0:06:06s\n",
      "epoch 65 | loss: 0.5977  | val_0_auc: 0.90398 |  0:06:11s\n",
      "epoch 66 | loss: 0.60032 | val_0_auc: 0.90183 |  0:06:16s\n",
      "epoch 67 | loss: 0.59202 | val_0_auc: 0.90101 |  0:06:21s\n",
      "epoch 68 | loss: 0.59363 | val_0_auc: 0.90079 |  0:06:26s\n",
      "epoch 69 | loss: 0.59196 | val_0_auc: 0.90465 |  0:06:31s\n",
      "epoch 70 | loss: 0.59123 | val_0_auc: 0.90389 |  0:06:37s\n",
      "epoch 71 | loss: 0.5911  | val_0_auc: 0.90498 |  0:06:42s\n",
      "epoch 72 | loss: 0.58642 | val_0_auc: 0.90488 |  0:06:48s\n",
      "epoch 73 | loss: 0.58538 | val_0_auc: 0.90594 |  0:06:54s\n",
      "epoch 74 | loss: 0.58655 | val_0_auc: 0.90509 |  0:06:59s\n",
      "epoch 75 | loss: 0.58629 | val_0_auc: 0.90389 |  0:07:04s\n",
      "epoch 76 | loss: 0.58491 | val_0_auc: 0.90332 |  0:07:09s\n",
      "epoch 77 | loss: 0.58145 | val_0_auc: 0.90379 |  0:07:15s\n",
      "epoch 78 | loss: 0.58001 | val_0_auc: 0.90319 |  0:07:20s\n",
      "epoch 79 | loss: 0.57783 | val_0_auc: 0.90539 |  0:07:25s\n",
      "epoch 80 | loss: 0.57557 | val_0_auc: 0.90541 |  0:07:30s\n",
      "epoch 81 | loss: 0.57321 | val_0_auc: 0.90592 |  0:07:35s\n",
      "epoch 82 | loss: 0.57587 | val_0_auc: 0.90365 |  0:07:41s\n",
      "epoch 83 | loss: 0.57301 | val_0_auc: 0.90364 |  0:07:46s\n",
      "epoch 84 | loss: 0.57293 | val_0_auc: 0.9029  |  0:07:51s\n",
      "epoch 85 | loss: 0.57519 | val_0_auc: 0.90484 |  0:07:57s\n",
      "epoch 86 | loss: 0.57056 | val_0_auc: 0.90408 |  0:08:03s\n",
      "epoch 87 | loss: 0.57016 | val_0_auc: 0.90384 |  0:08:08s\n",
      "epoch 88 | loss: 0.56855 | val_0_auc: 0.90273 |  0:08:14s\n",
      "epoch 89 | loss: 0.56756 | val_0_auc: 0.90333 |  0:08:19s\n",
      "epoch 90 | loss: 0.56538 | val_0_auc: 0.90433 |  0:08:24s\n",
      "epoch 91 | loss: 0.56501 | val_0_auc: 0.90479 |  0:08:29s\n",
      "epoch 92 | loss: 0.55968 | val_0_auc: 0.90429 |  0:08:34s\n",
      "epoch 93 | loss: 0.56089 | val_0_auc: 0.90409 |  0:08:39s\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 73 and best_val_0_auc = 0.90594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71298 | val_0_auc: 0.53234 |  0:00:00s\n",
      "epoch 1  | loss: 0.65464 | val_0_auc: 0.56116 |  0:00:01s\n",
      "epoch 2  | loss: 0.61696 | val_0_auc: 0.59287 |  0:00:02s\n",
      "epoch 3  | loss: 0.58688 | val_0_auc: 0.62592 |  0:00:03s\n",
      "epoch 4  | loss: 0.56071 | val_0_auc: 0.65469 |  0:00:04s\n",
      "epoch 5  | loss: 0.53558 | val_0_auc: 0.67987 |  0:00:05s\n",
      "epoch 6  | loss: 0.51714 | val_0_auc: 0.7053  |  0:00:06s\n",
      "epoch 7  | loss: 0.50163 | val_0_auc: 0.73047 |  0:00:07s\n",
      "epoch 8  | loss: 0.48156 | val_0_auc: 0.74073 |  0:00:07s\n",
      "epoch 9  | loss: 0.46963 | val_0_auc: 0.75503 |  0:00:08s\n",
      "epoch 10 | loss: 0.45834 | val_0_auc: 0.7662  |  0:00:09s\n",
      "epoch 11 | loss: 0.44552 | val_0_auc: 0.77717 |  0:00:10s\n",
      "epoch 12 | loss: 0.43662 | val_0_auc: 0.78731 |  0:00:11s\n",
      "epoch 13 | loss: 0.43002 | val_0_auc: 0.79557 |  0:00:12s\n",
      "epoch 14 | loss: 0.41895 | val_0_auc: 0.8044  |  0:00:13s\n",
      "epoch 15 | loss: 0.41067 | val_0_auc: 0.81188 |  0:00:14s\n",
      "epoch 16 | loss: 0.4018  | val_0_auc: 0.8187  |  0:00:15s\n",
      "epoch 17 | loss: 0.39523 | val_0_auc: 0.825   |  0:00:16s\n",
      "epoch 18 | loss: 0.39032 | val_0_auc: 0.8293  |  0:00:17s\n",
      "epoch 19 | loss: 0.38492 | val_0_auc: 0.83382 |  0:00:17s\n",
      "epoch 20 | loss: 0.38079 | val_0_auc: 0.83696 |  0:00:19s\n",
      "epoch 21 | loss: 0.37373 | val_0_auc: 0.8404  |  0:00:20s\n",
      "epoch 22 | loss: 0.37346 | val_0_auc: 0.84387 |  0:00:21s\n",
      "epoch 23 | loss: 0.36706 | val_0_auc: 0.84635 |  0:00:22s\n",
      "epoch 24 | loss: 0.36541 | val_0_auc: 0.8494  |  0:00:22s\n",
      "epoch 25 | loss: 0.35908 | val_0_auc: 0.85085 |  0:00:23s\n",
      "epoch 26 | loss: 0.35661 | val_0_auc: 0.85408 |  0:00:24s\n",
      "epoch 27 | loss: 0.35295 | val_0_auc: 0.85494 |  0:00:25s\n",
      "epoch 28 | loss: 0.34894 | val_0_auc: 0.85713 |  0:00:26s\n",
      "epoch 29 | loss: 0.34674 | val_0_auc: 0.85859 |  0:00:27s\n",
      "epoch 30 | loss: 0.34478 | val_0_auc: 0.86    |  0:00:28s\n",
      "epoch 31 | loss: 0.34281 | val_0_auc: 0.86202 |  0:00:29s\n",
      "epoch 32 | loss: 0.34385 | val_0_auc: 0.86217 |  0:00:30s\n",
      "epoch 33 | loss: 0.33608 | val_0_auc: 0.8639  |  0:00:31s\n",
      "epoch 34 | loss: 0.33695 | val_0_auc: 0.86545 |  0:00:32s\n",
      "epoch 35 | loss: 0.33246 | val_0_auc: 0.86767 |  0:00:33s\n",
      "epoch 36 | loss: 0.32909 | val_0_auc: 0.86886 |  0:00:35s\n",
      "epoch 37 | loss: 0.3297  | val_0_auc: 0.86897 |  0:00:38s\n",
      "epoch 38 | loss: 0.32624 | val_0_auc: 0.87122 |  0:00:39s\n",
      "epoch 39 | loss: 0.32575 | val_0_auc: 0.87227 |  0:00:40s\n",
      "epoch 40 | loss: 0.32409 | val_0_auc: 0.87272 |  0:00:41s\n",
      "epoch 41 | loss: 0.31952 | val_0_auc: 0.87357 |  0:00:43s\n",
      "epoch 42 | loss: 0.31967 | val_0_auc: 0.87498 |  0:00:44s\n",
      "epoch 43 | loss: 0.31974 | val_0_auc: 0.87532 |  0:00:45s\n",
      "epoch 44 | loss: 0.31411 | val_0_auc: 0.8761  |  0:00:46s\n",
      "epoch 45 | loss: 0.31389 | val_0_auc: 0.87604 |  0:00:47s\n",
      "epoch 46 | loss: 0.31087 | val_0_auc: 0.87619 |  0:00:48s\n",
      "epoch 47 | loss: 0.3115  | val_0_auc: 0.87713 |  0:00:49s\n",
      "epoch 48 | loss: 0.30869 | val_0_auc: 0.87704 |  0:00:50s\n",
      "epoch 49 | loss: 0.30657 | val_0_auc: 0.87761 |  0:00:51s\n",
      "epoch 50 | loss: 0.3062  | val_0_auc: 0.87789 |  0:00:52s\n",
      "epoch 51 | loss: 0.30405 | val_0_auc: 0.8795  |  0:00:53s\n",
      "epoch 52 | loss: 0.30185 | val_0_auc: 0.87974 |  0:00:54s\n",
      "epoch 53 | loss: 0.30225 | val_0_auc: 0.88003 |  0:00:55s\n",
      "epoch 54 | loss: 0.29949 | val_0_auc: 0.8801  |  0:00:56s\n",
      "epoch 55 | loss: 0.2979  | val_0_auc: 0.8807  |  0:00:57s\n",
      "epoch 56 | loss: 0.29719 | val_0_auc: 0.88164 |  0:00:57s\n",
      "epoch 57 | loss: 0.29345 | val_0_auc: 0.88273 |  0:00:58s\n",
      "epoch 58 | loss: 0.29421 | val_0_auc: 0.88216 |  0:00:59s\n",
      "epoch 59 | loss: 0.29193 | val_0_auc: 0.88207 |  0:01:00s\n",
      "epoch 60 | loss: 0.29174 | val_0_auc: 0.8825  |  0:01:01s\n",
      "epoch 61 | loss: 0.28948 | val_0_auc: 0.88213 |  0:01:02s\n",
      "epoch 62 | loss: 0.28975 | val_0_auc: 0.88267 |  0:01:04s\n",
      "epoch 63 | loss: 0.28725 | val_0_auc: 0.88326 |  0:01:05s\n",
      "epoch 64 | loss: 0.2866  | val_0_auc: 0.88257 |  0:01:06s\n",
      "epoch 65 | loss: 0.285   | val_0_auc: 0.8835  |  0:01:08s\n",
      "epoch 66 | loss: 0.2837  | val_0_auc: 0.88379 |  0:01:09s\n",
      "epoch 67 | loss: 0.28186 | val_0_auc: 0.88437 |  0:01:10s\n",
      "epoch 68 | loss: 0.28386 | val_0_auc: 0.8847  |  0:01:12s\n",
      "epoch 69 | loss: 0.28283 | val_0_auc: 0.88473 |  0:01:14s\n",
      "epoch 70 | loss: 0.28114 | val_0_auc: 0.88459 |  0:01:15s\n",
      "epoch 71 | loss: 0.28017 | val_0_auc: 0.88556 |  0:01:16s\n",
      "epoch 72 | loss: 0.27877 | val_0_auc: 0.88442 |  0:01:17s\n",
      "epoch 73 | loss: 0.27783 | val_0_auc: 0.88509 |  0:01:18s\n",
      "epoch 74 | loss: 0.27914 | val_0_auc: 0.88528 |  0:01:19s\n",
      "epoch 75 | loss: 0.27754 | val_0_auc: 0.88507 |  0:01:20s\n",
      "epoch 76 | loss: 0.27396 | val_0_auc: 0.88536 |  0:01:21s\n",
      "epoch 77 | loss: 0.27664 | val_0_auc: 0.886   |  0:01:22s\n",
      "epoch 78 | loss: 0.27523 | val_0_auc: 0.88624 |  0:01:23s\n",
      "epoch 79 | loss: 0.27438 | val_0_auc: 0.88643 |  0:01:24s\n",
      "epoch 80 | loss: 0.27371 | val_0_auc: 0.88688 |  0:01:25s\n",
      "epoch 81 | loss: 0.27453 | val_0_auc: 0.88677 |  0:01:26s\n",
      "epoch 82 | loss: 0.27242 | val_0_auc: 0.88671 |  0:01:28s\n",
      "epoch 83 | loss: 0.26951 | val_0_auc: 0.88648 |  0:01:29s\n",
      "epoch 84 | loss: 0.27126 | val_0_auc: 0.8868  |  0:01:30s\n",
      "epoch 85 | loss: 0.26883 | val_0_auc: 0.88677 |  0:01:31s\n",
      "epoch 86 | loss: 0.26962 | val_0_auc: 0.88721 |  0:01:32s\n",
      "epoch 87 | loss: 0.26701 | val_0_auc: 0.88736 |  0:01:33s\n",
      "epoch 88 | loss: 0.26907 | val_0_auc: 0.88689 |  0:01:34s\n",
      "epoch 89 | loss: 0.26705 | val_0_auc: 0.88789 |  0:01:35s\n",
      "epoch 90 | loss: 0.26494 | val_0_auc: 0.88705 |  0:01:38s\n",
      "epoch 91 | loss: 0.2645  | val_0_auc: 0.88765 |  0:01:40s\n",
      "epoch 92 | loss: 0.26561 | val_0_auc: 0.88637 |  0:01:41s\n",
      "epoch 93 | loss: 0.26527 | val_0_auc: 0.8868  |  0:01:42s\n",
      "epoch 94 | loss: 0.26318 | val_0_auc: 0.88656 |  0:01:43s\n",
      "epoch 95 | loss: 0.26607 | val_0_auc: 0.88486 |  0:01:44s\n",
      "epoch 96 | loss: 0.26168 | val_0_auc: 0.88628 |  0:01:45s\n",
      "epoch 97 | loss: 0.26022 | val_0_auc: 0.88625 |  0:01:47s\n",
      "epoch 98 | loss: 0.26122 | val_0_auc: 0.886   |  0:01:48s\n",
      "epoch 99 | loss: 0.25959 | val_0_auc: 0.88654 |  0:01:50s\n",
      "epoch 100| loss: 0.26181 | val_0_auc: 0.88473 |  0:01:52s\n",
      "epoch 101| loss: 0.25994 | val_0_auc: 0.88664 |  0:01:54s\n",
      "epoch 102| loss: 0.26046 | val_0_auc: 0.88647 |  0:01:55s\n",
      "epoch 103| loss: 0.25948 | val_0_auc: 0.88718 |  0:01:56s\n",
      "epoch 104| loss: 0.25898 | val_0_auc: 0.88606 |  0:01:57s\n",
      "epoch 105| loss: 0.2605  | val_0_auc: 0.88512 |  0:01:57s\n",
      "epoch 106| loss: 0.25703 | val_0_auc: 0.88719 |  0:01:58s\n",
      "epoch 107| loss: 0.25844 | val_0_auc: 0.88618 |  0:01:59s\n",
      "epoch 108| loss: 0.2542  | val_0_auc: 0.88639 |  0:02:00s\n",
      "epoch 109| loss: 0.25502 | val_0_auc: 0.88501 |  0:02:01s\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_val_0_auc = 0.88789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.58517 | val_0_auc: 0.6257  |  0:00:00s\n",
      "epoch 1  | loss: 0.4584  | val_0_auc: 0.77877 |  0:00:01s\n",
      "epoch 2  | loss: 0.40671 | val_0_auc: 0.83002 |  0:00:02s\n",
      "epoch 3  | loss: 0.37298 | val_0_auc: 0.85309 |  0:00:03s\n",
      "epoch 4  | loss: 0.3479  | val_0_auc: 0.86404 |  0:00:04s\n",
      "epoch 5  | loss: 0.33279 | val_0_auc: 0.87248 |  0:00:05s\n",
      "epoch 6  | loss: 0.32303 | val_0_auc: 0.8772  |  0:00:05s\n",
      "epoch 7  | loss: 0.31812 | val_0_auc: 0.88188 |  0:00:06s\n",
      "epoch 8  | loss: 0.31073 | val_0_auc: 0.88614 |  0:00:07s\n",
      "epoch 9  | loss: 0.30798 | val_0_auc: 0.89073 |  0:00:08s\n",
      "epoch 10 | loss: 0.30315 | val_0_auc: 0.89214 |  0:00:09s\n",
      "epoch 11 | loss: 0.29904 | val_0_auc: 0.89423 |  0:00:10s\n",
      "epoch 12 | loss: 0.2923  | val_0_auc: 0.89558 |  0:00:11s\n",
      "epoch 13 | loss: 0.29173 | val_0_auc: 0.89555 |  0:00:13s\n",
      "epoch 14 | loss: 0.28709 | val_0_auc: 0.89832 |  0:00:14s\n",
      "epoch 15 | loss: 0.28147 | val_0_auc: 0.89765 |  0:00:15s\n",
      "epoch 16 | loss: 0.27957 | val_0_auc: 0.89907 |  0:00:16s\n",
      "epoch 17 | loss: 0.27689 | val_0_auc: 0.89966 |  0:00:17s\n",
      "epoch 18 | loss: 0.27287 | val_0_auc: 0.89804 |  0:00:17s\n",
      "epoch 19 | loss: 0.27093 | val_0_auc: 0.89739 |  0:00:18s\n",
      "epoch 20 | loss: 0.26843 | val_0_auc: 0.90058 |  0:00:19s\n",
      "epoch 21 | loss: 0.26794 | val_0_auc: 0.9015  |  0:00:21s\n",
      "epoch 22 | loss: 0.26874 | val_0_auc: 0.90515 |  0:00:23s\n",
      "epoch 23 | loss: 0.26484 | val_0_auc: 0.90136 |  0:00:24s\n",
      "epoch 24 | loss: 0.26018 | val_0_auc: 0.90107 |  0:00:25s\n",
      "epoch 25 | loss: 0.25969 | val_0_auc: 0.90156 |  0:00:26s\n",
      "epoch 26 | loss: 0.25597 | val_0_auc: 0.90002 |  0:00:27s\n",
      "epoch 27 | loss: 0.25765 | val_0_auc: 0.90119 |  0:00:28s\n",
      "epoch 28 | loss: 0.25632 | val_0_auc: 0.90173 |  0:00:28s\n",
      "epoch 29 | loss: 0.2522  | val_0_auc: 0.90233 |  0:00:29s\n",
      "epoch 30 | loss: 0.25415 | val_0_auc: 0.89843 |  0:00:30s\n",
      "epoch 31 | loss: 0.25009 | val_0_auc: 0.90078 |  0:00:31s\n",
      "epoch 32 | loss: 0.24877 | val_0_auc: 0.89875 |  0:00:32s\n",
      "epoch 33 | loss: 0.24858 | val_0_auc: 0.89911 |  0:00:32s\n",
      "epoch 34 | loss: 0.24778 | val_0_auc: 0.89967 |  0:00:33s\n",
      "epoch 35 | loss: 0.24688 | val_0_auc: 0.89566 |  0:00:34s\n",
      "epoch 36 | loss: 0.24421 | val_0_auc: 0.9009  |  0:00:35s\n",
      "epoch 37 | loss: 0.24407 | val_0_auc: 0.89599 |  0:00:36s\n",
      "epoch 38 | loss: 0.24575 | val_0_auc: 0.89751 |  0:00:36s\n",
      "epoch 39 | loss: 0.24389 | val_0_auc: 0.89704 |  0:00:37s\n",
      "epoch 40 | loss: 0.24113 | val_0_auc: 0.89619 |  0:00:38s\n",
      "epoch 41 | loss: 0.23972 | val_0_auc: 0.89705 |  0:00:39s\n",
      "epoch 42 | loss: 0.23848 | val_0_auc: 0.89602 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.90515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.94711 | val_0_auc: 0.51303 |  0:00:01s\n",
      "epoch 1  | loss: 0.83719 | val_0_auc: 0.5389  |  0:00:03s\n",
      "epoch 2  | loss: 0.75844 | val_0_auc: 0.55544 |  0:00:05s\n",
      "epoch 3  | loss: 0.71088 | val_0_auc: 0.5836  |  0:00:07s\n",
      "epoch 4  | loss: 0.66637 | val_0_auc: 0.60137 |  0:00:09s\n",
      "epoch 5  | loss: 0.62313 | val_0_auc: 0.62873 |  0:00:11s\n",
      "epoch 6  | loss: 0.58866 | val_0_auc: 0.66536 |  0:00:13s\n",
      "epoch 7  | loss: 0.56391 | val_0_auc: 0.68727 |  0:00:15s\n",
      "epoch 8  | loss: 0.55184 | val_0_auc: 0.7099  |  0:00:17s\n",
      "epoch 9  | loss: 0.54063 | val_0_auc: 0.73149 |  0:00:18s\n",
      "epoch 10 | loss: 0.52638 | val_0_auc: 0.73344 |  0:00:20s\n",
      "epoch 11 | loss: 0.51433 | val_0_auc: 0.74176 |  0:00:22s\n",
      "epoch 12 | loss: 0.50774 | val_0_auc: 0.75138 |  0:00:24s\n",
      "epoch 13 | loss: 0.50082 | val_0_auc: 0.75619 |  0:00:26s\n",
      "epoch 14 | loss: 0.48795 | val_0_auc: 0.76817 |  0:00:28s\n",
      "epoch 15 | loss: 0.48439 | val_0_auc: 0.77799 |  0:00:30s\n",
      "epoch 16 | loss: 0.48265 | val_0_auc: 0.77978 |  0:00:32s\n",
      "epoch 17 | loss: 0.4765  | val_0_auc: 0.78398 |  0:00:34s\n",
      "epoch 18 | loss: 0.46916 | val_0_auc: 0.78754 |  0:00:36s\n",
      "epoch 19 | loss: 0.46598 | val_0_auc: 0.78963 |  0:00:38s\n",
      "epoch 20 | loss: 0.46024 | val_0_auc: 0.80048 |  0:00:39s\n",
      "epoch 21 | loss: 0.45575 | val_0_auc: 0.80395 |  0:00:41s\n",
      "epoch 22 | loss: 0.45552 | val_0_auc: 0.80331 |  0:00:43s\n",
      "epoch 23 | loss: 0.45426 | val_0_auc: 0.80741 |  0:00:45s\n",
      "epoch 24 | loss: 0.45246 | val_0_auc: 0.81049 |  0:00:47s\n",
      "epoch 25 | loss: 0.45248 | val_0_auc: 0.8088  |  0:00:49s\n",
      "epoch 26 | loss: 0.44636 | val_0_auc: 0.8087  |  0:00:51s\n",
      "epoch 27 | loss: 0.44933 | val_0_auc: 0.81255 |  0:00:53s\n",
      "epoch 28 | loss: 0.44426 | val_0_auc: 0.81707 |  0:00:55s\n",
      "epoch 29 | loss: 0.44248 | val_0_auc: 0.81399 |  0:00:57s\n",
      "epoch 30 | loss: 0.43787 | val_0_auc: 0.81486 |  0:00:59s\n",
      "epoch 31 | loss: 0.43788 | val_0_auc: 0.81809 |  0:01:00s\n",
      "epoch 32 | loss: 0.43559 | val_0_auc: 0.81728 |  0:01:02s\n",
      "epoch 33 | loss: 0.435   | val_0_auc: 0.81984 |  0:01:04s\n",
      "epoch 34 | loss: 0.43091 | val_0_auc: 0.82172 |  0:01:06s\n",
      "epoch 35 | loss: 0.4325  | val_0_auc: 0.82126 |  0:01:08s\n",
      "epoch 36 | loss: 0.42974 | val_0_auc: 0.82156 |  0:01:10s\n",
      "epoch 37 | loss: 0.43048 | val_0_auc: 0.82367 |  0:01:12s\n",
      "epoch 38 | loss: 0.42883 | val_0_auc: 0.82165 |  0:01:14s\n",
      "epoch 39 | loss: 0.42896 | val_0_auc: 0.8256  |  0:01:16s\n",
      "epoch 40 | loss: 0.42492 | val_0_auc: 0.82467 |  0:01:17s\n",
      "epoch 41 | loss: 0.42613 | val_0_auc: 0.82616 |  0:01:19s\n",
      "epoch 42 | loss: 0.42621 | val_0_auc: 0.8273  |  0:01:21s\n",
      "epoch 43 | loss: 0.42293 | val_0_auc: 0.82725 |  0:01:23s\n",
      "epoch 44 | loss: 0.42166 | val_0_auc: 0.82972 |  0:01:25s\n",
      "epoch 45 | loss: 0.42415 | val_0_auc: 0.83146 |  0:01:27s\n",
      "epoch 46 | loss: 0.4245  | val_0_auc: 0.8331  |  0:01:29s\n",
      "epoch 47 | loss: 0.42414 | val_0_auc: 0.83141 |  0:01:31s\n",
      "epoch 48 | loss: 0.41966 | val_0_auc: 0.83341 |  0:01:33s\n",
      "epoch 49 | loss: 0.42039 | val_0_auc: 0.83195 |  0:01:35s\n",
      "epoch 50 | loss: 0.41791 | val_0_auc: 0.83471 |  0:01:37s\n",
      "epoch 51 | loss: 0.41592 | val_0_auc: 0.83482 |  0:01:39s\n",
      "epoch 52 | loss: 0.41526 | val_0_auc: 0.83214 |  0:01:40s\n",
      "epoch 53 | loss: 0.41189 | val_0_auc: 0.83347 |  0:01:42s\n",
      "epoch 54 | loss: 0.41392 | val_0_auc: 0.83285 |  0:01:44s\n",
      "epoch 55 | loss: 0.41268 | val_0_auc: 0.83552 |  0:01:46s\n",
      "epoch 56 | loss: 0.41472 | val_0_auc: 0.83468 |  0:01:48s\n",
      "epoch 57 | loss: 0.41097 | val_0_auc: 0.83761 |  0:01:50s\n",
      "epoch 58 | loss: 0.41206 | val_0_auc: 0.83696 |  0:01:52s\n",
      "epoch 59 | loss: 0.41178 | val_0_auc: 0.83707 |  0:01:54s\n",
      "epoch 60 | loss: 0.41119 | val_0_auc: 0.83917 |  0:01:56s\n",
      "epoch 61 | loss: 0.4096  | val_0_auc: 0.8372  |  0:01:58s\n",
      "epoch 62 | loss: 0.41245 | val_0_auc: 0.83584 |  0:02:00s\n",
      "epoch 63 | loss: 0.41162 | val_0_auc: 0.83775 |  0:02:02s\n",
      "epoch 64 | loss: 0.41144 | val_0_auc: 0.83692 |  0:02:03s\n",
      "epoch 65 | loss: 0.41007 | val_0_auc: 0.83909 |  0:02:05s\n",
      "epoch 66 | loss: 0.40816 | val_0_auc: 0.8386  |  0:02:07s\n",
      "epoch 67 | loss: 0.40877 | val_0_auc: 0.84146 |  0:02:09s\n",
      "epoch 68 | loss: 0.40741 | val_0_auc: 0.83856 |  0:02:11s\n",
      "epoch 69 | loss: 0.40819 | val_0_auc: 0.83845 |  0:02:13s\n",
      "epoch 70 | loss: 0.40495 | val_0_auc: 0.84215 |  0:02:15s\n",
      "epoch 71 | loss: 0.40669 | val_0_auc: 0.84327 |  0:02:17s\n",
      "epoch 72 | loss: 0.40813 | val_0_auc: 0.84238 |  0:02:19s\n",
      "epoch 73 | loss: 0.40551 | val_0_auc: 0.84019 |  0:02:20s\n",
      "epoch 74 | loss: 0.40466 | val_0_auc: 0.84277 |  0:02:22s\n",
      "epoch 75 | loss: 0.40389 | val_0_auc: 0.8388  |  0:02:24s\n",
      "epoch 76 | loss: 0.40429 | val_0_auc: 0.84157 |  0:02:26s\n",
      "epoch 77 | loss: 0.40332 | val_0_auc: 0.84041 |  0:02:28s\n",
      "epoch 78 | loss: 0.40423 | val_0_auc: 0.84088 |  0:02:30s\n",
      "epoch 79 | loss: 0.40291 | val_0_auc: 0.84209 |  0:02:32s\n",
      "epoch 80 | loss: 0.40127 | val_0_auc: 0.84265 |  0:02:34s\n",
      "epoch 81 | loss: 0.40231 | val_0_auc: 0.84214 |  0:02:36s\n",
      "epoch 82 | loss: 0.40445 | val_0_auc: 0.84232 |  0:02:38s\n",
      "epoch 83 | loss: 0.404   | val_0_auc: 0.84462 |  0:02:40s\n",
      "epoch 84 | loss: 0.40267 | val_0_auc: 0.84317 |  0:02:41s\n",
      "epoch 85 | loss: 0.40321 | val_0_auc: 0.8451  |  0:02:43s\n",
      "epoch 86 | loss: 0.40204 | val_0_auc: 0.8461  |  0:02:45s\n",
      "epoch 87 | loss: 0.40051 | val_0_auc: 0.84113 |  0:02:47s\n",
      "epoch 88 | loss: 0.39926 | val_0_auc: 0.84401 |  0:02:49s\n",
      "epoch 89 | loss: 0.40022 | val_0_auc: 0.8442  |  0:02:51s\n",
      "epoch 90 | loss: 0.39823 | val_0_auc: 0.84308 |  0:02:53s\n",
      "epoch 91 | loss: 0.39974 | val_0_auc: 0.84218 |  0:02:55s\n",
      "epoch 92 | loss: 0.39886 | val_0_auc: 0.84624 |  0:02:57s\n",
      "epoch 93 | loss: 0.39495 | val_0_auc: 0.84656 |  0:02:59s\n",
      "epoch 94 | loss: 0.39424 | val_0_auc: 0.84592 |  0:03:01s\n",
      "epoch 95 | loss: 0.39265 | val_0_auc: 0.84714 |  0:03:02s\n",
      "epoch 96 | loss: 0.39818 | val_0_auc: 0.84736 |  0:03:04s\n",
      "epoch 97 | loss: 0.39624 | val_0_auc: 0.84659 |  0:03:06s\n",
      "epoch 98 | loss: 0.38986 | val_0_auc: 0.84463 |  0:03:08s\n",
      "epoch 99 | loss: 0.39171 | val_0_auc: 0.84816 |  0:03:10s\n",
      "epoch 100| loss: 0.39468 | val_0_auc: 0.84723 |  0:03:12s\n",
      "epoch 101| loss: 0.39287 | val_0_auc: 0.84822 |  0:03:14s\n",
      "epoch 102| loss: 0.39001 | val_0_auc: 0.85047 |  0:03:16s\n",
      "epoch 103| loss: 0.39126 | val_0_auc: 0.84996 |  0:03:18s\n",
      "epoch 104| loss: 0.38976 | val_0_auc: 0.84614 |  0:03:19s\n",
      "epoch 105| loss: 0.39186 | val_0_auc: 0.84693 |  0:03:21s\n",
      "epoch 106| loss: 0.39167 | val_0_auc: 0.85001 |  0:03:23s\n",
      "epoch 107| loss: 0.39149 | val_0_auc: 0.84875 |  0:03:25s\n",
      "epoch 108| loss: 0.38618 | val_0_auc: 0.84939 |  0:03:27s\n",
      "epoch 109| loss: 0.38963 | val_0_auc: 0.84783 |  0:03:29s\n",
      "epoch 110| loss: 0.38847 | val_0_auc: 0.84975 |  0:03:31s\n",
      "epoch 111| loss: 0.38774 | val_0_auc: 0.85026 |  0:03:33s\n",
      "epoch 112| loss: 0.38784 | val_0_auc: 0.84985 |  0:03:35s\n",
      "epoch 113| loss: 0.39174 | val_0_auc: 0.84885 |  0:03:37s\n",
      "epoch 114| loss: 0.38588 | val_0_auc: 0.84867 |  0:03:39s\n",
      "epoch 115| loss: 0.3854  | val_0_auc: 0.85268 |  0:03:41s\n",
      "epoch 116| loss: 0.38615 | val_0_auc: 0.85141 |  0:03:43s\n",
      "epoch 117| loss: 0.38832 | val_0_auc: 0.85109 |  0:03:44s\n",
      "epoch 118| loss: 0.38521 | val_0_auc: 0.84966 |  0:03:46s\n",
      "epoch 119| loss: 0.38507 | val_0_auc: 0.8511  |  0:03:48s\n",
      "epoch 120| loss: 0.38556 | val_0_auc: 0.85206 |  0:03:50s\n",
      "epoch 121| loss: 0.38247 | val_0_auc: 0.85261 |  0:03:52s\n",
      "epoch 122| loss: 0.38221 | val_0_auc: 0.85171 |  0:03:54s\n",
      "epoch 123| loss: 0.38534 | val_0_auc: 0.85165 |  0:03:56s\n",
      "epoch 124| loss: 0.38351 | val_0_auc: 0.85275 |  0:03:58s\n",
      "epoch 125| loss: 0.38275 | val_0_auc: 0.85317 |  0:03:59s\n",
      "epoch 126| loss: 0.3828  | val_0_auc: 0.85289 |  0:04:01s\n",
      "epoch 127| loss: 0.38066 | val_0_auc: 0.85334 |  0:04:03s\n",
      "epoch 128| loss: 0.37975 | val_0_auc: 0.85194 |  0:04:05s\n",
      "epoch 129| loss: 0.38007 | val_0_auc: 0.85482 |  0:04:07s\n",
      "epoch 130| loss: 0.37776 | val_0_auc: 0.85331 |  0:04:09s\n",
      "epoch 131| loss: 0.37865 | val_0_auc: 0.85351 |  0:04:11s\n",
      "epoch 132| loss: 0.38239 | val_0_auc: 0.8526  |  0:04:13s\n",
      "epoch 133| loss: 0.38124 | val_0_auc: 0.85441 |  0:04:15s\n",
      "epoch 134| loss: 0.37889 | val_0_auc: 0.8527  |  0:04:17s\n",
      "epoch 135| loss: 0.37956 | val_0_auc: 0.85268 |  0:04:18s\n",
      "epoch 136| loss: 0.37751 | val_0_auc: 0.8545  |  0:04:20s\n",
      "epoch 137| loss: 0.37816 | val_0_auc: 0.8523  |  0:04:22s\n",
      "epoch 138| loss: 0.37785 | val_0_auc: 0.85365 |  0:04:24s\n",
      "epoch 139| loss: 0.37835 | val_0_auc: 0.85474 |  0:04:26s\n",
      "epoch 140| loss: 0.38073 | val_0_auc: 0.85609 |  0:04:28s\n",
      "epoch 141| loss: 0.37879 | val_0_auc: 0.85579 |  0:04:30s\n",
      "epoch 142| loss: 0.37584 | val_0_auc: 0.85497 |  0:04:32s\n",
      "epoch 143| loss: 0.37761 | val_0_auc: 0.85608 |  0:04:34s\n",
      "epoch 144| loss: 0.37558 | val_0_auc: 0.85651 |  0:04:36s\n",
      "epoch 145| loss: 0.37586 | val_0_auc: 0.85503 |  0:04:37s\n",
      "epoch 146| loss: 0.37454 | val_0_auc: 0.85493 |  0:04:39s\n",
      "epoch 147| loss: 0.37527 | val_0_auc: 0.85564 |  0:04:41s\n",
      "epoch 148| loss: 0.37662 | val_0_auc: 0.85624 |  0:04:43s\n",
      "epoch 149| loss: 0.37383 | val_0_auc: 0.85751 |  0:04:45s\n",
      "epoch 150| loss: 0.37363 | val_0_auc: 0.8562  |  0:04:47s\n",
      "epoch 151| loss: 0.37354 | val_0_auc: 0.85591 |  0:04:49s\n",
      "epoch 152| loss: 0.37235 | val_0_auc: 0.85497 |  0:04:51s\n",
      "epoch 153| loss: 0.37105 | val_0_auc: 0.85746 |  0:04:53s\n",
      "epoch 154| loss: 0.37472 | val_0_auc: 0.85593 |  0:04:55s\n",
      "epoch 155| loss: 0.37031 | val_0_auc: 0.85604 |  0:04:56s\n",
      "epoch 156| loss: 0.37098 | val_0_auc: 0.85796 |  0:04:58s\n",
      "epoch 157| loss: 0.36896 | val_0_auc: 0.85771 |  0:05:00s\n",
      "epoch 158| loss: 0.37277 | val_0_auc: 0.85696 |  0:05:02s\n",
      "epoch 159| loss: 0.37321 | val_0_auc: 0.85959 |  0:05:04s\n",
      "epoch 160| loss: 0.36889 | val_0_auc: 0.8589  |  0:05:06s\n",
      "epoch 161| loss: 0.37247 | val_0_auc: 0.85799 |  0:05:08s\n",
      "epoch 162| loss: 0.37063 | val_0_auc: 0.8606  |  0:05:10s\n",
      "epoch 163| loss: 0.36789 | val_0_auc: 0.85882 |  0:05:12s\n",
      "epoch 164| loss: 0.37156 | val_0_auc: 0.85783 |  0:05:13s\n",
      "epoch 165| loss: 0.37121 | val_0_auc: 0.85833 |  0:05:15s\n",
      "epoch 166| loss: 0.36951 | val_0_auc: 0.85707 |  0:05:17s\n",
      "epoch 167| loss: 0.3703  | val_0_auc: 0.85536 |  0:05:19s\n",
      "epoch 168| loss: 0.36795 | val_0_auc: 0.85571 |  0:05:21s\n",
      "epoch 169| loss: 0.37222 | val_0_auc: 0.85492 |  0:05:23s\n",
      "epoch 170| loss: 0.37083 | val_0_auc: 0.85548 |  0:05:25s\n",
      "epoch 171| loss: 0.36874 | val_0_auc: 0.85507 |  0:05:27s\n",
      "epoch 172| loss: 0.36965 | val_0_auc: 0.85701 |  0:05:29s\n",
      "epoch 173| loss: 0.36876 | val_0_auc: 0.85734 |  0:05:31s\n",
      "epoch 174| loss: 0.36579 | val_0_auc: 0.85881 |  0:05:32s\n",
      "epoch 175| loss: 0.36867 | val_0_auc: 0.85917 |  0:05:34s\n",
      "epoch 176| loss: 0.36755 | val_0_auc: 0.86014 |  0:05:36s\n",
      "epoch 177| loss: 0.36598 | val_0_auc: 0.85891 |  0:05:38s\n",
      "epoch 178| loss: 0.3655  | val_0_auc: 0.85965 |  0:05:40s\n",
      "epoch 179| loss: 0.36562 | val_0_auc: 0.85869 |  0:05:42s\n",
      "epoch 180| loss: 0.36696 | val_0_auc: 0.85854 |  0:05:44s\n",
      "epoch 181| loss: 0.36425 | val_0_auc: 0.86141 |  0:05:46s\n",
      "epoch 182| loss: 0.36687 | val_0_auc: 0.86159 |  0:05:48s\n",
      "epoch 183| loss: 0.36393 | val_0_auc: 0.86316 |  0:05:50s\n",
      "epoch 184| loss: 0.36623 | val_0_auc: 0.86216 |  0:05:52s\n",
      "epoch 185| loss: 0.36707 | val_0_auc: 0.86234 |  0:05:53s\n",
      "epoch 186| loss: 0.36254 | val_0_auc: 0.86432 |  0:05:55s\n",
      "epoch 187| loss: 0.36485 | val_0_auc: 0.86504 |  0:05:57s\n",
      "epoch 188| loss: 0.36071 | val_0_auc: 0.86372 |  0:05:59s\n",
      "epoch 189| loss: 0.35979 | val_0_auc: 0.86269 |  0:06:01s\n",
      "epoch 190| loss: 0.36123 | val_0_auc: 0.8623  |  0:06:03s\n",
      "epoch 191| loss: 0.36178 | val_0_auc: 0.86279 |  0:06:05s\n",
      "epoch 192| loss: 0.3637  | val_0_auc: 0.86313 |  0:06:07s\n",
      "epoch 193| loss: 0.36107 | val_0_auc: 0.86089 |  0:06:09s\n",
      "epoch 194| loss: 0.36056 | val_0_auc: 0.86341 |  0:06:11s\n",
      "epoch 195| loss: 0.35864 | val_0_auc: 0.86243 |  0:06:13s\n",
      "epoch 196| loss: 0.35987 | val_0_auc: 0.86255 |  0:06:15s\n",
      "epoch 197| loss: 0.36094 | val_0_auc: 0.86191 |  0:06:17s\n",
      "epoch 198| loss: 0.36194 | val_0_auc: 0.86283 |  0:06:18s\n",
      "epoch 199| loss: 0.35977 | val_0_auc: 0.86221 |  0:06:20s\n",
      "epoch 200| loss: 0.36029 | val_0_auc: 0.86278 |  0:06:22s\n",
      "epoch 201| loss: 0.35911 | val_0_auc: 0.86229 |  0:06:24s\n",
      "epoch 202| loss: 0.35905 | val_0_auc: 0.86306 |  0:06:26s\n",
      "epoch 203| loss: 0.35744 | val_0_auc: 0.86392 |  0:06:28s\n",
      "epoch 204| loss: 0.35701 | val_0_auc: 0.86272 |  0:06:30s\n",
      "epoch 205| loss: 0.35518 | val_0_auc: 0.86306 |  0:06:32s\n",
      "epoch 206| loss: 0.35775 | val_0_auc: 0.86295 |  0:06:34s\n",
      "epoch 207| loss: 0.3569  | val_0_auc: 0.86339 |  0:06:35s\n",
      "\n",
      "Early stopping occurred at epoch 207 with best_epoch = 187 and best_val_0_auc = 0.86504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81651 | val_0_auc: 0.46876 |  0:00:00s\n",
      "epoch 1  | loss: 0.64363 | val_0_auc: 0.62117 |  0:00:01s\n",
      "epoch 2  | loss: 0.54877 | val_0_auc: 0.68464 |  0:00:02s\n",
      "epoch 3  | loss: 0.47567 | val_0_auc: 0.77912 |  0:00:03s\n",
      "epoch 4  | loss: 0.42228 | val_0_auc: 0.82942 |  0:00:04s\n",
      "epoch 5  | loss: 0.39058 | val_0_auc: 0.85413 |  0:00:05s\n",
      "epoch 6  | loss: 0.37024 | val_0_auc: 0.86417 |  0:00:06s\n",
      "epoch 7  | loss: 0.36134 | val_0_auc: 0.872   |  0:00:07s\n",
      "epoch 8  | loss: 0.35057 | val_0_auc: 0.87936 |  0:00:08s\n",
      "epoch 9  | loss: 0.34097 | val_0_auc: 0.88571 |  0:00:09s\n",
      "epoch 10 | loss: 0.33652 | val_0_auc: 0.88553 |  0:00:10s\n",
      "epoch 11 | loss: 0.3361  | val_0_auc: 0.88936 |  0:00:11s\n",
      "epoch 12 | loss: 0.33011 | val_0_auc: 0.89146 |  0:00:12s\n",
      "epoch 13 | loss: 0.32686 | val_0_auc: 0.89068 |  0:00:13s\n",
      "epoch 14 | loss: 0.32326 | val_0_auc: 0.89398 |  0:00:14s\n",
      "epoch 15 | loss: 0.32218 | val_0_auc: 0.89513 |  0:00:15s\n",
      "epoch 16 | loss: 0.31907 | val_0_auc: 0.89501 |  0:00:15s\n",
      "epoch 17 | loss: 0.31921 | val_0_auc: 0.89612 |  0:00:16s\n",
      "epoch 18 | loss: 0.31661 | val_0_auc: 0.89696 |  0:00:17s\n",
      "epoch 19 | loss: 0.31212 | val_0_auc: 0.89742 |  0:00:18s\n",
      "epoch 20 | loss: 0.31205 | val_0_auc: 0.89637 |  0:00:19s\n",
      "epoch 21 | loss: 0.30961 | val_0_auc: 0.89643 |  0:00:20s\n",
      "epoch 22 | loss: 0.30655 | val_0_auc: 0.89531 |  0:00:21s\n",
      "epoch 23 | loss: 0.30835 | val_0_auc: 0.89671 |  0:00:22s\n",
      "epoch 24 | loss: 0.3018  | val_0_auc: 0.89835 |  0:00:23s\n",
      "epoch 25 | loss: 0.3021  | val_0_auc: 0.89875 |  0:00:24s\n",
      "epoch 26 | loss: 0.30169 | val_0_auc: 0.89735 |  0:00:25s\n",
      "epoch 27 | loss: 0.30066 | val_0_auc: 0.89795 |  0:00:26s\n",
      "epoch 28 | loss: 0.29824 | val_0_auc: 0.89728 |  0:00:27s\n",
      "epoch 29 | loss: 0.2931  | val_0_auc: 0.89738 |  0:00:28s\n",
      "epoch 30 | loss: 0.29529 | val_0_auc: 0.89835 |  0:00:28s\n",
      "epoch 31 | loss: 0.2946  | val_0_auc: 0.89804 |  0:00:29s\n",
      "epoch 32 | loss: 0.29363 | val_0_auc: 0.89852 |  0:00:30s\n",
      "epoch 33 | loss: 0.29148 | val_0_auc: 0.89756 |  0:00:31s\n",
      "epoch 34 | loss: 0.29188 | val_0_auc: 0.89963 |  0:00:32s\n",
      "epoch 35 | loss: 0.28734 | val_0_auc: 0.89851 |  0:00:33s\n",
      "epoch 36 | loss: 0.28713 | val_0_auc: 0.89671 |  0:00:34s\n",
      "epoch 37 | loss: 0.28323 | val_0_auc: 0.8997  |  0:00:35s\n",
      "epoch 38 | loss: 0.28349 | val_0_auc: 0.89732 |  0:00:36s\n",
      "epoch 39 | loss: 0.28157 | val_0_auc: 0.89743 |  0:00:37s\n",
      "epoch 40 | loss: 0.28461 | val_0_auc: 0.89958 |  0:00:38s\n",
      "epoch 41 | loss: 0.28104 | val_0_auc: 0.89952 |  0:00:39s\n",
      "epoch 42 | loss: 0.28293 | val_0_auc: 0.89893 |  0:00:40s\n",
      "epoch 43 | loss: 0.28045 | val_0_auc: 0.90111 |  0:00:41s\n",
      "epoch 44 | loss: 0.27519 | val_0_auc: 0.89931 |  0:00:42s\n",
      "epoch 45 | loss: 0.27748 | val_0_auc: 0.89984 |  0:00:42s\n",
      "epoch 46 | loss: 0.27771 | val_0_auc: 0.89769 |  0:00:43s\n",
      "epoch 47 | loss: 0.27679 | val_0_auc: 0.89967 |  0:00:44s\n",
      "epoch 48 | loss: 0.27572 | val_0_auc: 0.8988  |  0:00:45s\n",
      "epoch 49 | loss: 0.2759  | val_0_auc: 0.89941 |  0:00:46s\n",
      "epoch 50 | loss: 0.27474 | val_0_auc: 0.89962 |  0:00:47s\n",
      "epoch 51 | loss: 0.2733  | val_0_auc: 0.89942 |  0:00:48s\n",
      "epoch 52 | loss: 0.27223 | val_0_auc: 0.89667 |  0:00:49s\n",
      "epoch 53 | loss: 0.2733  | val_0_auc: 0.89644 |  0:00:50s\n",
      "epoch 54 | loss: 0.27044 | val_0_auc: 0.89977 |  0:00:51s\n",
      "epoch 55 | loss: 0.27082 | val_0_auc: 0.89591 |  0:00:52s\n",
      "epoch 56 | loss: 0.27049 | val_0_auc: 0.8955  |  0:00:53s\n",
      "epoch 57 | loss: 0.27087 | val_0_auc: 0.89607 |  0:00:54s\n",
      "epoch 58 | loss: 0.26879 | val_0_auc: 0.89523 |  0:00:55s\n",
      "epoch 59 | loss: 0.27039 | val_0_auc: 0.89652 |  0:00:56s\n",
      "epoch 60 | loss: 0.26998 | val_0_auc: 0.89787 |  0:00:57s\n",
      "epoch 61 | loss: 0.26638 | val_0_auc: 0.89635 |  0:00:57s\n",
      "epoch 62 | loss: 0.26707 | val_0_auc: 0.89442 |  0:00:58s\n",
      "epoch 63 | loss: 0.26614 | val_0_auc: 0.89389 |  0:00:59s\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_0_auc = 0.90111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81071 | val_0_auc: 0.49398 |  0:00:01s\n",
      "epoch 1  | loss: 0.7801  | val_0_auc: 0.53232 |  0:00:02s\n",
      "epoch 2  | loss: 0.74977 | val_0_auc: 0.553   |  0:00:03s\n",
      "epoch 3  | loss: 0.72418 | val_0_auc: 0.57847 |  0:00:05s\n",
      "epoch 4  | loss: 0.70448 | val_0_auc: 0.61071 |  0:00:06s\n",
      "epoch 5  | loss: 0.6869  | val_0_auc: 0.64147 |  0:00:07s\n",
      "epoch 6  | loss: 0.66765 | val_0_auc: 0.66859 |  0:00:09s\n",
      "epoch 7  | loss: 0.64961 | val_0_auc: 0.68949 |  0:00:10s\n",
      "epoch 8  | loss: 0.62914 | val_0_auc: 0.71083 |  0:00:11s\n",
      "epoch 9  | loss: 0.60917 | val_0_auc: 0.72893 |  0:00:12s\n",
      "epoch 10 | loss: 0.59205 | val_0_auc: 0.74494 |  0:00:14s\n",
      "epoch 11 | loss: 0.57591 | val_0_auc: 0.75991 |  0:00:15s\n",
      "epoch 12 | loss: 0.5602  | val_0_auc: 0.76974 |  0:00:16s\n",
      "epoch 13 | loss: 0.54724 | val_0_auc: 0.77752 |  0:00:18s\n",
      "epoch 14 | loss: 0.53464 | val_0_auc: 0.78694 |  0:00:19s\n",
      "epoch 15 | loss: 0.52135 | val_0_auc: 0.79476 |  0:00:20s\n",
      "epoch 16 | loss: 0.51004 | val_0_auc: 0.79992 |  0:00:22s\n",
      "epoch 17 | loss: 0.49765 | val_0_auc: 0.80648 |  0:00:23s\n",
      "epoch 18 | loss: 0.48755 | val_0_auc: 0.8111  |  0:00:24s\n",
      "epoch 19 | loss: 0.47868 | val_0_auc: 0.8156  |  0:00:26s\n",
      "epoch 20 | loss: 0.46992 | val_0_auc: 0.81911 |  0:00:28s\n",
      "epoch 21 | loss: 0.46343 | val_0_auc: 0.82233 |  0:00:30s\n",
      "epoch 22 | loss: 0.45609 | val_0_auc: 0.82531 |  0:00:31s\n",
      "epoch 23 | loss: 0.44864 | val_0_auc: 0.82848 |  0:00:32s\n",
      "epoch 24 | loss: 0.4441  | val_0_auc: 0.83141 |  0:00:34s\n",
      "epoch 25 | loss: 0.43517 | val_0_auc: 0.83342 |  0:00:35s\n",
      "epoch 26 | loss: 0.43133 | val_0_auc: 0.83602 |  0:00:36s\n",
      "epoch 27 | loss: 0.42609 | val_0_auc: 0.83889 |  0:00:37s\n",
      "epoch 28 | loss: 0.41998 | val_0_auc: 0.84228 |  0:00:39s\n",
      "epoch 29 | loss: 0.41468 | val_0_auc: 0.84575 |  0:00:40s\n",
      "epoch 30 | loss: 0.4112  | val_0_auc: 0.84763 |  0:00:41s\n",
      "epoch 31 | loss: 0.40738 | val_0_auc: 0.85007 |  0:00:43s\n",
      "epoch 32 | loss: 0.40404 | val_0_auc: 0.85127 |  0:00:44s\n",
      "epoch 33 | loss: 0.39972 | val_0_auc: 0.85322 |  0:00:45s\n",
      "epoch 34 | loss: 0.39796 | val_0_auc: 0.85527 |  0:00:47s\n",
      "epoch 35 | loss: 0.39615 | val_0_auc: 0.85583 |  0:00:48s\n",
      "epoch 36 | loss: 0.39138 | val_0_auc: 0.85669 |  0:00:50s\n",
      "epoch 37 | loss: 0.39062 | val_0_auc: 0.8583  |  0:00:51s\n",
      "epoch 38 | loss: 0.38714 | val_0_auc: 0.86007 |  0:00:52s\n",
      "epoch 39 | loss: 0.38372 | val_0_auc: 0.86285 |  0:00:53s\n",
      "epoch 40 | loss: 0.38155 | val_0_auc: 0.86511 |  0:00:55s\n",
      "epoch 41 | loss: 0.37935 | val_0_auc: 0.86478 |  0:00:56s\n",
      "epoch 42 | loss: 0.37636 | val_0_auc: 0.86629 |  0:00:57s\n",
      "epoch 43 | loss: 0.37676 | val_0_auc: 0.86739 |  0:00:59s\n",
      "epoch 44 | loss: 0.37237 | val_0_auc: 0.86821 |  0:01:00s\n",
      "epoch 45 | loss: 0.3695  | val_0_auc: 0.86896 |  0:01:01s\n",
      "epoch 46 | loss: 0.36972 | val_0_auc: 0.87125 |  0:01:02s\n",
      "epoch 47 | loss: 0.36697 | val_0_auc: 0.87116 |  0:01:04s\n",
      "epoch 48 | loss: 0.36519 | val_0_auc: 0.87228 |  0:01:05s\n",
      "epoch 49 | loss: 0.36343 | val_0_auc: 0.87461 |  0:01:06s\n",
      "epoch 50 | loss: 0.36163 | val_0_auc: 0.87468 |  0:01:07s\n",
      "epoch 51 | loss: 0.35932 | val_0_auc: 0.87574 |  0:01:09s\n",
      "epoch 52 | loss: 0.35977 | val_0_auc: 0.87652 |  0:01:10s\n",
      "epoch 53 | loss: 0.3544  | val_0_auc: 0.87655 |  0:01:11s\n",
      "epoch 54 | loss: 0.3564  | val_0_auc: 0.87725 |  0:01:13s\n",
      "epoch 55 | loss: 0.35281 | val_0_auc: 0.87779 |  0:01:14s\n",
      "epoch 56 | loss: 0.35228 | val_0_auc: 0.87914 |  0:01:15s\n",
      "epoch 57 | loss: 0.34949 | val_0_auc: 0.8789  |  0:01:16s\n",
      "epoch 58 | loss: 0.35128 | val_0_auc: 0.87924 |  0:01:18s\n",
      "epoch 59 | loss: 0.34779 | val_0_auc: 0.87998 |  0:01:19s\n",
      "epoch 60 | loss: 0.34791 | val_0_auc: 0.88041 |  0:01:20s\n",
      "epoch 61 | loss: 0.34687 | val_0_auc: 0.88245 |  0:01:22s\n",
      "epoch 62 | loss: 0.34253 | val_0_auc: 0.8834  |  0:01:23s\n",
      "epoch 63 | loss: 0.34469 | val_0_auc: 0.88303 |  0:01:24s\n",
      "epoch 64 | loss: 0.34374 | val_0_auc: 0.88324 |  0:01:25s\n",
      "epoch 65 | loss: 0.3417  | val_0_auc: 0.88395 |  0:01:27s\n",
      "epoch 66 | loss: 0.34013 | val_0_auc: 0.88384 |  0:01:28s\n",
      "epoch 67 | loss: 0.33673 | val_0_auc: 0.88533 |  0:01:29s\n",
      "epoch 68 | loss: 0.34026 | val_0_auc: 0.88572 |  0:01:30s\n",
      "epoch 69 | loss: 0.33747 | val_0_auc: 0.88579 |  0:01:32s\n",
      "epoch 70 | loss: 0.33409 | val_0_auc: 0.88581 |  0:01:33s\n",
      "epoch 71 | loss: 0.33601 | val_0_auc: 0.8865  |  0:01:34s\n",
      "epoch 72 | loss: 0.33454 | val_0_auc: 0.88592 |  0:01:36s\n",
      "epoch 73 | loss: 0.33296 | val_0_auc: 0.8882  |  0:01:37s\n",
      "epoch 74 | loss: 0.33202 | val_0_auc: 0.88929 |  0:01:38s\n",
      "epoch 75 | loss: 0.33377 | val_0_auc: 0.88867 |  0:01:40s\n",
      "epoch 76 | loss: 0.33111 | val_0_auc: 0.88945 |  0:01:41s\n",
      "epoch 77 | loss: 0.32841 | val_0_auc: 0.88963 |  0:01:42s\n",
      "epoch 78 | loss: 0.33205 | val_0_auc: 0.8894  |  0:01:43s\n",
      "epoch 79 | loss: 0.32858 | val_0_auc: 0.88976 |  0:01:45s\n",
      "epoch 80 | loss: 0.32842 | val_0_auc: 0.89095 |  0:01:46s\n",
      "epoch 81 | loss: 0.32846 | val_0_auc: 0.89027 |  0:01:47s\n",
      "epoch 82 | loss: 0.32817 | val_0_auc: 0.89025 |  0:01:49s\n",
      "epoch 83 | loss: 0.32649 | val_0_auc: 0.89099 |  0:01:50s\n",
      "epoch 84 | loss: 0.32775 | val_0_auc: 0.89122 |  0:01:51s\n",
      "epoch 85 | loss: 0.32643 | val_0_auc: 0.89079 |  0:01:52s\n",
      "epoch 86 | loss: 0.32609 | val_0_auc: 0.89152 |  0:01:54s\n",
      "epoch 87 | loss: 0.32234 | val_0_auc: 0.89137 |  0:01:55s\n",
      "epoch 88 | loss: 0.32693 | val_0_auc: 0.89258 |  0:01:56s\n",
      "epoch 89 | loss: 0.32331 | val_0_auc: 0.89352 |  0:01:58s\n",
      "epoch 90 | loss: 0.32264 | val_0_auc: 0.89419 |  0:01:59s\n",
      "epoch 91 | loss: 0.32395 | val_0_auc: 0.89366 |  0:02:00s\n",
      "epoch 92 | loss: 0.32091 | val_0_auc: 0.89278 |  0:02:01s\n",
      "epoch 93 | loss: 0.32157 | val_0_auc: 0.89354 |  0:02:03s\n",
      "epoch 94 | loss: 0.31992 | val_0_auc: 0.89379 |  0:02:04s\n",
      "epoch 95 | loss: 0.32032 | val_0_auc: 0.89342 |  0:02:05s\n",
      "epoch 96 | loss: 0.32095 | val_0_auc: 0.89374 |  0:02:06s\n",
      "epoch 97 | loss: 0.31775 | val_0_auc: 0.89395 |  0:02:08s\n",
      "epoch 98 | loss: 0.3177  | val_0_auc: 0.89413 |  0:02:09s\n",
      "epoch 99 | loss: 0.31807 | val_0_auc: 0.89462 |  0:02:10s\n",
      "epoch 100| loss: 0.3178  | val_0_auc: 0.89518 |  0:02:12s\n",
      "epoch 101| loss: 0.31732 | val_0_auc: 0.89503 |  0:02:13s\n",
      "epoch 102| loss: 0.31509 | val_0_auc: 0.89548 |  0:02:14s\n",
      "epoch 103| loss: 0.31681 | val_0_auc: 0.8949  |  0:02:15s\n",
      "epoch 104| loss: 0.31767 | val_0_auc: 0.89548 |  0:02:17s\n",
      "epoch 105| loss: 0.31539 | val_0_auc: 0.89519 |  0:02:18s\n",
      "epoch 106| loss: 0.31592 | val_0_auc: 0.89582 |  0:02:19s\n",
      "epoch 107| loss: 0.31508 | val_0_auc: 0.89592 |  0:02:21s\n",
      "epoch 108| loss: 0.31572 | val_0_auc: 0.89598 |  0:02:22s\n",
      "epoch 109| loss: 0.31387 | val_0_auc: 0.8957  |  0:02:23s\n",
      "epoch 110| loss: 0.31382 | val_0_auc: 0.89599 |  0:02:24s\n",
      "epoch 111| loss: 0.31221 | val_0_auc: 0.89713 |  0:02:26s\n",
      "epoch 112| loss: 0.31194 | val_0_auc: 0.89697 |  0:02:27s\n",
      "epoch 113| loss: 0.31305 | val_0_auc: 0.89749 |  0:02:28s\n",
      "epoch 114| loss: 0.31127 | val_0_auc: 0.89597 |  0:02:30s\n",
      "epoch 115| loss: 0.30987 | val_0_auc: 0.89537 |  0:02:31s\n",
      "epoch 116| loss: 0.3115  | val_0_auc: 0.89542 |  0:02:32s\n",
      "epoch 117| loss: 0.31131 | val_0_auc: 0.89595 |  0:02:33s\n",
      "epoch 118| loss: 0.31109 | val_0_auc: 0.8964  |  0:02:35s\n",
      "epoch 119| loss: 0.30649 | val_0_auc: 0.89698 |  0:02:36s\n",
      "epoch 120| loss: 0.30889 | val_0_auc: 0.89643 |  0:02:37s\n",
      "epoch 121| loss: 0.30756 | val_0_auc: 0.89775 |  0:02:38s\n",
      "epoch 122| loss: 0.30997 | val_0_auc: 0.89738 |  0:02:40s\n",
      "epoch 123| loss: 0.30626 | val_0_auc: 0.89795 |  0:02:41s\n",
      "epoch 124| loss: 0.3055  | val_0_auc: 0.89823 |  0:02:42s\n",
      "epoch 125| loss: 0.30827 | val_0_auc: 0.89799 |  0:02:43s\n",
      "epoch 126| loss: 0.30511 | val_0_auc: 0.89755 |  0:02:45s\n",
      "epoch 127| loss: 0.30527 | val_0_auc: 0.89753 |  0:02:46s\n",
      "epoch 128| loss: 0.30576 | val_0_auc: 0.89737 |  0:02:47s\n",
      "epoch 129| loss: 0.30536 | val_0_auc: 0.89773 |  0:02:49s\n",
      "epoch 130| loss: 0.30519 | val_0_auc: 0.89782 |  0:02:50s\n",
      "epoch 131| loss: 0.3049  | val_0_auc: 0.898   |  0:02:51s\n",
      "epoch 132| loss: 0.30607 | val_0_auc: 0.89899 |  0:02:53s\n",
      "epoch 133| loss: 0.30402 | val_0_auc: 0.89864 |  0:02:54s\n",
      "epoch 134| loss: 0.30612 | val_0_auc: 0.89868 |  0:02:55s\n",
      "epoch 135| loss: 0.30344 | val_0_auc: 0.89866 |  0:02:56s\n",
      "epoch 136| loss: 0.30122 | val_0_auc: 0.89807 |  0:02:58s\n",
      "epoch 137| loss: 0.30098 | val_0_auc: 0.89764 |  0:02:59s\n",
      "epoch 138| loss: 0.30363 | val_0_auc: 0.8972  |  0:03:00s\n",
      "epoch 139| loss: 0.3002  | val_0_auc: 0.89717 |  0:03:01s\n",
      "epoch 140| loss: 0.30002 | val_0_auc: 0.89707 |  0:03:03s\n",
      "epoch 141| loss: 0.30034 | val_0_auc: 0.8971  |  0:03:04s\n",
      "epoch 142| loss: 0.29963 | val_0_auc: 0.89724 |  0:03:05s\n",
      "epoch 143| loss: 0.29988 | val_0_auc: 0.89822 |  0:03:07s\n",
      "epoch 144| loss: 0.29853 | val_0_auc: 0.89764 |  0:03:08s\n",
      "epoch 145| loss: 0.30042 | val_0_auc: 0.89805 |  0:03:09s\n",
      "epoch 146| loss: 0.30082 | val_0_auc: 0.89839 |  0:03:11s\n",
      "epoch 147| loss: 0.29644 | val_0_auc: 0.89881 |  0:03:12s\n",
      "epoch 148| loss: 0.29652 | val_0_auc: 0.89801 |  0:03:13s\n",
      "epoch 149| loss: 0.29881 | val_0_auc: 0.89837 |  0:03:15s\n",
      "epoch 150| loss: 0.29806 | val_0_auc: 0.90005 |  0:03:16s\n",
      "epoch 151| loss: 0.29663 | val_0_auc: 0.89915 |  0:03:17s\n",
      "epoch 152| loss: 0.29321 | val_0_auc: 0.89713 |  0:03:18s\n",
      "epoch 153| loss: 0.29317 | val_0_auc: 0.89781 |  0:03:20s\n",
      "epoch 154| loss: 0.2931  | val_0_auc: 0.89769 |  0:03:21s\n",
      "epoch 155| loss: 0.2898  | val_0_auc: 0.89666 |  0:03:22s\n",
      "epoch 156| loss: 0.28871 | val_0_auc: 0.89634 |  0:03:24s\n",
      "epoch 157| loss: 0.29009 | val_0_auc: 0.89759 |  0:03:25s\n",
      "epoch 158| loss: 0.29106 | val_0_auc: 0.89803 |  0:03:26s\n",
      "epoch 159| loss: 0.29171 | val_0_auc: 0.89813 |  0:03:27s\n",
      "epoch 160| loss: 0.28994 | val_0_auc: 0.89852 |  0:03:29s\n",
      "epoch 161| loss: 0.29073 | val_0_auc: 0.89945 |  0:03:30s\n",
      "epoch 162| loss: 0.28854 | val_0_auc: 0.89876 |  0:03:31s\n",
      "epoch 163| loss: 0.28852 | val_0_auc: 0.89888 |  0:03:32s\n",
      "epoch 164| loss: 0.28871 | val_0_auc: 0.89947 |  0:03:34s\n",
      "epoch 165| loss: 0.28687 | val_0_auc: 0.90048 |  0:03:35s\n",
      "epoch 166| loss: 0.28727 | val_0_auc: 0.89842 |  0:03:36s\n",
      "epoch 167| loss: 0.28952 | val_0_auc: 0.89923 |  0:03:38s\n",
      "epoch 168| loss: 0.28612 | val_0_auc: 0.90019 |  0:03:39s\n",
      "epoch 169| loss: 0.28714 | val_0_auc: 0.90022 |  0:03:40s\n",
      "epoch 170| loss: 0.28506 | val_0_auc: 0.9002  |  0:03:41s\n",
      "epoch 171| loss: 0.28372 | val_0_auc: 0.89972 |  0:03:43s\n",
      "epoch 172| loss: 0.28495 | val_0_auc: 0.8988  |  0:03:44s\n",
      "epoch 173| loss: 0.28428 | val_0_auc: 0.89935 |  0:03:45s\n",
      "epoch 174| loss: 0.28519 | val_0_auc: 0.89888 |  0:03:47s\n",
      "epoch 175| loss: 0.28453 | val_0_auc: 0.89869 |  0:03:48s\n",
      "epoch 176| loss: 0.28538 | val_0_auc: 0.898   |  0:03:49s\n",
      "epoch 177| loss: 0.28252 | val_0_auc: 0.89899 |  0:03:50s\n",
      "epoch 178| loss: 0.2828  | val_0_auc: 0.89956 |  0:03:52s\n",
      "epoch 179| loss: 0.28405 | val_0_auc: 0.89943 |  0:03:53s\n",
      "epoch 180| loss: 0.28221 | val_0_auc: 0.89857 |  0:03:54s\n",
      "epoch 181| loss: 0.28522 | val_0_auc: 0.89792 |  0:03:56s\n",
      "epoch 182| loss: 0.28289 | val_0_auc: 0.89888 |  0:03:57s\n",
      "epoch 183| loss: 0.28237 | val_0_auc: 0.89753 |  0:03:58s\n",
      "epoch 184| loss: 0.28221 | val_0_auc: 0.89763 |  0:04:00s\n",
      "epoch 185| loss: 0.27976 | val_0_auc: 0.89936 |  0:04:01s\n",
      "\n",
      "Early stopping occurred at epoch 185 with best_epoch = 165 and best_val_0_auc = 0.90048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.48915 | val_0_auc: 0.72706 |  0:00:01s\n",
      "epoch 1  | loss: 0.42079 | val_0_auc: 0.7669  |  0:00:02s\n",
      "epoch 2  | loss: 0.40405 | val_0_auc: 0.79566 |  0:00:03s\n",
      "epoch 3  | loss: 0.3918  | val_0_auc: 0.82805 |  0:00:04s\n",
      "epoch 4  | loss: 0.37681 | val_0_auc: 0.83409 |  0:00:06s\n",
      "epoch 5  | loss: 0.35679 | val_0_auc: 0.85907 |  0:00:07s\n",
      "epoch 6  | loss: 0.34227 | val_0_auc: 0.87654 |  0:00:09s\n",
      "epoch 7  | loss: 0.32597 | val_0_auc: 0.86733 |  0:00:10s\n",
      "epoch 8  | loss: 0.31425 | val_0_auc: 0.89214 |  0:00:11s\n",
      "epoch 9  | loss: 0.30432 | val_0_auc: 0.90139 |  0:00:12s\n",
      "epoch 10 | loss: 0.29809 | val_0_auc: 0.89918 |  0:00:13s\n",
      "epoch 11 | loss: 0.29796 | val_0_auc: 0.89963 |  0:00:14s\n",
      "epoch 12 | loss: 0.29284 | val_0_auc: 0.90713 |  0:00:15s\n",
      "epoch 13 | loss: 0.28791 | val_0_auc: 0.90383 |  0:00:16s\n",
      "epoch 14 | loss: 0.28826 | val_0_auc: 0.91009 |  0:00:17s\n",
      "epoch 15 | loss: 0.28693 | val_0_auc: 0.9143  |  0:00:18s\n",
      "epoch 16 | loss: 0.2834  | val_0_auc: 0.91141 |  0:00:19s\n",
      "epoch 17 | loss: 0.28471 | val_0_auc: 0.90958 |  0:00:20s\n",
      "epoch 18 | loss: 0.28119 | val_0_auc: 0.91573 |  0:00:21s\n",
      "epoch 19 | loss: 0.28054 | val_0_auc: 0.91695 |  0:00:22s\n",
      "epoch 20 | loss: 0.27871 | val_0_auc: 0.91573 |  0:00:23s\n",
      "epoch 21 | loss: 0.27507 | val_0_auc: 0.91412 |  0:00:24s\n",
      "epoch 22 | loss: 0.27372 | val_0_auc: 0.91514 |  0:00:25s\n",
      "epoch 23 | loss: 0.2753  | val_0_auc: 0.91606 |  0:00:26s\n",
      "epoch 24 | loss: 0.27322 | val_0_auc: 0.91683 |  0:00:28s\n",
      "epoch 25 | loss: 0.27186 | val_0_auc: 0.91417 |  0:00:29s\n",
      "epoch 26 | loss: 0.26987 | val_0_auc: 0.91424 |  0:00:30s\n",
      "epoch 27 | loss: 0.27348 | val_0_auc: 0.91534 |  0:00:31s\n",
      "epoch 28 | loss: 0.27133 | val_0_auc: 0.91495 |  0:00:32s\n",
      "epoch 29 | loss: 0.27057 | val_0_auc: 0.91409 |  0:00:33s\n",
      "epoch 30 | loss: 0.26924 | val_0_auc: 0.91633 |  0:00:34s\n",
      "epoch 31 | loss: 0.26825 | val_0_auc: 0.91374 |  0:00:35s\n",
      "epoch 32 | loss: 0.26963 | val_0_auc: 0.91579 |  0:00:36s\n",
      "epoch 33 | loss: 0.26487 | val_0_auc: 0.91603 |  0:00:37s\n",
      "epoch 34 | loss: 0.26735 | val_0_auc: 0.91727 |  0:00:38s\n",
      "epoch 35 | loss: 0.26563 | val_0_auc: 0.91762 |  0:00:39s\n",
      "epoch 36 | loss: 0.26488 | val_0_auc: 0.91619 |  0:00:40s\n",
      "epoch 37 | loss: 0.26355 | val_0_auc: 0.91558 |  0:00:41s\n",
      "epoch 38 | loss: 0.26134 | val_0_auc: 0.91696 |  0:00:42s\n",
      "epoch 39 | loss: 0.26391 | val_0_auc: 0.91609 |  0:00:43s\n",
      "epoch 40 | loss: 0.2651  | val_0_auc: 0.91554 |  0:00:44s\n",
      "epoch 41 | loss: 0.26348 | val_0_auc: 0.91622 |  0:00:45s\n",
      "epoch 42 | loss: 0.25962 | val_0_auc: 0.91557 |  0:00:46s\n",
      "epoch 43 | loss: 0.26301 | val_0_auc: 0.91637 |  0:00:47s\n",
      "epoch 44 | loss: 0.26094 | val_0_auc: 0.91544 |  0:00:48s\n",
      "epoch 45 | loss: 0.25954 | val_0_auc: 0.91648 |  0:00:49s\n",
      "epoch 46 | loss: 0.25934 | val_0_auc: 0.91443 |  0:00:50s\n",
      "epoch 47 | loss: 0.25697 | val_0_auc: 0.91871 |  0:00:51s\n",
      "epoch 48 | loss: 0.25786 | val_0_auc: 0.91671 |  0:00:52s\n",
      "epoch 49 | loss: 0.25664 | val_0_auc: 0.91534 |  0:00:54s\n",
      "epoch 50 | loss: 0.25802 | val_0_auc: 0.91314 |  0:00:55s\n",
      "epoch 51 | loss: 0.25584 | val_0_auc: 0.91537 |  0:00:56s\n",
      "epoch 52 | loss: 0.25515 | val_0_auc: 0.91777 |  0:00:57s\n",
      "epoch 53 | loss: 0.25513 | val_0_auc: 0.91552 |  0:00:58s\n",
      "epoch 54 | loss: 0.25896 | val_0_auc: 0.91442 |  0:00:59s\n",
      "epoch 55 | loss: 0.25335 | val_0_auc: 0.91513 |  0:01:00s\n",
      "epoch 56 | loss: 0.25257 | val_0_auc: 0.91725 |  0:01:01s\n",
      "epoch 57 | loss: 0.25172 | val_0_auc: 0.91571 |  0:01:02s\n",
      "epoch 58 | loss: 0.2546  | val_0_auc: 0.91418 |  0:01:03s\n",
      "epoch 59 | loss: 0.25511 | val_0_auc: 0.91565 |  0:01:04s\n",
      "epoch 60 | loss: 0.24989 | val_0_auc: 0.91653 |  0:01:05s\n",
      "epoch 61 | loss: 0.25272 | val_0_auc: 0.91365 |  0:01:06s\n",
      "epoch 62 | loss: 0.25126 | val_0_auc: 0.91465 |  0:01:07s\n",
      "epoch 63 | loss: 0.25104 | val_0_auc: 0.91468 |  0:01:08s\n",
      "epoch 64 | loss: 0.25122 | val_0_auc: 0.91424 |  0:01:09s\n",
      "epoch 65 | loss: 0.25175 | val_0_auc: 0.90921 |  0:01:10s\n",
      "epoch 66 | loss: 0.24954 | val_0_auc: 0.91523 |  0:01:11s\n",
      "epoch 67 | loss: 0.25077 | val_0_auc: 0.91178 |  0:01:12s\n",
      "\n",
      "Early stopping occurred at epoch 67 with best_epoch = 47 and best_val_0_auc = 0.91871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.62153 | val_0_auc: 0.83576 |  0:00:03s\n",
      "epoch 1  | loss: 0.37556 | val_0_auc: 0.86106 |  0:00:07s\n",
      "epoch 2  | loss: 0.33724 | val_0_auc: 0.87156 |  0:00:11s\n",
      "epoch 3  | loss: 0.31317 | val_0_auc: 0.88019 |  0:00:15s\n",
      "epoch 4  | loss: 0.30162 | val_0_auc: 0.89008 |  0:00:19s\n",
      "epoch 5  | loss: 0.29007 | val_0_auc: 0.89073 |  0:00:24s\n",
      "epoch 6  | loss: 0.28425 | val_0_auc: 0.89406 |  0:00:28s\n",
      "epoch 7  | loss: 0.27913 | val_0_auc: 0.90038 |  0:00:32s\n",
      "epoch 8  | loss: 0.26795 | val_0_auc: 0.90307 |  0:00:36s\n",
      "epoch 9  | loss: 0.26322 | val_0_auc: 0.90169 |  0:00:40s\n",
      "epoch 10 | loss: 0.2557  | val_0_auc: 0.90376 |  0:00:43s\n",
      "epoch 11 | loss: 0.2492  | val_0_auc: 0.90332 |  0:00:47s\n",
      "epoch 12 | loss: 0.24282 | val_0_auc: 0.90408 |  0:00:51s\n",
      "epoch 13 | loss: 0.24187 | val_0_auc: 0.90154 |  0:00:55s\n",
      "epoch 14 | loss: 0.23792 | val_0_auc: 0.90094 |  0:00:59s\n",
      "epoch 15 | loss: 0.23123 | val_0_auc: 0.89888 |  0:01:03s\n",
      "epoch 16 | loss: 0.22232 | val_0_auc: 0.89997 |  0:01:07s\n",
      "epoch 17 | loss: 0.21806 | val_0_auc: 0.8938  |  0:01:10s\n",
      "epoch 18 | loss: 0.21622 | val_0_auc: 0.89638 |  0:01:14s\n",
      "epoch 19 | loss: 0.20968 | val_0_auc: 0.89176 |  0:01:18s\n",
      "epoch 20 | loss: 0.20714 | val_0_auc: 0.89159 |  0:01:23s\n",
      "epoch 21 | loss: 0.19891 | val_0_auc: 0.89466 |  0:01:27s\n",
      "epoch 22 | loss: 0.1924  | val_0_auc: 0.88974 |  0:01:31s\n",
      "epoch 23 | loss: 0.19078 | val_0_auc: 0.88934 |  0:01:35s\n",
      "epoch 24 | loss: 0.18557 | val_0_auc: 0.88674 |  0:01:39s\n",
      "epoch 25 | loss: 0.18551 | val_0_auc: 0.89347 |  0:01:43s\n",
      "epoch 26 | loss: 0.18097 | val_0_auc: 0.88517 |  0:01:47s\n",
      "epoch 27 | loss: 0.17269 | val_0_auc: 0.8852  |  0:01:51s\n",
      "epoch 28 | loss: 0.16588 | val_0_auc: 0.88229 |  0:01:55s\n",
      "epoch 29 | loss: 0.16357 | val_0_auc: 0.88875 |  0:01:59s\n",
      "epoch 30 | loss: 0.1643  | val_0_auc: 0.8897  |  0:02:03s\n",
      "epoch 31 | loss: 0.16116 | val_0_auc: 0.88469 |  0:02:07s\n",
      "epoch 32 | loss: 0.15575 | val_0_auc: 0.88373 |  0:02:12s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_0_auc = 0.90408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.70361 | val_0_auc: 0.46309 |  0:00:02s\n",
      "epoch 1  | loss: 4.26058 | val_0_auc: 0.46342 |  0:00:06s\n",
      "epoch 2  | loss: 3.89307 | val_0_auc: 0.47532 |  0:00:09s\n",
      "epoch 3  | loss: 3.59726 | val_0_auc: 0.48397 |  0:00:12s\n",
      "epoch 4  | loss: 3.233   | val_0_auc: 0.50317 |  0:00:15s\n",
      "epoch 5  | loss: 2.94774 | val_0_auc: 0.49444 |  0:00:18s\n",
      "epoch 6  | loss: 2.7128  | val_0_auc: 0.5061  |  0:00:21s\n",
      "epoch 7  | loss: 2.49528 | val_0_auc: 0.50394 |  0:00:24s\n",
      "epoch 8  | loss: 2.2537  | val_0_auc: 0.51677 |  0:00:27s\n",
      "epoch 9  | loss: 2.07372 | val_0_auc: 0.5002  |  0:00:31s\n",
      "epoch 10 | loss: 1.88767 | val_0_auc: 0.49598 |  0:00:34s\n",
      "epoch 11 | loss: 1.7034  | val_0_auc: 0.51079 |  0:00:37s\n",
      "epoch 12 | loss: 1.57882 | val_0_auc: 0.51499 |  0:00:40s\n",
      "epoch 13 | loss: 1.4799  | val_0_auc: 0.53837 |  0:00:42s\n",
      "epoch 14 | loss: 1.40115 | val_0_auc: 0.54116 |  0:00:46s\n",
      "epoch 15 | loss: 1.32693 | val_0_auc: 0.55373 |  0:00:49s\n",
      "epoch 16 | loss: 1.23932 | val_0_auc: 0.56357 |  0:00:52s\n",
      "epoch 17 | loss: 1.15429 | val_0_auc: 0.56646 |  0:00:55s\n",
      "epoch 18 | loss: 1.0994  | val_0_auc: 0.56896 |  0:00:58s\n",
      "epoch 19 | loss: 1.03082 | val_0_auc: 0.5813  |  0:01:01s\n",
      "epoch 20 | loss: 0.9753  | val_0_auc: 0.58696 |  0:01:04s\n",
      "epoch 21 | loss: 0.90748 | val_0_auc: 0.59878 |  0:01:06s\n",
      "epoch 22 | loss: 0.84178 | val_0_auc: 0.59869 |  0:01:09s\n",
      "epoch 23 | loss: 0.79844 | val_0_auc: 0.60767 |  0:01:12s\n",
      "epoch 24 | loss: 0.75336 | val_0_auc: 0.57946 |  0:01:16s\n",
      "epoch 25 | loss: 0.72257 | val_0_auc: 0.5899  |  0:01:19s\n",
      "epoch 26 | loss: 0.71108 | val_0_auc: 0.57239 |  0:01:23s\n",
      "epoch 27 | loss: 0.68568 | val_0_auc: 0.57077 |  0:01:26s\n",
      "epoch 28 | loss: 0.65992 | val_0_auc: 0.5845  |  0:01:28s\n",
      "epoch 29 | loss: 0.64756 | val_0_auc: 0.58787 |  0:01:31s\n",
      "epoch 30 | loss: 0.62899 | val_0_auc: 0.5767  |  0:01:34s\n",
      "epoch 31 | loss: 0.61187 | val_0_auc: 0.58552 |  0:01:37s\n",
      "epoch 32 | loss: 0.60209 | val_0_auc: 0.60113 |  0:01:41s\n",
      "epoch 33 | loss: 0.58896 | val_0_auc: 0.59097 |  0:01:43s\n",
      "epoch 34 | loss: 0.58234 | val_0_auc: 0.62362 |  0:01:46s\n",
      "epoch 35 | loss: 0.5756  | val_0_auc: 0.63433 |  0:01:49s\n",
      "epoch 36 | loss: 0.56996 | val_0_auc: 0.64861 |  0:01:52s\n",
      "epoch 37 | loss: 0.56585 | val_0_auc: 0.64597 |  0:01:55s\n",
      "epoch 38 | loss: 0.56123 | val_0_auc: 0.66119 |  0:01:58s\n",
      "epoch 39 | loss: 0.55732 | val_0_auc: 0.66601 |  0:02:01s\n",
      "epoch 40 | loss: 0.55241 | val_0_auc: 0.68088 |  0:02:05s\n",
      "epoch 41 | loss: 0.54947 | val_0_auc: 0.68925 |  0:02:08s\n",
      "epoch 42 | loss: 0.543   | val_0_auc: 0.7055  |  0:02:11s\n",
      "epoch 43 | loss: 0.53907 | val_0_auc: 0.71324 |  0:02:14s\n",
      "epoch 44 | loss: 0.53307 | val_0_auc: 0.72952 |  0:02:17s\n",
      "epoch 45 | loss: 0.52644 | val_0_auc: 0.74172 |  0:02:20s\n",
      "epoch 46 | loss: 0.523   | val_0_auc: 0.74686 |  0:02:23s\n",
      "epoch 47 | loss: 0.51916 | val_0_auc: 0.75128 |  0:02:27s\n",
      "epoch 48 | loss: 0.51857 | val_0_auc: 0.7484  |  0:02:30s\n",
      "epoch 49 | loss: 0.51525 | val_0_auc: 0.75045 |  0:02:33s\n",
      "epoch 50 | loss: 0.51529 | val_0_auc: 0.75274 |  0:02:36s\n",
      "epoch 51 | loss: 0.51158 | val_0_auc: 0.74499 |  0:02:39s\n",
      "epoch 52 | loss: 0.50884 | val_0_auc: 0.75397 |  0:02:42s\n",
      "epoch 53 | loss: 0.50305 | val_0_auc: 0.75825 |  0:02:45s\n",
      "epoch 54 | loss: 0.5007  | val_0_auc: 0.75614 |  0:02:48s\n",
      "epoch 55 | loss: 0.50027 | val_0_auc: 0.75961 |  0:02:51s\n",
      "epoch 56 | loss: 0.50189 | val_0_auc: 0.75666 |  0:02:54s\n",
      "epoch 57 | loss: 0.50127 | val_0_auc: 0.75574 |  0:02:58s\n",
      "epoch 58 | loss: 0.49694 | val_0_auc: 0.75776 |  0:03:02s\n",
      "epoch 59 | loss: 0.49731 | val_0_auc: 0.76765 |  0:03:05s\n",
      "epoch 60 | loss: 0.4994  | val_0_auc: 0.76327 |  0:03:07s\n",
      "epoch 61 | loss: 0.49369 | val_0_auc: 0.77087 |  0:03:10s\n",
      "epoch 62 | loss: 0.49641 | val_0_auc: 0.77478 |  0:03:13s\n",
      "epoch 63 | loss: 0.48643 | val_0_auc: 0.77422 |  0:03:16s\n",
      "epoch 64 | loss: 0.48855 | val_0_auc: 0.77391 |  0:03:19s\n",
      "epoch 65 | loss: 0.48786 | val_0_auc: 0.77727 |  0:03:23s\n",
      "epoch 66 | loss: 0.48723 | val_0_auc: 0.77688 |  0:03:26s\n",
      "epoch 67 | loss: 0.48528 | val_0_auc: 0.77686 |  0:03:29s\n",
      "epoch 68 | loss: 0.48256 | val_0_auc: 0.77958 |  0:03:32s\n",
      "epoch 69 | loss: 0.48479 | val_0_auc: 0.77869 |  0:03:35s\n",
      "epoch 70 | loss: 0.48506 | val_0_auc: 0.78715 |  0:03:38s\n",
      "epoch 71 | loss: 0.48071 | val_0_auc: 0.78303 |  0:03:41s\n",
      "epoch 72 | loss: 0.48403 | val_0_auc: 0.77889 |  0:03:44s\n",
      "epoch 73 | loss: 0.48566 | val_0_auc: 0.78032 |  0:03:47s\n",
      "epoch 74 | loss: 0.48222 | val_0_auc: 0.77749 |  0:03:50s\n",
      "epoch 75 | loss: 0.48637 | val_0_auc: 0.77857 |  0:03:53s\n",
      "epoch 76 | loss: 0.48286 | val_0_auc: 0.77843 |  0:03:56s\n",
      "epoch 77 | loss: 0.48377 | val_0_auc: 0.78081 |  0:04:02s\n",
      "epoch 78 | loss: 0.48392 | val_0_auc: 0.78771 |  0:04:05s\n",
      "epoch 79 | loss: 0.48313 | val_0_auc: 0.7755  |  0:04:08s\n",
      "epoch 80 | loss: 0.4835  | val_0_auc: 0.77377 |  0:04:11s\n",
      "epoch 81 | loss: 0.48395 | val_0_auc: 0.77598 |  0:04:14s\n",
      "epoch 82 | loss: 0.48736 | val_0_auc: 0.77603 |  0:04:17s\n",
      "epoch 83 | loss: 0.48676 | val_0_auc: 0.7786  |  0:04:20s\n",
      "epoch 84 | loss: 0.48242 | val_0_auc: 0.77902 |  0:04:23s\n",
      "epoch 85 | loss: 0.48091 | val_0_auc: 0.78164 |  0:04:26s\n",
      "epoch 86 | loss: 0.47891 | val_0_auc: 0.78501 |  0:04:30s\n",
      "epoch 87 | loss: 0.47858 | val_0_auc: 0.7848  |  0:04:33s\n",
      "epoch 88 | loss: 0.48604 | val_0_auc: 0.77481 |  0:04:38s\n",
      "epoch 89 | loss: 0.4798  | val_0_auc: 0.78619 |  0:04:42s\n",
      "epoch 90 | loss: 0.48001 | val_0_auc: 0.77853 |  0:04:45s\n",
      "epoch 91 | loss: 0.48184 | val_0_auc: 0.77858 |  0:04:49s\n",
      "epoch 92 | loss: 0.47766 | val_0_auc: 0.78129 |  0:04:52s\n",
      "epoch 93 | loss: 0.47701 | val_0_auc: 0.78319 |  0:04:54s\n",
      "epoch 94 | loss: 0.47373 | val_0_auc: 0.78648 |  0:04:57s\n",
      "epoch 95 | loss: 0.4766  | val_0_auc: 0.78841 |  0:05:00s\n",
      "epoch 96 | loss: 0.47413 | val_0_auc: 0.78778 |  0:05:03s\n",
      "epoch 97 | loss: 0.47629 | val_0_auc: 0.79105 |  0:05:06s\n",
      "epoch 98 | loss: 0.47403 | val_0_auc: 0.78837 |  0:05:10s\n",
      "epoch 99 | loss: 0.47416 | val_0_auc: 0.79009 |  0:05:13s\n",
      "epoch 100| loss: 0.47247 | val_0_auc: 0.79799 |  0:05:16s\n",
      "epoch 101| loss: 0.4753  | val_0_auc: 0.79637 |  0:05:19s\n",
      "epoch 102| loss: 0.47196 | val_0_auc: 0.79307 |  0:05:23s\n",
      "epoch 103| loss: 0.47528 | val_0_auc: 0.79522 |  0:05:27s\n",
      "epoch 104| loss: 0.47171 | val_0_auc: 0.79937 |  0:05:30s\n",
      "epoch 105| loss: 0.47467 | val_0_auc: 0.80031 |  0:05:34s\n",
      "epoch 106| loss: 0.47074 | val_0_auc: 0.79651 |  0:05:37s\n",
      "epoch 107| loss: 0.46993 | val_0_auc: 0.79934 |  0:05:40s\n",
      "epoch 108| loss: 0.47254 | val_0_auc: 0.79996 |  0:05:43s\n",
      "epoch 109| loss: 0.47199 | val_0_auc: 0.79514 |  0:05:47s\n",
      "epoch 110| loss: 0.47018 | val_0_auc: 0.79657 |  0:05:50s\n",
      "epoch 111| loss: 0.47054 | val_0_auc: 0.80015 |  0:05:54s\n",
      "epoch 112| loss: 0.47171 | val_0_auc: 0.80175 |  0:05:57s\n",
      "epoch 113| loss: 0.46813 | val_0_auc: 0.79735 |  0:06:00s\n",
      "epoch 114| loss: 0.46828 | val_0_auc: 0.80166 |  0:06:03s\n",
      "epoch 115| loss: 0.46702 | val_0_auc: 0.80598 |  0:06:06s\n",
      "epoch 116| loss: 0.47022 | val_0_auc: 0.79623 |  0:06:09s\n",
      "epoch 117| loss: 0.46691 | val_0_auc: 0.79928 |  0:06:12s\n",
      "epoch 118| loss: 0.47126 | val_0_auc: 0.79689 |  0:06:15s\n",
      "epoch 119| loss: 0.46711 | val_0_auc: 0.80062 |  0:06:18s\n",
      "epoch 120| loss: 0.46683 | val_0_auc: 0.80423 |  0:06:23s\n",
      "epoch 121| loss: 0.4687  | val_0_auc: 0.79951 |  0:06:27s\n",
      "epoch 122| loss: 0.46761 | val_0_auc: 0.80225 |  0:06:30s\n",
      "epoch 123| loss: 0.46562 | val_0_auc: 0.806   |  0:06:35s\n",
      "epoch 124| loss: 0.4649  | val_0_auc: 0.8047  |  0:06:38s\n",
      "epoch 125| loss: 0.46376 | val_0_auc: 0.80603 |  0:06:41s\n",
      "epoch 126| loss: 0.46248 | val_0_auc: 0.80353 |  0:06:45s\n",
      "epoch 127| loss: 0.46121 | val_0_auc: 0.80019 |  0:06:48s\n",
      "epoch 128| loss: 0.46222 | val_0_auc: 0.80251 |  0:06:51s\n",
      "epoch 129| loss: 0.46413 | val_0_auc: 0.80599 |  0:06:53s\n",
      "epoch 130| loss: 0.46078 | val_0_auc: 0.8094  |  0:06:56s\n",
      "epoch 131| loss: 0.464   | val_0_auc: 0.80707 |  0:06:59s\n",
      "epoch 132| loss: 0.46158 | val_0_auc: 0.80908 |  0:07:02s\n",
      "epoch 133| loss: 0.46149 | val_0_auc: 0.80964 |  0:07:05s\n",
      "epoch 134| loss: 0.46333 | val_0_auc: 0.80905 |  0:07:08s\n",
      "epoch 135| loss: 0.4625  | val_0_auc: 0.80503 |  0:07:12s\n",
      "epoch 136| loss: 0.46437 | val_0_auc: 0.80551 |  0:07:15s\n",
      "epoch 137| loss: 0.46144 | val_0_auc: 0.80716 |  0:07:18s\n",
      "epoch 138| loss: 0.46165 | val_0_auc: 0.80505 |  0:07:21s\n",
      "epoch 139| loss: 0.4647  | val_0_auc: 0.80801 |  0:07:24s\n",
      "epoch 140| loss: 0.46338 | val_0_auc: 0.80513 |  0:07:27s\n",
      "epoch 141| loss: 0.4614  | val_0_auc: 0.80794 |  0:07:30s\n",
      "epoch 142| loss: 0.46193 | val_0_auc: 0.80953 |  0:07:33s\n",
      "epoch 143| loss: 0.46124 | val_0_auc: 0.80607 |  0:07:36s\n",
      "epoch 144| loss: 0.46025 | val_0_auc: 0.80934 |  0:07:39s\n",
      "epoch 145| loss: 0.46447 | val_0_auc: 0.81175 |  0:07:42s\n",
      "epoch 146| loss: 0.45902 | val_0_auc: 0.81026 |  0:07:46s\n",
      "epoch 147| loss: 0.45917 | val_0_auc: 0.80935 |  0:07:48s\n",
      "epoch 148| loss: 0.45897 | val_0_auc: 0.80718 |  0:07:51s\n",
      "epoch 149| loss: 0.45852 | val_0_auc: 0.80953 |  0:07:54s\n",
      "epoch 150| loss: 0.45986 | val_0_auc: 0.80967 |  0:07:57s\n",
      "epoch 151| loss: 0.46249 | val_0_auc: 0.81197 |  0:08:00s\n",
      "epoch 152| loss: 0.46112 | val_0_auc: 0.81342 |  0:08:03s\n",
      "epoch 153| loss: 0.46301 | val_0_auc: 0.8133  |  0:08:06s\n",
      "epoch 154| loss: 0.4634  | val_0_auc: 0.81189 |  0:08:09s\n",
      "epoch 155| loss: 0.46073 | val_0_auc: 0.81329 |  0:08:12s\n",
      "epoch 156| loss: 0.45945 | val_0_auc: 0.8062  |  0:08:15s\n",
      "epoch 157| loss: 0.45801 | val_0_auc: 0.81137 |  0:08:18s\n",
      "epoch 158| loss: 0.46168 | val_0_auc: 0.81279 |  0:08:21s\n",
      "epoch 159| loss: 0.45546 | val_0_auc: 0.81437 |  0:08:24s\n",
      "epoch 160| loss: 0.45369 | val_0_auc: 0.81635 |  0:08:27s\n",
      "epoch 161| loss: 0.45342 | val_0_auc: 0.81491 |  0:08:30s\n",
      "epoch 162| loss: 0.45202 | val_0_auc: 0.81665 |  0:08:33s\n",
      "epoch 163| loss: 0.45312 | val_0_auc: 0.8172  |  0:08:36s\n",
      "epoch 164| loss: 0.45608 | val_0_auc: 0.82036 |  0:08:40s\n",
      "epoch 165| loss: 0.45117 | val_0_auc: 0.81946 |  0:08:43s\n",
      "epoch 166| loss: 0.4525  | val_0_auc: 0.81834 |  0:08:46s\n",
      "epoch 167| loss: 0.45573 | val_0_auc: 0.81829 |  0:08:49s\n",
      "epoch 168| loss: 0.4569  | val_0_auc: 0.81837 |  0:08:53s\n",
      "epoch 169| loss: 0.45456 | val_0_auc: 0.81819 |  0:08:56s\n",
      "epoch 170| loss: 0.45249 | val_0_auc: 0.82157 |  0:08:59s\n",
      "epoch 171| loss: 0.45607 | val_0_auc: 0.82263 |  0:09:02s\n",
      "epoch 172| loss: 0.45264 | val_0_auc: 0.81922 |  0:09:05s\n",
      "epoch 173| loss: 0.45222 | val_0_auc: 0.8162  |  0:09:08s\n",
      "epoch 174| loss: 0.4478  | val_0_auc: 0.81712 |  0:09:13s\n",
      "epoch 175| loss: 0.45094 | val_0_auc: 0.82011 |  0:09:16s\n",
      "epoch 176| loss: 0.4527  | val_0_auc: 0.82032 |  0:09:19s\n",
      "epoch 177| loss: 0.45028 | val_0_auc: 0.82056 |  0:09:22s\n",
      "epoch 178| loss: 0.45041 | val_0_auc: 0.81768 |  0:09:25s\n",
      "epoch 179| loss: 0.4504  | val_0_auc: 0.82223 |  0:09:28s\n",
      "epoch 180| loss: 0.449   | val_0_auc: 0.81895 |  0:09:31s\n",
      "epoch 181| loss: 0.45092 | val_0_auc: 0.822   |  0:09:34s\n",
      "epoch 182| loss: 0.44823 | val_0_auc: 0.82432 |  0:09:37s\n",
      "epoch 183| loss: 0.44431 | val_0_auc: 0.81846 |  0:09:41s\n",
      "epoch 184| loss: 0.44558 | val_0_auc: 0.81353 |  0:09:43s\n",
      "epoch 185| loss: 0.44765 | val_0_auc: 0.81998 |  0:09:47s\n",
      "epoch 186| loss: 0.44577 | val_0_auc: 0.8207  |  0:09:50s\n",
      "epoch 187| loss: 0.44064 | val_0_auc: 0.82066 |  0:09:53s\n",
      "epoch 188| loss: 0.44509 | val_0_auc: 0.82344 |  0:09:56s\n",
      "epoch 189| loss: 0.4515  | val_0_auc: 0.82202 |  0:09:58s\n",
      "epoch 190| loss: 0.44569 | val_0_auc: 0.82044 |  0:10:01s\n",
      "epoch 191| loss: 0.44571 | val_0_auc: 0.82275 |  0:10:05s\n",
      "epoch 192| loss: 0.44489 | val_0_auc: 0.82377 |  0:10:08s\n",
      "epoch 193| loss: 0.4468  | val_0_auc: 0.82431 |  0:10:11s\n",
      "epoch 194| loss: 0.44202 | val_0_auc: 0.82396 |  0:10:13s\n",
      "epoch 195| loss: 0.44138 | val_0_auc: 0.82508 |  0:10:16s\n",
      "epoch 196| loss: 0.43951 | val_0_auc: 0.82327 |  0:10:19s\n",
      "epoch 197| loss: 0.44055 | val_0_auc: 0.82535 |  0:10:23s\n",
      "epoch 198| loss: 0.44106 | val_0_auc: 0.82641 |  0:10:26s\n",
      "epoch 199| loss: 0.44065 | val_0_auc: 0.82452 |  0:10:29s\n",
      "epoch 200| loss: 0.44052 | val_0_auc: 0.82641 |  0:10:32s\n",
      "epoch 201| loss: 0.44063 | val_0_auc: 0.82606 |  0:10:35s\n",
      "epoch 202| loss: 0.44352 | val_0_auc: 0.82619 |  0:10:38s\n",
      "epoch 203| loss: 0.43848 | val_0_auc: 0.82965 |  0:10:41s\n",
      "epoch 204| loss: 0.44143 | val_0_auc: 0.82755 |  0:10:44s\n",
      "epoch 205| loss: 0.44142 | val_0_auc: 0.82502 |  0:10:47s\n",
      "epoch 206| loss: 0.43994 | val_0_auc: 0.82678 |  0:10:50s\n",
      "epoch 207| loss: 0.44212 | val_0_auc: 0.82422 |  0:10:52s\n",
      "epoch 208| loss: 0.43964 | val_0_auc: 0.82883 |  0:10:56s\n",
      "epoch 209| loss: 0.44292 | val_0_auc: 0.82657 |  0:10:59s\n",
      "epoch 210| loss: 0.44096 | val_0_auc: 0.82993 |  0:11:02s\n",
      "epoch 211| loss: 0.44064 | val_0_auc: 0.83254 |  0:11:05s\n",
      "epoch 212| loss: 0.44282 | val_0_auc: 0.82958 |  0:11:08s\n",
      "epoch 213| loss: 0.43994 | val_0_auc: 0.82881 |  0:11:11s\n",
      "epoch 214| loss: 0.4427  | val_0_auc: 0.83111 |  0:11:14s\n",
      "epoch 215| loss: 0.43929 | val_0_auc: 0.83015 |  0:11:17s\n",
      "epoch 216| loss: 0.44164 | val_0_auc: 0.83013 |  0:11:20s\n",
      "epoch 217| loss: 0.43981 | val_0_auc: 0.83369 |  0:11:23s\n",
      "epoch 218| loss: 0.43769 | val_0_auc: 0.82841 |  0:11:26s\n",
      "epoch 219| loss: 0.43873 | val_0_auc: 0.82795 |  0:11:29s\n",
      "epoch 220| loss: 0.44372 | val_0_auc: 0.82244 |  0:11:32s\n",
      "epoch 221| loss: 0.44262 | val_0_auc: 0.82459 |  0:11:35s\n",
      "epoch 222| loss: 0.43962 | val_0_auc: 0.82788 |  0:11:37s\n",
      "epoch 223| loss: 0.43955 | val_0_auc: 0.82468 |  0:11:40s\n",
      "epoch 224| loss: 0.44087 | val_0_auc: 0.82991 |  0:11:43s\n",
      "epoch 225| loss: 0.44029 | val_0_auc: 0.82838 |  0:11:46s\n",
      "epoch 226| loss: 0.44065 | val_0_auc: 0.82839 |  0:11:49s\n",
      "epoch 227| loss: 0.44098 | val_0_auc: 0.82836 |  0:11:53s\n",
      "epoch 228| loss: 0.44496 | val_0_auc: 0.82912 |  0:11:56s\n",
      "epoch 229| loss: 0.43911 | val_0_auc: 0.82866 |  0:11:59s\n",
      "epoch 230| loss: 0.43839 | val_0_auc: 0.82896 |  0:12:02s\n",
      "epoch 231| loss: 0.44049 | val_0_auc: 0.82871 |  0:12:06s\n",
      "epoch 232| loss: 0.43701 | val_0_auc: 0.83248 |  0:12:09s\n",
      "epoch 233| loss: 0.43664 | val_0_auc: 0.8279  |  0:12:12s\n",
      "epoch 234| loss: 0.43987 | val_0_auc: 0.82947 |  0:12:15s\n",
      "epoch 235| loss: 0.43722 | val_0_auc: 0.83378 |  0:12:18s\n",
      "epoch 236| loss: 0.43437 | val_0_auc: 0.83536 |  0:12:21s\n",
      "epoch 237| loss: 0.43685 | val_0_auc: 0.83258 |  0:12:24s\n",
      "epoch 238| loss: 0.43584 | val_0_auc: 0.8355  |  0:12:27s\n",
      "epoch 239| loss: 0.43485 | val_0_auc: 0.83755 |  0:12:31s\n",
      "epoch 240| loss: 0.43251 | val_0_auc: 0.83592 |  0:12:33s\n",
      "epoch 241| loss: 0.43282 | val_0_auc: 0.83473 |  0:12:36s\n",
      "epoch 242| loss: 0.43384 | val_0_auc: 0.83674 |  0:12:40s\n",
      "epoch 243| loss: 0.42871 | val_0_auc: 0.83654 |  0:12:43s\n",
      "epoch 244| loss: 0.42986 | val_0_auc: 0.8333  |  0:12:46s\n",
      "epoch 245| loss: 0.42902 | val_0_auc: 0.83305 |  0:12:49s\n",
      "epoch 246| loss: 0.43188 | val_0_auc: 0.83054 |  0:12:52s\n",
      "epoch 247| loss: 0.43399 | val_0_auc: 0.82937 |  0:12:55s\n",
      "epoch 248| loss: 0.43341 | val_0_auc: 0.83613 |  0:12:59s\n",
      "epoch 249| loss: 0.43067 | val_0_auc: 0.8363  |  0:13:02s\n",
      "epoch 250| loss: 0.43155 | val_0_auc: 0.83646 |  0:13:05s\n",
      "epoch 251| loss: 0.43002 | val_0_auc: 0.83336 |  0:13:08s\n",
      "epoch 252| loss: 0.43453 | val_0_auc: 0.83392 |  0:13:11s\n",
      "epoch 253| loss: 0.43096 | val_0_auc: 0.839   |  0:13:14s\n",
      "epoch 254| loss: 0.42946 | val_0_auc: 0.83774 |  0:13:17s\n",
      "epoch 255| loss: 0.42972 | val_0_auc: 0.8379  |  0:13:20s\n",
      "epoch 256| loss: 0.43088 | val_0_auc: 0.83627 |  0:13:23s\n",
      "epoch 257| loss: 0.42753 | val_0_auc: 0.8376  |  0:13:26s\n",
      "epoch 258| loss: 0.4257  | val_0_auc: 0.83874 |  0:13:29s\n",
      "epoch 259| loss: 0.42577 | val_0_auc: 0.8391  |  0:13:32s\n",
      "epoch 260| loss: 0.42317 | val_0_auc: 0.83649 |  0:13:35s\n",
      "epoch 261| loss: 0.42443 | val_0_auc: 0.83528 |  0:13:38s\n",
      "epoch 262| loss: 0.42885 | val_0_auc: 0.83521 |  0:13:41s\n",
      "epoch 263| loss: 0.42326 | val_0_auc: 0.83661 |  0:13:44s\n",
      "epoch 264| loss: 0.42286 | val_0_auc: 0.83755 |  0:13:47s\n",
      "epoch 265| loss: 0.42673 | val_0_auc: 0.83683 |  0:13:50s\n",
      "epoch 266| loss: 0.42477 | val_0_auc: 0.83683 |  0:13:53s\n",
      "epoch 267| loss: 0.42544 | val_0_auc: 0.83726 |  0:13:56s\n",
      "epoch 268| loss: 0.42359 | val_0_auc: 0.83579 |  0:13:59s\n",
      "epoch 269| loss: 0.4288  | val_0_auc: 0.83637 |  0:14:01s\n",
      "epoch 270| loss: 0.42383 | val_0_auc: 0.83819 |  0:14:04s\n",
      "epoch 271| loss: 0.42402 | val_0_auc: 0.83909 |  0:14:07s\n",
      "epoch 272| loss: 0.42109 | val_0_auc: 0.83947 |  0:14:10s\n",
      "epoch 273| loss: 0.42206 | val_0_auc: 0.83956 |  0:14:13s\n",
      "epoch 274| loss: 0.42098 | val_0_auc: 0.83802 |  0:14:16s\n",
      "epoch 275| loss: 0.42311 | val_0_auc: 0.83939 |  0:14:19s\n",
      "epoch 276| loss: 0.42067 | val_0_auc: 0.83876 |  0:14:22s\n",
      "epoch 277| loss: 0.42062 | val_0_auc: 0.84004 |  0:14:24s\n",
      "epoch 278| loss: 0.42043 | val_0_auc: 0.84187 |  0:14:27s\n",
      "epoch 279| loss: 0.41947 | val_0_auc: 0.84377 |  0:14:30s\n",
      "epoch 280| loss: 0.415   | val_0_auc: 0.84526 |  0:14:33s\n",
      "epoch 281| loss: 0.41813 | val_0_auc: 0.84358 |  0:14:36s\n",
      "epoch 282| loss: 0.42019 | val_0_auc: 0.84305 |  0:14:39s\n",
      "epoch 283| loss: 0.41837 | val_0_auc: 0.84366 |  0:14:42s\n",
      "epoch 284| loss: 0.419   | val_0_auc: 0.84228 |  0:14:45s\n",
      "epoch 285| loss: 0.41569 | val_0_auc: 0.84092 |  0:14:48s\n",
      "epoch 286| loss: 0.41577 | val_0_auc: 0.84276 |  0:14:54s\n",
      "epoch 287| loss: 0.41632 | val_0_auc: 0.84128 |  0:14:57s\n",
      "epoch 288| loss: 0.41545 | val_0_auc: 0.84358 |  0:15:01s\n",
      "epoch 289| loss: 0.41609 | val_0_auc: 0.84271 |  0:15:07s\n",
      "epoch 290| loss: 0.41865 | val_0_auc: 0.84013 |  0:15:10s\n",
      "epoch 291| loss: 0.4137  | val_0_auc: 0.84313 |  0:15:13s\n",
      "epoch 292| loss: 0.41542 | val_0_auc: 0.84387 |  0:15:16s\n",
      "epoch 293| loss: 0.41386 | val_0_auc: 0.84699 |  0:15:19s\n",
      "epoch 294| loss: 0.41463 | val_0_auc: 0.84712 |  0:15:22s\n",
      "epoch 295| loss: 0.41202 | val_0_auc: 0.84576 |  0:15:25s\n",
      "epoch 296| loss: 0.41246 | val_0_auc: 0.8478  |  0:15:27s\n",
      "epoch 297| loss: 0.4106  | val_0_auc: 0.84772 |  0:15:31s\n",
      "epoch 298| loss: 0.40999 | val_0_auc: 0.84577 |  0:15:34s\n",
      "epoch 299| loss: 0.41077 | val_0_auc: 0.84583 |  0:15:37s\n",
      "epoch 300| loss: 0.41165 | val_0_auc: 0.84375 |  0:15:40s\n",
      "epoch 301| loss: 0.40686 | val_0_auc: 0.84352 |  0:15:43s\n",
      "epoch 302| loss: 0.41066 | val_0_auc: 0.84297 |  0:15:45s\n",
      "epoch 303| loss: 0.40926 | val_0_auc: 0.84337 |  0:15:48s\n",
      "epoch 304| loss: 0.4085  | val_0_auc: 0.84277 |  0:15:51s\n",
      "epoch 305| loss: 0.40847 | val_0_auc: 0.84221 |  0:15:54s\n",
      "epoch 306| loss: 0.40781 | val_0_auc: 0.84543 |  0:15:58s\n",
      "epoch 307| loss: 0.40866 | val_0_auc: 0.84609 |  0:16:03s\n",
      "epoch 308| loss: 0.40864 | val_0_auc: 0.84671 |  0:16:07s\n",
      "epoch 309| loss: 0.40779 | val_0_auc: 0.84523 |  0:16:10s\n",
      "epoch 310| loss: 0.40682 | val_0_auc: 0.8456  |  0:16:13s\n",
      "epoch 311| loss: 0.40987 | val_0_auc: 0.84553 |  0:16:16s\n",
      "epoch 312| loss: 0.40834 | val_0_auc: 0.84625 |  0:16:21s\n",
      "epoch 313| loss: 0.40705 | val_0_auc: 0.84783 |  0:16:27s\n",
      "epoch 314| loss: 0.40486 | val_0_auc: 0.84654 |  0:16:30s\n",
      "epoch 315| loss: 0.40523 | val_0_auc: 0.84749 |  0:16:33s\n",
      "epoch 316| loss: 0.40831 | val_0_auc: 0.84739 |  0:16:38s\n",
      "epoch 317| loss: 0.407   | val_0_auc: 0.84808 |  0:16:41s\n",
      "epoch 318| loss: 0.40591 | val_0_auc: 0.85004 |  0:16:44s\n",
      "epoch 319| loss: 0.40535 | val_0_auc: 0.85122 |  0:16:47s\n",
      "epoch 320| loss: 0.40717 | val_0_auc: 0.84929 |  0:16:50s\n",
      "epoch 321| loss: 0.40685 | val_0_auc: 0.84783 |  0:16:53s\n",
      "epoch 322| loss: 0.40936 | val_0_auc: 0.85081 |  0:16:56s\n",
      "epoch 323| loss: 0.40966 | val_0_auc: 0.852   |  0:17:00s\n",
      "epoch 324| loss: 0.4099  | val_0_auc: 0.85042 |  0:17:03s\n",
      "epoch 325| loss: 0.40807 | val_0_auc: 0.85012 |  0:17:06s\n",
      "epoch 326| loss: 0.40939 | val_0_auc: 0.85048 |  0:17:09s\n",
      "epoch 327| loss: 0.40976 | val_0_auc: 0.84681 |  0:17:12s\n",
      "epoch 328| loss: 0.40962 | val_0_auc: 0.84795 |  0:17:15s\n",
      "epoch 329| loss: 0.4082  | val_0_auc: 0.84176 |  0:17:17s\n",
      "epoch 330| loss: 0.40844 | val_0_auc: 0.84153 |  0:17:20s\n",
      "epoch 331| loss: 0.41017 | val_0_auc: 0.84248 |  0:17:23s\n",
      "epoch 332| loss: 0.40798 | val_0_auc: 0.84255 |  0:17:27s\n",
      "epoch 333| loss: 0.40812 | val_0_auc: 0.84165 |  0:17:30s\n",
      "epoch 334| loss: 0.40937 | val_0_auc: 0.84145 |  0:17:32s\n",
      "epoch 335| loss: 0.40745 | val_0_auc: 0.84886 |  0:17:35s\n",
      "epoch 336| loss: 0.40603 | val_0_auc: 0.85047 |  0:17:38s\n",
      "epoch 337| loss: 0.40373 | val_0_auc: 0.85142 |  0:17:42s\n",
      "epoch 338| loss: 0.40315 | val_0_auc: 0.85194 |  0:17:45s\n",
      "epoch 339| loss: 0.40438 | val_0_auc: 0.85143 |  0:17:47s\n",
      "epoch 340| loss: 0.40213 | val_0_auc: 0.85202 |  0:17:50s\n",
      "epoch 341| loss: 0.403   | val_0_auc: 0.85177 |  0:17:53s\n",
      "epoch 342| loss: 0.40089 | val_0_auc: 0.8522  |  0:17:56s\n",
      "epoch 343| loss: 0.40347 | val_0_auc: 0.85281 |  0:17:59s\n",
      "epoch 344| loss: 0.40239 | val_0_auc: 0.85339 |  0:18:02s\n",
      "epoch 345| loss: 0.4006  | val_0_auc: 0.85459 |  0:18:05s\n",
      "epoch 346| loss: 0.39595 | val_0_auc: 0.85456 |  0:18:08s\n",
      "epoch 347| loss: 0.40199 | val_0_auc: 0.85545 |  0:18:11s\n",
      "epoch 348| loss: 0.40168 | val_0_auc: 0.8567  |  0:18:14s\n",
      "epoch 349| loss: 0.39882 | val_0_auc: 0.8549  |  0:18:17s\n",
      "epoch 350| loss: 0.39974 | val_0_auc: 0.85509 |  0:18:20s\n",
      "epoch 351| loss: 0.40077 | val_0_auc: 0.85492 |  0:18:23s\n",
      "epoch 352| loss: 0.39879 | val_0_auc: 0.85546 |  0:18:26s\n",
      "epoch 353| loss: 0.40096 | val_0_auc: 0.8571  |  0:18:29s\n",
      "epoch 354| loss: 0.39839 | val_0_auc: 0.85581 |  0:18:32s\n",
      "epoch 355| loss: 0.39811 | val_0_auc: 0.85597 |  0:18:38s\n",
      "epoch 356| loss: 0.39821 | val_0_auc: 0.85733 |  0:18:42s\n",
      "epoch 357| loss: 0.39925 | val_0_auc: 0.85563 |  0:18:46s\n",
      "epoch 358| loss: 0.40043 | val_0_auc: 0.85738 |  0:18:50s\n",
      "epoch 359| loss: 0.39555 | val_0_auc: 0.85568 |  0:18:54s\n",
      "epoch 360| loss: 0.39873 | val_0_auc: 0.85581 |  0:18:57s\n",
      "epoch 361| loss: 0.39883 | val_0_auc: 0.8571  |  0:19:00s\n",
      "epoch 362| loss: 0.40048 | val_0_auc: 0.85632 |  0:19:04s\n",
      "epoch 363| loss: 0.39779 | val_0_auc: 0.85635 |  0:19:07s\n",
      "epoch 364| loss: 0.40115 | val_0_auc: 0.85437 |  0:19:11s\n",
      "epoch 365| loss: 0.39908 | val_0_auc: 0.85728 |  0:19:16s\n",
      "epoch 366| loss: 0.39714 | val_0_auc: 0.85637 |  0:19:21s\n",
      "epoch 367| loss: 0.39938 | val_0_auc: 0.85503 |  0:19:24s\n",
      "epoch 368| loss: 0.39569 | val_0_auc: 0.85836 |  0:19:27s\n",
      "epoch 369| loss: 0.39348 | val_0_auc: 0.85719 |  0:19:30s\n",
      "epoch 370| loss: 0.39455 | val_0_auc: 0.85876 |  0:19:33s\n",
      "epoch 371| loss: 0.39649 | val_0_auc: 0.8585  |  0:19:36s\n",
      "epoch 372| loss: 0.39484 | val_0_auc: 0.858   |  0:19:39s\n",
      "epoch 373| loss: 0.39554 | val_0_auc: 0.85589 |  0:19:42s\n",
      "epoch 374| loss: 0.39396 | val_0_auc: 0.85572 |  0:19:45s\n",
      "epoch 375| loss: 0.39479 | val_0_auc: 0.85177 |  0:19:48s\n",
      "epoch 376| loss: 0.39871 | val_0_auc: 0.85367 |  0:19:51s\n",
      "epoch 377| loss: 0.39846 | val_0_auc: 0.85414 |  0:19:54s\n",
      "epoch 378| loss: 0.40073 | val_0_auc: 0.85364 |  0:19:57s\n",
      "epoch 379| loss: 0.40056 | val_0_auc: 0.85495 |  0:20:00s\n",
      "epoch 380| loss: 0.40334 | val_0_auc: 0.85551 |  0:20:04s\n",
      "epoch 381| loss: 0.40485 | val_0_auc: 0.85294 |  0:20:08s\n",
      "epoch 382| loss: 0.40104 | val_0_auc: 0.85328 |  0:20:12s\n",
      "epoch 383| loss: 0.40044 | val_0_auc: 0.8565  |  0:20:16s\n",
      "epoch 384| loss: 0.39698 | val_0_auc: 0.85852 |  0:20:19s\n",
      "epoch 385| loss: 0.40026 | val_0_auc: 0.85787 |  0:20:22s\n",
      "epoch 386| loss: 0.40121 | val_0_auc: 0.85771 |  0:20:25s\n",
      "epoch 387| loss: 0.39787 | val_0_auc: 0.85427 |  0:20:29s\n",
      "epoch 388| loss: 0.39739 | val_0_auc: 0.85326 |  0:20:34s\n",
      "epoch 389| loss: 0.39823 | val_0_auc: 0.85456 |  0:20:41s\n",
      "epoch 390| loss: 0.39594 | val_0_auc: 0.85418 |  0:20:45s\n",
      "\n",
      "Early stopping occurred at epoch 390 with best_epoch = 370 and best_val_0_auc = 0.85876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78623 | val_0_auc: 0.64052 |  0:00:04s\n",
      "epoch 1  | loss: 0.46719 | val_0_auc: 0.79667 |  0:00:09s\n",
      "epoch 2  | loss: 0.40223 | val_0_auc: 0.83792 |  0:00:14s\n",
      "epoch 3  | loss: 0.37273 | val_0_auc: 0.8724  |  0:00:18s\n",
      "epoch 4  | loss: 0.35869 | val_0_auc: 0.8819  |  0:00:23s\n",
      "epoch 5  | loss: 0.34573 | val_0_auc: 0.88926 |  0:00:28s\n",
      "epoch 6  | loss: 0.3313  | val_0_auc: 0.89233 |  0:00:36s\n",
      "epoch 7  | loss: 0.32417 | val_0_auc: 0.8969  |  0:00:40s\n",
      "epoch 8  | loss: 0.31603 | val_0_auc: 0.90887 |  0:00:44s\n",
      "epoch 9  | loss: 0.30664 | val_0_auc: 0.90997 |  0:00:48s\n",
      "epoch 10 | loss: 0.30485 | val_0_auc: 0.91324 |  0:00:51s\n",
      "epoch 11 | loss: 0.29472 | val_0_auc: 0.91393 |  0:00:55s\n",
      "epoch 12 | loss: 0.29234 | val_0_auc: 0.91677 |  0:00:59s\n",
      "epoch 13 | loss: 0.29137 | val_0_auc: 0.91353 |  0:01:02s\n",
      "epoch 14 | loss: 0.28824 | val_0_auc: 0.91596 |  0:01:06s\n",
      "epoch 15 | loss: 0.28341 | val_0_auc: 0.9158  |  0:01:10s\n",
      "epoch 16 | loss: 0.28089 | val_0_auc: 0.9159  |  0:01:14s\n",
      "epoch 17 | loss: 0.27607 | val_0_auc: 0.91426 |  0:01:17s\n",
      "epoch 18 | loss: 0.27578 | val_0_auc: 0.91725 |  0:01:21s\n",
      "epoch 19 | loss: 0.2732  | val_0_auc: 0.91624 |  0:01:25s\n",
      "epoch 20 | loss: 0.27109 | val_0_auc: 0.91755 |  0:01:29s\n",
      "epoch 21 | loss: 0.26767 | val_0_auc: 0.91557 |  0:01:35s\n",
      "epoch 22 | loss: 0.26374 | val_0_auc: 0.91355 |  0:01:39s\n",
      "epoch 23 | loss: 0.26021 | val_0_auc: 0.91644 |  0:01:44s\n",
      "epoch 24 | loss: 0.25915 | val_0_auc: 0.91322 |  0:01:49s\n",
      "epoch 25 | loss: 0.25683 | val_0_auc: 0.91192 |  0:01:53s\n",
      "epoch 26 | loss: 0.25793 | val_0_auc: 0.91239 |  0:01:57s\n",
      "epoch 27 | loss: 0.25548 | val_0_auc: 0.91345 |  0:02:01s\n",
      "epoch 28 | loss: 0.25317 | val_0_auc: 0.90912 |  0:02:05s\n",
      "epoch 29 | loss: 0.24894 | val_0_auc: 0.908   |  0:02:08s\n",
      "epoch 30 | loss: 0.24788 | val_0_auc: 0.90526 |  0:02:14s\n",
      "epoch 31 | loss: 0.24871 | val_0_auc: 0.90785 |  0:02:19s\n",
      "epoch 32 | loss: 0.24695 | val_0_auc: 0.90893 |  0:02:24s\n",
      "epoch 33 | loss: 0.24696 | val_0_auc: 0.90716 |  0:02:30s\n",
      "epoch 34 | loss: 0.24397 | val_0_auc: 0.90593 |  0:02:36s\n",
      "epoch 35 | loss: 0.23869 | val_0_auc: 0.90393 |  0:02:41s\n",
      "epoch 36 | loss: 0.2364  | val_0_auc: 0.90248 |  0:02:45s\n",
      "epoch 37 | loss: 0.23809 | val_0_auc: 0.90231 |  0:02:50s\n",
      "epoch 38 | loss: 0.23611 | val_0_auc: 0.90361 |  0:02:54s\n",
      "epoch 39 | loss: 0.23285 | val_0_auc: 0.90081 |  0:02:59s\n",
      "epoch 40 | loss: 0.23319 | val_0_auc: 0.89627 |  0:03:03s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_0_auc = 0.91755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.82706 | val_0_auc: 0.53269 |  0:00:04s\n",
      "epoch 1  | loss: 0.62012 | val_0_auc: 0.59801 |  0:00:10s\n",
      "epoch 2  | loss: 0.53812 | val_0_auc: 0.67582 |  0:00:21s\n",
      "epoch 3  | loss: 0.49746 | val_0_auc: 0.68336 |  0:00:25s\n",
      "epoch 4  | loss: 0.48161 | val_0_auc: 0.74507 |  0:00:32s\n",
      "epoch 5  | loss: 0.46049 | val_0_auc: 0.75622 |  0:00:37s\n",
      "epoch 6  | loss: 0.45171 | val_0_auc: 0.7657  |  0:00:41s\n",
      "epoch 7  | loss: 0.44633 | val_0_auc: 0.77266 |  0:00:45s\n",
      "epoch 8  | loss: 0.44522 | val_0_auc: 0.78777 |  0:00:49s\n",
      "epoch 9  | loss: 0.44259 | val_0_auc: 0.80142 |  0:00:53s\n",
      "epoch 10 | loss: 0.42986 | val_0_auc: 0.79709 |  0:00:58s\n",
      "epoch 11 | loss: 0.42629 | val_0_auc: 0.80688 |  0:01:02s\n",
      "epoch 12 | loss: 0.4261  | val_0_auc: 0.81487 |  0:01:06s\n",
      "epoch 13 | loss: 0.42818 | val_0_auc: 0.81354 |  0:01:11s\n",
      "epoch 14 | loss: 0.41997 | val_0_auc: 0.83027 |  0:01:15s\n",
      "epoch 15 | loss: 0.40882 | val_0_auc: 0.8436  |  0:01:19s\n",
      "epoch 16 | loss: 0.39672 | val_0_auc: 0.85084 |  0:01:23s\n",
      "epoch 17 | loss: 0.38588 | val_0_auc: 0.85211 |  0:01:27s\n",
      "epoch 18 | loss: 0.38021 | val_0_auc: 0.85504 |  0:01:31s\n",
      "epoch 19 | loss: 0.37755 | val_0_auc: 0.85584 |  0:01:36s\n",
      "epoch 20 | loss: 0.37619 | val_0_auc: 0.85648 |  0:01:40s\n",
      "epoch 21 | loss: 0.37198 | val_0_auc: 0.85999 |  0:01:44s\n",
      "epoch 22 | loss: 0.36944 | val_0_auc: 0.86251 |  0:01:48s\n",
      "epoch 23 | loss: 0.3647  | val_0_auc: 0.86597 |  0:01:52s\n",
      "epoch 24 | loss: 0.36251 | val_0_auc: 0.87085 |  0:01:56s\n",
      "epoch 25 | loss: 0.36106 | val_0_auc: 0.87059 |  0:02:01s\n",
      "epoch 26 | loss: 0.35743 | val_0_auc: 0.87309 |  0:02:05s\n",
      "epoch 27 | loss: 0.3545  | val_0_auc: 0.87415 |  0:02:10s\n",
      "epoch 28 | loss: 0.35209 | val_0_auc: 0.87494 |  0:02:14s\n",
      "epoch 29 | loss: 0.35353 | val_0_auc: 0.87354 |  0:02:19s\n",
      "epoch 30 | loss: 0.35209 | val_0_auc: 0.87492 |  0:02:24s\n",
      "epoch 31 | loss: 0.34969 | val_0_auc: 0.87738 |  0:02:28s\n",
      "epoch 32 | loss: 0.35047 | val_0_auc: 0.87978 |  0:02:33s\n",
      "epoch 33 | loss: 0.34656 | val_0_auc: 0.87827 |  0:02:38s\n",
      "epoch 34 | loss: 0.34253 | val_0_auc: 0.87872 |  0:02:42s\n",
      "epoch 35 | loss: 0.3414  | val_0_auc: 0.88375 |  0:02:46s\n",
      "epoch 36 | loss: 0.33804 | val_0_auc: 0.88125 |  0:02:50s\n",
      "epoch 37 | loss: 0.33971 | val_0_auc: 0.88061 |  0:02:54s\n",
      "epoch 38 | loss: 0.3375  | val_0_auc: 0.8828  |  0:02:58s\n",
      "epoch 39 | loss: 0.33419 | val_0_auc: 0.88509 |  0:03:03s\n",
      "epoch 40 | loss: 0.33359 | val_0_auc: 0.88373 |  0:03:07s\n",
      "epoch 41 | loss: 0.33138 | val_0_auc: 0.88672 |  0:03:11s\n",
      "epoch 42 | loss: 0.3283  | val_0_auc: 0.88672 |  0:03:15s\n",
      "epoch 43 | loss: 0.32571 | val_0_auc: 0.88765 |  0:03:19s\n",
      "epoch 44 | loss: 0.32747 | val_0_auc: 0.88642 |  0:03:23s\n",
      "epoch 45 | loss: 0.32501 | val_0_auc: 0.88675 |  0:03:27s\n",
      "epoch 46 | loss: 0.3226  | val_0_auc: 0.88689 |  0:03:31s\n",
      "epoch 47 | loss: 0.32306 | val_0_auc: 0.88793 |  0:03:35s\n",
      "epoch 48 | loss: 0.32065 | val_0_auc: 0.88893 |  0:03:39s\n",
      "epoch 49 | loss: 0.3209  | val_0_auc: 0.88795 |  0:03:45s\n",
      "epoch 50 | loss: 0.31677 | val_0_auc: 0.8886  |  0:03:49s\n",
      "epoch 51 | loss: 0.31725 | val_0_auc: 0.88734 |  0:03:53s\n",
      "epoch 52 | loss: 0.31451 | val_0_auc: 0.88778 |  0:03:58s\n",
      "epoch 53 | loss: 0.31838 | val_0_auc: 0.88678 |  0:04:05s\n",
      "epoch 54 | loss: 0.32653 | val_0_auc: 0.89046 |  0:04:13s\n",
      "epoch 55 | loss: 0.32544 | val_0_auc: 0.88924 |  0:04:18s\n",
      "epoch 56 | loss: 0.32259 | val_0_auc: 0.89045 |  0:04:23s\n",
      "epoch 57 | loss: 0.31991 | val_0_auc: 0.89389 |  0:04:29s\n",
      "epoch 58 | loss: 0.31716 | val_0_auc: 0.89432 |  0:04:35s\n",
      "epoch 59 | loss: 0.31996 | val_0_auc: 0.89094 |  0:04:41s\n",
      "epoch 60 | loss: 0.31668 | val_0_auc: 0.8945  |  0:04:47s\n",
      "epoch 61 | loss: 0.31475 | val_0_auc: 0.89538 |  0:04:53s\n",
      "epoch 62 | loss: 0.31301 | val_0_auc: 0.89472 |  0:04:59s\n",
      "epoch 63 | loss: 0.31049 | val_0_auc: 0.89467 |  0:05:05s\n",
      "epoch 64 | loss: 0.31036 | val_0_auc: 0.89497 |  0:05:10s\n",
      "epoch 65 | loss: 0.30847 | val_0_auc: 0.89715 |  0:05:15s\n",
      "epoch 66 | loss: 0.30664 | val_0_auc: 0.89712 |  0:05:20s\n",
      "epoch 67 | loss: 0.30774 | val_0_auc: 0.89434 |  0:05:25s\n",
      "epoch 68 | loss: 0.30653 | val_0_auc: 0.89749 |  0:05:29s\n",
      "epoch 69 | loss: 0.30604 | val_0_auc: 0.89849 |  0:05:34s\n",
      "epoch 70 | loss: 0.30947 | val_0_auc: 0.8984  |  0:05:39s\n",
      "epoch 71 | loss: 0.30576 | val_0_auc: 0.89685 |  0:05:44s\n",
      "epoch 72 | loss: 0.30248 | val_0_auc: 0.89704 |  0:05:49s\n",
      "epoch 73 | loss: 0.3047  | val_0_auc: 0.89554 |  0:05:54s\n",
      "epoch 74 | loss: 0.31113 | val_0_auc: 0.89318 |  0:05:59s\n",
      "epoch 75 | loss: 0.31532 | val_0_auc: 0.89439 |  0:06:03s\n",
      "epoch 76 | loss: 0.31318 | val_0_auc: 0.89655 |  0:06:08s\n",
      "epoch 77 | loss: 0.31215 | val_0_auc: 0.89553 |  0:06:13s\n",
      "epoch 78 | loss: 0.30977 | val_0_auc: 0.89549 |  0:06:18s\n",
      "epoch 79 | loss: 0.30542 | val_0_auc: 0.89786 |  0:06:23s\n",
      "epoch 80 | loss: 0.30618 | val_0_auc: 0.89665 |  0:06:28s\n",
      "epoch 81 | loss: 0.30727 | val_0_auc: 0.89558 |  0:06:33s\n",
      "epoch 82 | loss: 0.30447 | val_0_auc: 0.89603 |  0:06:38s\n",
      "epoch 83 | loss: 0.30463 | val_0_auc: 0.89932 |  0:06:43s\n",
      "epoch 84 | loss: 0.30266 | val_0_auc: 0.89962 |  0:06:48s\n",
      "epoch 85 | loss: 0.30026 | val_0_auc: 0.89715 |  0:06:53s\n",
      "epoch 86 | loss: 0.30031 | val_0_auc: 0.89867 |  0:06:58s\n",
      "epoch 87 | loss: 0.30347 | val_0_auc: 0.9     |  0:07:03s\n",
      "epoch 88 | loss: 0.29973 | val_0_auc: 0.89897 |  0:07:08s\n",
      "epoch 89 | loss: 0.30077 | val_0_auc: 0.90066 |  0:07:13s\n",
      "epoch 90 | loss: 0.30118 | val_0_auc: 0.90122 |  0:07:18s\n",
      "epoch 91 | loss: 0.29751 | val_0_auc: 0.90052 |  0:07:23s\n",
      "epoch 92 | loss: 0.2977  | val_0_auc: 0.90107 |  0:07:29s\n",
      "epoch 93 | loss: 0.29602 | val_0_auc: 0.90322 |  0:07:34s\n",
      "epoch 94 | loss: 0.29551 | val_0_auc: 0.90342 |  0:07:39s\n",
      "epoch 95 | loss: 0.29308 | val_0_auc: 0.90272 |  0:07:44s\n",
      "epoch 96 | loss: 0.29262 | val_0_auc: 0.90307 |  0:07:49s\n",
      "epoch 97 | loss: 0.29316 | val_0_auc: 0.90106 |  0:07:53s\n",
      "epoch 98 | loss: 0.29051 | val_0_auc: 0.90175 |  0:07:58s\n",
      "epoch 99 | loss: 0.2889  | val_0_auc: 0.90171 |  0:08:03s\n",
      "epoch 100| loss: 0.28799 | val_0_auc: 0.90268 |  0:08:08s\n",
      "epoch 101| loss: 0.29127 | val_0_auc: 0.90198 |  0:08:12s\n",
      "epoch 102| loss: 0.28818 | val_0_auc: 0.90247 |  0:08:17s\n",
      "epoch 103| loss: 0.28841 | val_0_auc: 0.90164 |  0:08:22s\n",
      "epoch 104| loss: 0.2867  | val_0_auc: 0.9023  |  0:08:27s\n",
      "epoch 105| loss: 0.28683 | val_0_auc: 0.90337 |  0:08:32s\n",
      "epoch 106| loss: 0.28678 | val_0_auc: 0.90316 |  0:08:38s\n",
      "epoch 107| loss: 0.28679 | val_0_auc: 0.90259 |  0:08:43s\n",
      "epoch 108| loss: 0.28532 | val_0_auc: 0.90301 |  0:08:48s\n",
      "epoch 109| loss: 0.28613 | val_0_auc: 0.90154 |  0:08:52s\n",
      "epoch 110| loss: 0.28433 | val_0_auc: 0.90238 |  0:08:57s\n",
      "epoch 111| loss: 0.28452 | val_0_auc: 0.90165 |  0:09:02s\n",
      "epoch 112| loss: 0.28281 | val_0_auc: 0.90336 |  0:09:09s\n",
      "epoch 113| loss: 0.28412 | val_0_auc: 0.90214 |  0:09:14s\n",
      "epoch 114| loss: 0.28219 | val_0_auc: 0.90201 |  0:09:19s\n",
      "\n",
      "Early stopping occurred at epoch 114 with best_epoch = 94 and best_val_0_auc = 0.90342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69505 | val_0_auc: 0.61492 |  0:00:01s\n",
      "epoch 1  | loss: 0.50971 | val_0_auc: 0.75779 |  0:00:02s\n",
      "epoch 2  | loss: 0.43203 | val_0_auc: 0.81716 |  0:00:04s\n",
      "epoch 3  | loss: 0.38333 | val_0_auc: 0.83709 |  0:00:05s\n",
      "epoch 4  | loss: 0.36395 | val_0_auc: 0.85369 |  0:00:07s\n",
      "epoch 5  | loss: 0.34601 | val_0_auc: 0.86183 |  0:00:08s\n",
      "epoch 6  | loss: 0.33368 | val_0_auc: 0.87228 |  0:00:10s\n",
      "epoch 7  | loss: 0.32396 | val_0_auc: 0.87917 |  0:00:11s\n",
      "epoch 8  | loss: 0.31206 | val_0_auc: 0.88193 |  0:00:13s\n",
      "epoch 9  | loss: 0.30204 | val_0_auc: 0.89049 |  0:00:14s\n",
      "epoch 10 | loss: 0.29743 | val_0_auc: 0.8952  |  0:00:16s\n",
      "epoch 11 | loss: 0.2886  | val_0_auc: 0.89238 |  0:00:17s\n",
      "epoch 12 | loss: 0.28893 | val_0_auc: 0.89832 |  0:00:19s\n",
      "epoch 13 | loss: 0.28531 | val_0_auc: 0.89995 |  0:00:20s\n",
      "epoch 14 | loss: 0.27983 | val_0_auc: 0.90329 |  0:00:21s\n",
      "epoch 15 | loss: 0.27657 | val_0_auc: 0.90439 |  0:00:23s\n",
      "epoch 16 | loss: 0.27552 | val_0_auc: 0.90597 |  0:00:24s\n",
      "epoch 17 | loss: 0.27667 | val_0_auc: 0.90655 |  0:00:26s\n",
      "epoch 18 | loss: 0.26693 | val_0_auc: 0.90372 |  0:00:27s\n",
      "epoch 19 | loss: 0.26834 | val_0_auc: 0.90408 |  0:00:29s\n",
      "epoch 20 | loss: 0.26329 | val_0_auc: 0.90539 |  0:00:30s\n",
      "epoch 21 | loss: 0.26463 | val_0_auc: 0.90128 |  0:00:32s\n",
      "epoch 22 | loss: 0.26239 | val_0_auc: 0.9034  |  0:00:33s\n",
      "epoch 23 | loss: 0.25892 | val_0_auc: 0.9064  |  0:00:34s\n",
      "epoch 24 | loss: 0.25572 | val_0_auc: 0.90428 |  0:00:36s\n",
      "epoch 25 | loss: 0.25336 | val_0_auc: 0.90152 |  0:00:37s\n",
      "epoch 26 | loss: 0.25163 | val_0_auc: 0.90308 |  0:00:39s\n",
      "epoch 27 | loss: 0.25138 | val_0_auc: 0.90266 |  0:00:40s\n",
      "epoch 28 | loss: 0.24934 | val_0_auc: 0.90462 |  0:00:42s\n",
      "epoch 29 | loss: 0.2497  | val_0_auc: 0.9022  |  0:00:43s\n",
      "epoch 30 | loss: 0.24548 | val_0_auc: 0.90135 |  0:00:44s\n",
      "epoch 31 | loss: 0.24604 | val_0_auc: 0.90098 |  0:00:46s\n",
      "epoch 32 | loss: 0.24598 | val_0_auc: 0.90249 |  0:00:47s\n",
      "epoch 33 | loss: 0.24373 | val_0_auc: 0.90033 |  0:00:49s\n",
      "epoch 34 | loss: 0.24356 | val_0_auc: 0.90058 |  0:00:50s\n",
      "epoch 35 | loss: 0.24046 | val_0_auc: 0.89981 |  0:00:51s\n",
      "epoch 36 | loss: 0.24036 | val_0_auc: 0.90254 |  0:00:53s\n",
      "epoch 37 | loss: 0.23846 | val_0_auc: 0.90217 |  0:00:55s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_auc = 0.90655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.02683 | val_0_auc: 0.48944 |  0:00:02s\n",
      "epoch 1  | loss: 1.61592 | val_0_auc: 0.49337 |  0:00:04s\n",
      "epoch 2  | loss: 1.33449 | val_0_auc: 0.51431 |  0:00:07s\n",
      "epoch 3  | loss: 1.09403 | val_0_auc: 0.5151  |  0:00:09s\n",
      "epoch 4  | loss: 0.91553 | val_0_auc: 0.51327 |  0:00:12s\n",
      "epoch 5  | loss: 0.8054  | val_0_auc: 0.5143  |  0:00:14s\n",
      "epoch 6  | loss: 0.71971 | val_0_auc: 0.51418 |  0:00:17s\n",
      "epoch 7  | loss: 0.67084 | val_0_auc: 0.52143 |  0:00:19s\n",
      "epoch 8  | loss: 0.64069 | val_0_auc: 0.52844 |  0:00:22s\n",
      "epoch 9  | loss: 0.61135 | val_0_auc: 0.525   |  0:00:24s\n",
      "epoch 10 | loss: 0.59795 | val_0_auc: 0.5499  |  0:00:26s\n",
      "epoch 11 | loss: 0.58893 | val_0_auc: 0.55463 |  0:00:29s\n",
      "epoch 12 | loss: 0.58592 | val_0_auc: 0.5742  |  0:00:32s\n",
      "epoch 13 | loss: 0.57416 | val_0_auc: 0.58651 |  0:00:34s\n",
      "epoch 14 | loss: 0.57007 | val_0_auc: 0.60115 |  0:00:37s\n",
      "epoch 15 | loss: 0.56723 | val_0_auc: 0.60585 |  0:00:39s\n",
      "epoch 16 | loss: 0.55837 | val_0_auc: 0.60223 |  0:00:42s\n",
      "epoch 17 | loss: 0.54901 | val_0_auc: 0.64834 |  0:00:44s\n",
      "epoch 18 | loss: 0.54372 | val_0_auc: 0.66258 |  0:00:46s\n",
      "epoch 19 | loss: 0.5384  | val_0_auc: 0.66745 |  0:00:49s\n",
      "epoch 20 | loss: 0.52688 | val_0_auc: 0.68847 |  0:00:52s\n",
      "epoch 21 | loss: 0.5203  | val_0_auc: 0.70893 |  0:00:54s\n",
      "epoch 22 | loss: 0.51532 | val_0_auc: 0.70609 |  0:00:57s\n",
      "epoch 23 | loss: 0.51134 | val_0_auc: 0.70596 |  0:01:00s\n",
      "epoch 24 | loss: 0.50127 | val_0_auc: 0.72637 |  0:01:03s\n",
      "epoch 25 | loss: 0.49533 | val_0_auc: 0.72872 |  0:01:06s\n",
      "epoch 26 | loss: 0.49442 | val_0_auc: 0.72777 |  0:01:09s\n",
      "epoch 27 | loss: 0.48991 | val_0_auc: 0.73847 |  0:01:12s\n",
      "epoch 28 | loss: 0.48496 | val_0_auc: 0.7497  |  0:01:15s\n",
      "epoch 29 | loss: 0.48376 | val_0_auc: 0.76026 |  0:01:18s\n",
      "epoch 30 | loss: 0.47825 | val_0_auc: 0.76216 |  0:01:21s\n",
      "epoch 31 | loss: 0.47627 | val_0_auc: 0.7608  |  0:01:24s\n",
      "epoch 32 | loss: 0.47061 | val_0_auc: 0.77126 |  0:01:27s\n",
      "epoch 33 | loss: 0.46714 | val_0_auc: 0.77126 |  0:01:29s\n",
      "epoch 34 | loss: 0.46202 | val_0_auc: 0.77356 |  0:01:32s\n",
      "epoch 35 | loss: 0.45984 | val_0_auc: 0.77094 |  0:01:34s\n",
      "epoch 36 | loss: 0.46117 | val_0_auc: 0.77531 |  0:01:37s\n",
      "epoch 37 | loss: 0.45707 | val_0_auc: 0.77118 |  0:01:39s\n",
      "epoch 38 | loss: 0.45478 | val_0_auc: 0.78025 |  0:01:41s\n",
      "epoch 39 | loss: 0.45528 | val_0_auc: 0.77684 |  0:01:44s\n",
      "epoch 40 | loss: 0.45028 | val_0_auc: 0.78135 |  0:01:46s\n",
      "epoch 41 | loss: 0.45207 | val_0_auc: 0.78193 |  0:01:48s\n",
      "epoch 42 | loss: 0.4513  | val_0_auc: 0.77811 |  0:01:51s\n",
      "epoch 43 | loss: 0.45166 | val_0_auc: 0.78092 |  0:01:54s\n",
      "epoch 44 | loss: 0.44907 | val_0_auc: 0.78018 |  0:01:56s\n",
      "epoch 45 | loss: 0.44832 | val_0_auc: 0.78547 |  0:01:59s\n",
      "epoch 46 | loss: 0.44635 | val_0_auc: 0.78473 |  0:02:03s\n",
      "epoch 47 | loss: 0.4465  | val_0_auc: 0.78764 |  0:02:08s\n",
      "epoch 48 | loss: 0.4437  | val_0_auc: 0.79128 |  0:02:10s\n",
      "epoch 49 | loss: 0.4441  | val_0_auc: 0.78864 |  0:02:13s\n",
      "epoch 50 | loss: 0.44266 | val_0_auc: 0.79389 |  0:02:16s\n",
      "epoch 51 | loss: 0.44259 | val_0_auc: 0.79174 |  0:02:18s\n",
      "epoch 52 | loss: 0.44227 | val_0_auc: 0.79585 |  0:02:21s\n",
      "epoch 53 | loss: 0.43788 | val_0_auc: 0.79926 |  0:02:24s\n",
      "epoch 54 | loss: 0.43812 | val_0_auc: 0.79442 |  0:02:26s\n",
      "epoch 55 | loss: 0.44016 | val_0_auc: 0.79896 |  0:02:29s\n",
      "epoch 56 | loss: 0.43818 | val_0_auc: 0.79714 |  0:02:31s\n",
      "epoch 57 | loss: 0.43551 | val_0_auc: 0.79905 |  0:02:33s\n",
      "epoch 58 | loss: 0.43071 | val_0_auc: 0.80029 |  0:02:36s\n",
      "epoch 59 | loss: 0.43548 | val_0_auc: 0.8001  |  0:02:38s\n",
      "epoch 60 | loss: 0.43242 | val_0_auc: 0.79754 |  0:02:41s\n",
      "epoch 61 | loss: 0.43241 | val_0_auc: 0.79928 |  0:02:43s\n",
      "epoch 62 | loss: 0.43051 | val_0_auc: 0.80059 |  0:02:46s\n",
      "epoch 63 | loss: 0.42943 | val_0_auc: 0.79751 |  0:02:48s\n",
      "epoch 64 | loss: 0.42907 | val_0_auc: 0.80515 |  0:02:51s\n",
      "epoch 65 | loss: 0.42708 | val_0_auc: 0.80895 |  0:02:53s\n",
      "epoch 66 | loss: 0.42757 | val_0_auc: 0.80636 |  0:02:56s\n",
      "epoch 67 | loss: 0.42955 | val_0_auc: 0.80595 |  0:02:58s\n",
      "epoch 68 | loss: 0.42918 | val_0_auc: 0.80665 |  0:03:01s\n",
      "epoch 69 | loss: 0.42836 | val_0_auc: 0.80494 |  0:03:03s\n",
      "epoch 70 | loss: 0.4274  | val_0_auc: 0.80837 |  0:03:06s\n",
      "epoch 71 | loss: 0.42482 | val_0_auc: 0.81051 |  0:03:08s\n",
      "epoch 72 | loss: 0.42262 | val_0_auc: 0.80859 |  0:03:11s\n",
      "epoch 73 | loss: 0.42402 | val_0_auc: 0.80485 |  0:03:13s\n",
      "epoch 74 | loss: 0.42263 | val_0_auc: 0.80473 |  0:03:16s\n",
      "epoch 75 | loss: 0.42609 | val_0_auc: 0.81013 |  0:03:20s\n",
      "epoch 76 | loss: 0.42056 | val_0_auc: 0.80752 |  0:03:22s\n",
      "epoch 77 | loss: 0.42077 | val_0_auc: 0.80574 |  0:03:25s\n",
      "epoch 78 | loss: 0.42118 | val_0_auc: 0.80689 |  0:03:27s\n",
      "epoch 79 | loss: 0.42082 | val_0_auc: 0.80616 |  0:03:29s\n",
      "epoch 80 | loss: 0.4189  | val_0_auc: 0.80751 |  0:03:32s\n",
      "epoch 81 | loss: 0.41906 | val_0_auc: 0.80626 |  0:03:34s\n",
      "epoch 82 | loss: 0.41462 | val_0_auc: 0.81003 |  0:03:36s\n",
      "epoch 83 | loss: 0.41867 | val_0_auc: 0.81017 |  0:03:39s\n",
      "epoch 84 | loss: 0.41587 | val_0_auc: 0.80901 |  0:03:41s\n",
      "epoch 85 | loss: 0.41768 | val_0_auc: 0.81026 |  0:03:44s\n",
      "epoch 86 | loss: 0.41612 | val_0_auc: 0.80854 |  0:03:46s\n",
      "epoch 87 | loss: 0.41569 | val_0_auc: 0.80802 |  0:03:48s\n",
      "epoch 88 | loss: 0.41573 | val_0_auc: 0.80766 |  0:03:51s\n",
      "epoch 89 | loss: 0.41213 | val_0_auc: 0.80826 |  0:03:53s\n",
      "epoch 90 | loss: 0.41363 | val_0_auc: 0.81123 |  0:03:56s\n",
      "epoch 91 | loss: 0.41146 | val_0_auc: 0.80478 |  0:03:58s\n",
      "epoch 92 | loss: 0.41363 | val_0_auc: 0.80973 |  0:04:01s\n",
      "epoch 93 | loss: 0.41282 | val_0_auc: 0.81254 |  0:04:03s\n",
      "epoch 94 | loss: 0.41035 | val_0_auc: 0.81465 |  0:04:05s\n",
      "epoch 95 | loss: 0.40724 | val_0_auc: 0.81485 |  0:04:08s\n",
      "epoch 96 | loss: 0.41237 | val_0_auc: 0.81625 |  0:04:11s\n",
      "epoch 97 | loss: 0.40864 | val_0_auc: 0.81858 |  0:04:13s\n",
      "epoch 98 | loss: 0.41029 | val_0_auc: 0.81604 |  0:04:16s\n",
      "epoch 99 | loss: 0.40685 | val_0_auc: 0.81757 |  0:04:19s\n",
      "epoch 100| loss: 0.40761 | val_0_auc: 0.81608 |  0:04:21s\n",
      "epoch 101| loss: 0.41009 | val_0_auc: 0.81533 |  0:04:23s\n",
      "epoch 102| loss: 0.40949 | val_0_auc: 0.81411 |  0:04:26s\n",
      "epoch 103| loss: 0.40926 | val_0_auc: 0.8154  |  0:04:28s\n",
      "epoch 104| loss: 0.40488 | val_0_auc: 0.81302 |  0:04:31s\n",
      "epoch 105| loss: 0.40479 | val_0_auc: 0.8159  |  0:04:33s\n",
      "epoch 106| loss: 0.4057  | val_0_auc: 0.81902 |  0:04:35s\n",
      "epoch 107| loss: 0.40338 | val_0_auc: 0.81726 |  0:04:38s\n",
      "epoch 108| loss: 0.4047  | val_0_auc: 0.81835 |  0:04:40s\n",
      "epoch 109| loss: 0.40358 | val_0_auc: 0.81812 |  0:04:43s\n",
      "epoch 110| loss: 0.4041  | val_0_auc: 0.81938 |  0:04:45s\n",
      "epoch 111| loss: 0.40299 | val_0_auc: 0.82223 |  0:04:47s\n",
      "epoch 112| loss: 0.40083 | val_0_auc: 0.82279 |  0:04:50s\n",
      "epoch 113| loss: 0.40287 | val_0_auc: 0.82076 |  0:04:52s\n",
      "epoch 114| loss: 0.39772 | val_0_auc: 0.82273 |  0:04:55s\n",
      "epoch 115| loss: 0.40153 | val_0_auc: 0.82289 |  0:04:57s\n",
      "epoch 116| loss: 0.39907 | val_0_auc: 0.82817 |  0:05:00s\n",
      "epoch 117| loss: 0.39824 | val_0_auc: 0.82806 |  0:05:02s\n",
      "epoch 118| loss: 0.39769 | val_0_auc: 0.82653 |  0:05:05s\n",
      "epoch 119| loss: 0.39676 | val_0_auc: 0.82746 |  0:05:07s\n",
      "epoch 120| loss: 0.39628 | val_0_auc: 0.82707 |  0:05:10s\n",
      "epoch 121| loss: 0.39496 | val_0_auc: 0.82691 |  0:05:12s\n",
      "epoch 122| loss: 0.39718 | val_0_auc: 0.82933 |  0:05:15s\n",
      "epoch 123| loss: 0.39777 | val_0_auc: 0.83036 |  0:05:17s\n",
      "epoch 124| loss: 0.39473 | val_0_auc: 0.83021 |  0:05:19s\n",
      "epoch 125| loss: 0.39298 | val_0_auc: 0.83106 |  0:05:22s\n",
      "epoch 126| loss: 0.39489 | val_0_auc: 0.82946 |  0:05:24s\n",
      "epoch 127| loss: 0.39204 | val_0_auc: 0.83231 |  0:05:27s\n",
      "epoch 128| loss: 0.38981 | val_0_auc: 0.83308 |  0:05:29s\n",
      "epoch 129| loss: 0.3887  | val_0_auc: 0.83221 |  0:05:32s\n",
      "epoch 130| loss: 0.38943 | val_0_auc: 0.83343 |  0:05:34s\n",
      "epoch 131| loss: 0.38507 | val_0_auc: 0.83471 |  0:05:37s\n",
      "epoch 132| loss: 0.38938 | val_0_auc: 0.83384 |  0:05:39s\n",
      "epoch 133| loss: 0.38489 | val_0_auc: 0.83418 |  0:05:41s\n",
      "epoch 134| loss: 0.38167 | val_0_auc: 0.83904 |  0:05:44s\n",
      "epoch 135| loss: 0.38214 | val_0_auc: 0.83791 |  0:05:46s\n",
      "epoch 136| loss: 0.38255 | val_0_auc: 0.83862 |  0:05:48s\n",
      "epoch 137| loss: 0.37897 | val_0_auc: 0.83837 |  0:05:51s\n",
      "epoch 138| loss: 0.38412 | val_0_auc: 0.83876 |  0:05:53s\n",
      "epoch 139| loss: 0.37989 | val_0_auc: 0.83871 |  0:05:56s\n",
      "epoch 140| loss: 0.38357 | val_0_auc: 0.83883 |  0:05:58s\n",
      "epoch 141| loss: 0.38121 | val_0_auc: 0.84005 |  0:06:00s\n",
      "epoch 142| loss: 0.38041 | val_0_auc: 0.83983 |  0:06:03s\n",
      "epoch 143| loss: 0.38045 | val_0_auc: 0.84091 |  0:06:05s\n",
      "epoch 144| loss: 0.37771 | val_0_auc: 0.83989 |  0:06:08s\n",
      "epoch 145| loss: 0.37839 | val_0_auc: 0.84154 |  0:06:10s\n",
      "epoch 146| loss: 0.37907 | val_0_auc: 0.84167 |  0:06:13s\n",
      "epoch 147| loss: 0.37743 | val_0_auc: 0.84225 |  0:06:15s\n",
      "epoch 148| loss: 0.37821 | val_0_auc: 0.84079 |  0:06:18s\n",
      "epoch 149| loss: 0.37509 | val_0_auc: 0.84422 |  0:06:20s\n",
      "epoch 150| loss: 0.37373 | val_0_auc: 0.84516 |  0:06:23s\n",
      "epoch 151| loss: 0.37242 | val_0_auc: 0.84366 |  0:06:25s\n",
      "epoch 152| loss: 0.37305 | val_0_auc: 0.8441  |  0:06:27s\n",
      "epoch 153| loss: 0.37416 | val_0_auc: 0.8435  |  0:06:30s\n",
      "epoch 154| loss: 0.37188 | val_0_auc: 0.84429 |  0:06:32s\n",
      "epoch 155| loss: 0.37289 | val_0_auc: 0.84739 |  0:06:35s\n",
      "epoch 156| loss: 0.37107 | val_0_auc: 0.84963 |  0:06:37s\n",
      "epoch 157| loss: 0.37115 | val_0_auc: 0.84781 |  0:06:39s\n",
      "epoch 158| loss: 0.36863 | val_0_auc: 0.84773 |  0:06:42s\n",
      "epoch 159| loss: 0.3729  | val_0_auc: 0.84941 |  0:06:44s\n",
      "epoch 160| loss: 0.36916 | val_0_auc: 0.8505  |  0:06:47s\n",
      "epoch 161| loss: 0.3703  | val_0_auc: 0.84958 |  0:06:49s\n",
      "epoch 162| loss: 0.36896 | val_0_auc: 0.85333 |  0:06:51s\n",
      "epoch 163| loss: 0.36738 | val_0_auc: 0.85104 |  0:06:54s\n",
      "epoch 164| loss: 0.36705 | val_0_auc: 0.85276 |  0:06:56s\n",
      "epoch 165| loss: 0.36639 | val_0_auc: 0.85215 |  0:06:59s\n",
      "epoch 166| loss: 0.36596 | val_0_auc: 0.85185 |  0:07:01s\n",
      "epoch 167| loss: 0.36526 | val_0_auc: 0.85166 |  0:07:04s\n",
      "epoch 168| loss: 0.3671  | val_0_auc: 0.85288 |  0:07:06s\n",
      "epoch 169| loss: 0.3662  | val_0_auc: 0.85208 |  0:07:09s\n",
      "epoch 170| loss: 0.36643 | val_0_auc: 0.85196 |  0:07:12s\n",
      "epoch 171| loss: 0.36603 | val_0_auc: 0.85347 |  0:07:14s\n",
      "epoch 172| loss: 0.36562 | val_0_auc: 0.85299 |  0:07:17s\n",
      "epoch 173| loss: 0.36372 | val_0_auc: 0.85411 |  0:07:19s\n",
      "epoch 174| loss: 0.36312 | val_0_auc: 0.855   |  0:07:22s\n",
      "epoch 175| loss: 0.36165 | val_0_auc: 0.85437 |  0:07:24s\n",
      "epoch 176| loss: 0.36273 | val_0_auc: 0.8542  |  0:07:26s\n",
      "epoch 177| loss: 0.36147 | val_0_auc: 0.85303 |  0:07:29s\n",
      "epoch 178| loss: 0.36135 | val_0_auc: 0.85376 |  0:07:33s\n",
      "epoch 179| loss: 0.36022 | val_0_auc: 0.85468 |  0:07:36s\n",
      "epoch 180| loss: 0.36169 | val_0_auc: 0.85302 |  0:07:39s\n",
      "epoch 181| loss: 0.36002 | val_0_auc: 0.85567 |  0:07:42s\n",
      "epoch 182| loss: 0.35872 | val_0_auc: 0.85654 |  0:07:44s\n",
      "epoch 183| loss: 0.35819 | val_0_auc: 0.85512 |  0:07:46s\n",
      "epoch 184| loss: 0.3587  | val_0_auc: 0.8577  |  0:07:49s\n",
      "epoch 185| loss: 0.35853 | val_0_auc: 0.85309 |  0:07:52s\n",
      "epoch 186| loss: 0.35469 | val_0_auc: 0.85383 |  0:07:56s\n",
      "epoch 187| loss: 0.35703 | val_0_auc: 0.85648 |  0:07:59s\n",
      "epoch 188| loss: 0.35801 | val_0_auc: 0.85487 |  0:08:01s\n",
      "epoch 189| loss: 0.3538  | val_0_auc: 0.85722 |  0:08:04s\n",
      "epoch 190| loss: 0.35744 | val_0_auc: 0.85771 |  0:08:06s\n",
      "epoch 191| loss: 0.35299 | val_0_auc: 0.8587  |  0:08:09s\n",
      "epoch 192| loss: 0.35228 | val_0_auc: 0.85969 |  0:08:12s\n",
      "epoch 193| loss: 0.35174 | val_0_auc: 0.8581  |  0:08:16s\n",
      "epoch 194| loss: 0.35047 | val_0_auc: 0.85805 |  0:08:19s\n",
      "epoch 195| loss: 0.35184 | val_0_auc: 0.85952 |  0:08:22s\n",
      "epoch 196| loss: 0.35189 | val_0_auc: 0.86044 |  0:08:24s\n",
      "epoch 197| loss: 0.35095 | val_0_auc: 0.86123 |  0:08:26s\n",
      "epoch 198| loss: 0.35042 | val_0_auc: 0.86058 |  0:08:29s\n",
      "epoch 199| loss: 0.35075 | val_0_auc: 0.86037 |  0:08:31s\n",
      "epoch 200| loss: 0.35117 | val_0_auc: 0.86343 |  0:08:34s\n",
      "epoch 201| loss: 0.3488  | val_0_auc: 0.86249 |  0:08:36s\n",
      "epoch 202| loss: 0.34788 | val_0_auc: 0.86247 |  0:08:39s\n",
      "epoch 203| loss: 0.35332 | val_0_auc: 0.86149 |  0:08:41s\n",
      "epoch 204| loss: 0.34973 | val_0_auc: 0.86235 |  0:08:44s\n",
      "epoch 205| loss: 0.35166 | val_0_auc: 0.8644  |  0:08:47s\n",
      "epoch 206| loss: 0.34682 | val_0_auc: 0.86413 |  0:08:50s\n",
      "epoch 207| loss: 0.34807 | val_0_auc: 0.86305 |  0:08:52s\n",
      "epoch 208| loss: 0.34558 | val_0_auc: 0.86502 |  0:08:55s\n",
      "epoch 209| loss: 0.34626 | val_0_auc: 0.86486 |  0:08:57s\n",
      "epoch 210| loss: 0.34575 | val_0_auc: 0.86418 |  0:08:59s\n",
      "epoch 211| loss: 0.34713 | val_0_auc: 0.8628  |  0:09:02s\n",
      "epoch 212| loss: 0.34591 | val_0_auc: 0.86232 |  0:09:04s\n",
      "epoch 213| loss: 0.34464 | val_0_auc: 0.86381 |  0:09:06s\n",
      "epoch 214| loss: 0.34181 | val_0_auc: 0.8641  |  0:09:09s\n",
      "epoch 215| loss: 0.34095 | val_0_auc: 0.86316 |  0:09:12s\n",
      "epoch 216| loss: 0.34235 | val_0_auc: 0.86645 |  0:09:14s\n",
      "epoch 217| loss: 0.34051 | val_0_auc: 0.86622 |  0:09:17s\n",
      "epoch 218| loss: 0.34287 | val_0_auc: 0.8679  |  0:09:19s\n",
      "epoch 219| loss: 0.34329 | val_0_auc: 0.8697  |  0:09:22s\n",
      "epoch 220| loss: 0.34177 | val_0_auc: 0.86852 |  0:09:24s\n",
      "epoch 221| loss: 0.33808 | val_0_auc: 0.86969 |  0:09:26s\n",
      "epoch 222| loss: 0.34105 | val_0_auc: 0.87034 |  0:09:29s\n",
      "epoch 223| loss: 0.33974 | val_0_auc: 0.86894 |  0:09:31s\n",
      "epoch 224| loss: 0.33853 | val_0_auc: 0.86918 |  0:09:33s\n",
      "epoch 225| loss: 0.33746 | val_0_auc: 0.86979 |  0:09:36s\n",
      "epoch 226| loss: 0.33692 | val_0_auc: 0.86847 |  0:09:38s\n",
      "epoch 227| loss: 0.33851 | val_0_auc: 0.86932 |  0:09:41s\n",
      "epoch 228| loss: 0.33729 | val_0_auc: 0.86902 |  0:09:43s\n",
      "epoch 229| loss: 0.33573 | val_0_auc: 0.87001 |  0:09:45s\n",
      "epoch 230| loss: 0.33573 | val_0_auc: 0.87058 |  0:09:48s\n",
      "epoch 231| loss: 0.33707 | val_0_auc: 0.86942 |  0:09:50s\n",
      "epoch 232| loss: 0.33407 | val_0_auc: 0.87059 |  0:09:53s\n",
      "epoch 233| loss: 0.33351 | val_0_auc: 0.86978 |  0:09:55s\n",
      "epoch 234| loss: 0.33613 | val_0_auc: 0.87045 |  0:09:57s\n",
      "epoch 235| loss: 0.33748 | val_0_auc: 0.86964 |  0:10:00s\n",
      "epoch 236| loss: 0.3332  | val_0_auc: 0.87213 |  0:10:02s\n",
      "epoch 237| loss: 0.33549 | val_0_auc: 0.86977 |  0:10:04s\n",
      "epoch 238| loss: 0.33188 | val_0_auc: 0.87153 |  0:10:07s\n",
      "epoch 239| loss: 0.3359  | val_0_auc: 0.8717  |  0:10:09s\n",
      "epoch 240| loss: 0.33669 | val_0_auc: 0.87242 |  0:10:12s\n",
      "epoch 241| loss: 0.33424 | val_0_auc: 0.87258 |  0:10:14s\n",
      "epoch 242| loss: 0.33607 | val_0_auc: 0.873   |  0:10:16s\n",
      "epoch 243| loss: 0.33179 | val_0_auc: 0.87195 |  0:10:19s\n",
      "epoch 244| loss: 0.32916 | val_0_auc: 0.87243 |  0:10:21s\n",
      "epoch 245| loss: 0.33091 | val_0_auc: 0.87235 |  0:10:24s\n",
      "epoch 246| loss: 0.32922 | val_0_auc: 0.87364 |  0:10:26s\n",
      "epoch 247| loss: 0.33081 | val_0_auc: 0.8727  |  0:10:29s\n",
      "epoch 248| loss: 0.32933 | val_0_auc: 0.87179 |  0:10:31s\n",
      "epoch 249| loss: 0.32972 | val_0_auc: 0.87272 |  0:10:33s\n",
      "epoch 250| loss: 0.32849 | val_0_auc: 0.87281 |  0:10:37s\n",
      "epoch 251| loss: 0.32765 | val_0_auc: 0.87333 |  0:10:39s\n",
      "epoch 252| loss: 0.32786 | val_0_auc: 0.87372 |  0:10:42s\n",
      "epoch 253| loss: 0.3267  | val_0_auc: 0.87485 |  0:10:44s\n",
      "epoch 254| loss: 0.32666 | val_0_auc: 0.8737  |  0:10:46s\n",
      "epoch 255| loss: 0.32537 | val_0_auc: 0.87294 |  0:10:49s\n",
      "epoch 256| loss: 0.32658 | val_0_auc: 0.87446 |  0:10:51s\n",
      "epoch 257| loss: 0.32598 | val_0_auc: 0.87307 |  0:10:54s\n",
      "epoch 258| loss: 0.32756 | val_0_auc: 0.87475 |  0:10:56s\n",
      "epoch 259| loss: 0.32411 | val_0_auc: 0.87231 |  0:10:58s\n",
      "epoch 260| loss: 0.32737 | val_0_auc: 0.87316 |  0:11:01s\n",
      "epoch 261| loss: 0.32402 | val_0_auc: 0.87339 |  0:11:03s\n",
      "epoch 262| loss: 0.32495 | val_0_auc: 0.87339 |  0:11:05s\n",
      "epoch 263| loss: 0.32313 | val_0_auc: 0.87363 |  0:11:08s\n",
      "epoch 264| loss: 0.32204 | val_0_auc: 0.8747  |  0:11:10s\n",
      "epoch 265| loss: 0.32242 | val_0_auc: 0.8747  |  0:11:13s\n",
      "epoch 266| loss: 0.32431 | val_0_auc: 0.87397 |  0:11:15s\n",
      "epoch 267| loss: 0.32121 | val_0_auc: 0.87734 |  0:11:17s\n",
      "epoch 268| loss: 0.32254 | val_0_auc: 0.87866 |  0:11:20s\n",
      "epoch 269| loss: 0.32079 | val_0_auc: 0.87835 |  0:11:22s\n",
      "epoch 270| loss: 0.32052 | val_0_auc: 0.87615 |  0:11:25s\n",
      "epoch 271| loss: 0.31987 | val_0_auc: 0.87615 |  0:11:27s\n",
      "epoch 272| loss: 0.32146 | val_0_auc: 0.8759  |  0:11:29s\n",
      "epoch 273| loss: 0.31934 | val_0_auc: 0.87858 |  0:11:31s\n",
      "epoch 274| loss: 0.31704 | val_0_auc: 0.87814 |  0:11:34s\n",
      "epoch 275| loss: 0.31788 | val_0_auc: 0.87673 |  0:11:36s\n",
      "epoch 276| loss: 0.31934 | val_0_auc: 0.87651 |  0:11:39s\n",
      "epoch 277| loss: 0.31794 | val_0_auc: 0.87678 |  0:11:41s\n",
      "epoch 278| loss: 0.31692 | val_0_auc: 0.87611 |  0:11:43s\n",
      "epoch 279| loss: 0.31757 | val_0_auc: 0.87719 |  0:11:46s\n",
      "epoch 280| loss: 0.31894 | val_0_auc: 0.87812 |  0:11:48s\n",
      "epoch 281| loss: 0.31675 | val_0_auc: 0.87788 |  0:11:50s\n",
      "epoch 282| loss: 0.31722 | val_0_auc: 0.87827 |  0:11:53s\n",
      "epoch 283| loss: 0.31765 | val_0_auc: 0.87983 |  0:11:55s\n",
      "epoch 284| loss: 0.31703 | val_0_auc: 0.88057 |  0:11:58s\n",
      "epoch 285| loss: 0.31597 | val_0_auc: 0.8797  |  0:12:00s\n",
      "epoch 286| loss: 0.31655 | val_0_auc: 0.88206 |  0:12:02s\n",
      "epoch 287| loss: 0.31559 | val_0_auc: 0.88086 |  0:12:05s\n",
      "epoch 288| loss: 0.31581 | val_0_auc: 0.88047 |  0:12:07s\n",
      "epoch 289| loss: 0.31264 | val_0_auc: 0.88149 |  0:12:09s\n",
      "epoch 290| loss: 0.31501 | val_0_auc: 0.88126 |  0:12:12s\n",
      "epoch 291| loss: 0.31511 | val_0_auc: 0.88256 |  0:12:14s\n",
      "epoch 292| loss: 0.31526 | val_0_auc: 0.88228 |  0:12:16s\n",
      "epoch 293| loss: 0.3149  | val_0_auc: 0.88196 |  0:12:19s\n",
      "epoch 294| loss: 0.31281 | val_0_auc: 0.88305 |  0:12:21s\n",
      "epoch 295| loss: 0.31137 | val_0_auc: 0.88177 |  0:12:24s\n",
      "epoch 296| loss: 0.31183 | val_0_auc: 0.88356 |  0:12:26s\n",
      "epoch 297| loss: 0.31371 | val_0_auc: 0.88414 |  0:12:28s\n",
      "epoch 298| loss: 0.30987 | val_0_auc: 0.88437 |  0:12:31s\n",
      "epoch 299| loss: 0.30982 | val_0_auc: 0.88442 |  0:12:33s\n",
      "epoch 300| loss: 0.31213 | val_0_auc: 0.8843  |  0:12:35s\n",
      "epoch 301| loss: 0.31273 | val_0_auc: 0.8826  |  0:12:38s\n",
      "epoch 302| loss: 0.31062 | val_0_auc: 0.88435 |  0:12:40s\n",
      "epoch 303| loss: 0.30889 | val_0_auc: 0.8834  |  0:12:42s\n",
      "epoch 304| loss: 0.3123  | val_0_auc: 0.88355 |  0:12:45s\n",
      "epoch 305| loss: 0.31086 | val_0_auc: 0.88296 |  0:12:47s\n",
      "epoch 306| loss: 0.30914 | val_0_auc: 0.88638 |  0:12:50s\n",
      "epoch 307| loss: 0.30855 | val_0_auc: 0.88556 |  0:12:52s\n",
      "epoch 308| loss: 0.30723 | val_0_auc: 0.88586 |  0:12:54s\n",
      "epoch 309| loss: 0.30964 | val_0_auc: 0.88507 |  0:12:57s\n",
      "epoch 310| loss: 0.31008 | val_0_auc: 0.88475 |  0:12:59s\n",
      "epoch 311| loss: 0.31064 | val_0_auc: 0.88467 |  0:13:02s\n",
      "epoch 312| loss: 0.30857 | val_0_auc: 0.88501 |  0:13:04s\n",
      "epoch 313| loss: 0.31004 | val_0_auc: 0.88508 |  0:13:06s\n",
      "epoch 314| loss: 0.30907 | val_0_auc: 0.88503 |  0:13:09s\n",
      "epoch 315| loss: 0.30773 | val_0_auc: 0.88596 |  0:13:11s\n",
      "epoch 316| loss: 0.30843 | val_0_auc: 0.8855  |  0:13:13s\n",
      "epoch 317| loss: 0.30592 | val_0_auc: 0.88656 |  0:13:16s\n",
      "epoch 318| loss: 0.30283 | val_0_auc: 0.88679 |  0:13:18s\n",
      "epoch 319| loss: 0.30598 | val_0_auc: 0.88596 |  0:13:21s\n",
      "epoch 320| loss: 0.30155 | val_0_auc: 0.88595 |  0:13:23s\n",
      "epoch 321| loss: 0.30692 | val_0_auc: 0.88633 |  0:13:25s\n",
      "epoch 322| loss: 0.3044  | val_0_auc: 0.88596 |  0:13:28s\n",
      "epoch 323| loss: 0.30586 | val_0_auc: 0.88549 |  0:13:30s\n",
      "epoch 324| loss: 0.30651 | val_0_auc: 0.88795 |  0:13:32s\n",
      "epoch 325| loss: 0.30631 | val_0_auc: 0.88608 |  0:13:35s\n",
      "epoch 326| loss: 0.30535 | val_0_auc: 0.88736 |  0:13:37s\n",
      "epoch 327| loss: 0.30304 | val_0_auc: 0.88636 |  0:13:39s\n",
      "epoch 328| loss: 0.30395 | val_0_auc: 0.88679 |  0:13:42s\n",
      "epoch 329| loss: 0.30311 | val_0_auc: 0.88707 |  0:13:44s\n",
      "epoch 330| loss: 0.30182 | val_0_auc: 0.88699 |  0:13:46s\n",
      "epoch 331| loss: 0.30555 | val_0_auc: 0.887   |  0:13:49s\n",
      "epoch 332| loss: 0.30001 | val_0_auc: 0.88825 |  0:13:51s\n",
      "epoch 333| loss: 0.3018  | val_0_auc: 0.88649 |  0:13:53s\n",
      "epoch 334| loss: 0.29966 | val_0_auc: 0.88728 |  0:13:56s\n",
      "epoch 335| loss: 0.30292 | val_0_auc: 0.88792 |  0:13:58s\n",
      "epoch 336| loss: 0.30164 | val_0_auc: 0.88717 |  0:14:01s\n",
      "epoch 337| loss: 0.30176 | val_0_auc: 0.88868 |  0:14:03s\n",
      "epoch 338| loss: 0.29931 | val_0_auc: 0.8888  |  0:14:05s\n",
      "epoch 339| loss: 0.29928 | val_0_auc: 0.88916 |  0:14:08s\n",
      "epoch 340| loss: 0.30101 | val_0_auc: 0.88798 |  0:14:10s\n",
      "epoch 341| loss: 0.29985 | val_0_auc: 0.8877  |  0:14:13s\n",
      "epoch 342| loss: 0.29989 | val_0_auc: 0.8888  |  0:14:15s\n",
      "epoch 343| loss: 0.30012 | val_0_auc: 0.88939 |  0:14:17s\n",
      "epoch 344| loss: 0.29962 | val_0_auc: 0.88845 |  0:14:20s\n",
      "epoch 345| loss: 0.29612 | val_0_auc: 0.88916 |  0:14:22s\n",
      "epoch 346| loss: 0.29828 | val_0_auc: 0.88943 |  0:14:24s\n",
      "epoch 347| loss: 0.29824 | val_0_auc: 0.89005 |  0:14:27s\n",
      "epoch 348| loss: 0.29564 | val_0_auc: 0.88987 |  0:14:29s\n",
      "epoch 349| loss: 0.29835 | val_0_auc: 0.88927 |  0:14:32s\n",
      "epoch 350| loss: 0.29779 | val_0_auc: 0.88968 |  0:14:34s\n",
      "epoch 351| loss: 0.29882 | val_0_auc: 0.88921 |  0:14:36s\n",
      "epoch 352| loss: 0.29822 | val_0_auc: 0.88985 |  0:14:39s\n",
      "epoch 353| loss: 0.29758 | val_0_auc: 0.88921 |  0:14:41s\n",
      "epoch 354| loss: 0.29926 | val_0_auc: 0.89046 |  0:14:44s\n",
      "epoch 355| loss: 0.29534 | val_0_auc: 0.88985 |  0:14:46s\n",
      "epoch 356| loss: 0.29453 | val_0_auc: 0.89017 |  0:14:49s\n",
      "epoch 357| loss: 0.29634 | val_0_auc: 0.89019 |  0:14:51s\n",
      "epoch 358| loss: 0.29461 | val_0_auc: 0.8902  |  0:14:53s\n",
      "epoch 359| loss: 0.29392 | val_0_auc: 0.89036 |  0:14:56s\n",
      "epoch 360| loss: 0.29248 | val_0_auc: 0.89002 |  0:14:58s\n",
      "epoch 361| loss: 0.29179 | val_0_auc: 0.8907  |  0:15:00s\n",
      "epoch 362| loss: 0.29441 | val_0_auc: 0.89189 |  0:15:03s\n",
      "epoch 363| loss: 0.29097 | val_0_auc: 0.8907  |  0:15:05s\n",
      "epoch 364| loss: 0.29464 | val_0_auc: 0.89047 |  0:15:07s\n",
      "epoch 365| loss: 0.29132 | val_0_auc: 0.89102 |  0:15:10s\n",
      "epoch 366| loss: 0.29213 | val_0_auc: 0.89111 |  0:15:12s\n",
      "epoch 367| loss: 0.29183 | val_0_auc: 0.8896  |  0:15:14s\n",
      "epoch 368| loss: 0.29209 | val_0_auc: 0.89073 |  0:15:17s\n",
      "epoch 369| loss: 0.28941 | val_0_auc: 0.89102 |  0:15:19s\n",
      "epoch 370| loss: 0.2912  | val_0_auc: 0.88923 |  0:15:21s\n",
      "epoch 371| loss: 0.2927  | val_0_auc: 0.89016 |  0:15:24s\n",
      "epoch 372| loss: 0.29237 | val_0_auc: 0.89111 |  0:15:26s\n",
      "epoch 373| loss: 0.28947 | val_0_auc: 0.89232 |  0:15:29s\n",
      "epoch 374| loss: 0.28826 | val_0_auc: 0.89244 |  0:15:31s\n",
      "epoch 375| loss: 0.28841 | val_0_auc: 0.89276 |  0:15:34s\n",
      "epoch 376| loss: 0.28811 | val_0_auc: 0.89347 |  0:15:36s\n",
      "epoch 377| loss: 0.29129 | val_0_auc: 0.8931  |  0:15:38s\n",
      "epoch 378| loss: 0.28836 | val_0_auc: 0.89373 |  0:15:41s\n",
      "epoch 379| loss: 0.28777 | val_0_auc: 0.89418 |  0:15:43s\n",
      "epoch 380| loss: 0.2894  | val_0_auc: 0.89305 |  0:15:46s\n",
      "epoch 381| loss: 0.28627 | val_0_auc: 0.89392 |  0:15:48s\n",
      "epoch 382| loss: 0.28675 | val_0_auc: 0.89407 |  0:15:50s\n",
      "epoch 383| loss: 0.2861  | val_0_auc: 0.89352 |  0:15:53s\n",
      "epoch 384| loss: 0.28561 | val_0_auc: 0.8933  |  0:15:55s\n",
      "epoch 385| loss: 0.28672 | val_0_auc: 0.89444 |  0:15:58s\n",
      "epoch 386| loss: 0.28805 | val_0_auc: 0.89332 |  0:16:00s\n",
      "epoch 387| loss: 0.28496 | val_0_auc: 0.89506 |  0:16:02s\n",
      "epoch 388| loss: 0.28466 | val_0_auc: 0.89337 |  0:16:05s\n",
      "epoch 389| loss: 0.28667 | val_0_auc: 0.89387 |  0:16:07s\n",
      "epoch 390| loss: 0.28534 | val_0_auc: 0.89311 |  0:16:10s\n",
      "epoch 391| loss: 0.28594 | val_0_auc: 0.89374 |  0:16:12s\n",
      "epoch 392| loss: 0.28298 | val_0_auc: 0.89327 |  0:16:15s\n",
      "epoch 393| loss: 0.28282 | val_0_auc: 0.89371 |  0:16:17s\n",
      "epoch 394| loss: 0.28651 | val_0_auc: 0.8929  |  0:16:20s\n",
      "epoch 395| loss: 0.28494 | val_0_auc: 0.89273 |  0:16:22s\n",
      "epoch 396| loss: 0.2854  | val_0_auc: 0.89352 |  0:16:25s\n",
      "epoch 397| loss: 0.28379 | val_0_auc: 0.89428 |  0:16:27s\n",
      "epoch 398| loss: 0.2844  | val_0_auc: 0.89475 |  0:16:29s\n",
      "epoch 399| loss: 0.28411 | val_0_auc: 0.89414 |  0:16:32s\n",
      "epoch 400| loss: 0.28527 | val_0_auc: 0.89454 |  0:16:34s\n",
      "epoch 401| loss: 0.28271 | val_0_auc: 0.8947  |  0:16:37s\n",
      "epoch 402| loss: 0.28425 | val_0_auc: 0.89423 |  0:16:39s\n",
      "epoch 403| loss: 0.28222 | val_0_auc: 0.89509 |  0:16:41s\n",
      "epoch 404| loss: 0.28444 | val_0_auc: 0.89351 |  0:16:44s\n",
      "epoch 405| loss: 0.28157 | val_0_auc: 0.89422 |  0:16:46s\n",
      "epoch 406| loss: 0.28366 | val_0_auc: 0.89239 |  0:16:48s\n",
      "epoch 407| loss: 0.28188 | val_0_auc: 0.89268 |  0:16:51s\n",
      "epoch 408| loss: 0.28142 | val_0_auc: 0.89174 |  0:16:53s\n",
      "epoch 409| loss: 0.28361 | val_0_auc: 0.89231 |  0:16:56s\n",
      "epoch 410| loss: 0.28161 | val_0_auc: 0.89186 |  0:16:58s\n",
      "epoch 411| loss: 0.28363 | val_0_auc: 0.89329 |  0:17:00s\n",
      "epoch 412| loss: 0.28157 | val_0_auc: 0.89365 |  0:17:03s\n",
      "epoch 413| loss: 0.28167 | val_0_auc: 0.89323 |  0:17:05s\n",
      "epoch 414| loss: 0.28073 | val_0_auc: 0.8935  |  0:17:08s\n",
      "epoch 415| loss: 0.28046 | val_0_auc: 0.89354 |  0:17:10s\n",
      "epoch 416| loss: 0.27763 | val_0_auc: 0.89592 |  0:17:13s\n",
      "epoch 417| loss: 0.27829 | val_0_auc: 0.89511 |  0:17:15s\n",
      "epoch 418| loss: 0.27938 | val_0_auc: 0.89559 |  0:17:17s\n",
      "epoch 419| loss: 0.27812 | val_0_auc: 0.8941  |  0:17:20s\n",
      "epoch 420| loss: 0.28028 | val_0_auc: 0.89438 |  0:17:22s\n",
      "epoch 421| loss: 0.28034 | val_0_auc: 0.89527 |  0:17:24s\n",
      "epoch 422| loss: 0.27991 | val_0_auc: 0.89601 |  0:17:27s\n",
      "epoch 423| loss: 0.28208 | val_0_auc: 0.89665 |  0:17:29s\n",
      "epoch 424| loss: 0.27964 | val_0_auc: 0.89637 |  0:17:32s\n",
      "epoch 425| loss: 0.28196 | val_0_auc: 0.8954  |  0:17:34s\n",
      "epoch 426| loss: 0.28233 | val_0_auc: 0.89505 |  0:17:36s\n",
      "epoch 427| loss: 0.27918 | val_0_auc: 0.89457 |  0:17:39s\n",
      "epoch 428| loss: 0.27844 | val_0_auc: 0.89561 |  0:17:41s\n",
      "epoch 429| loss: 0.27969 | val_0_auc: 0.89594 |  0:17:44s\n",
      "epoch 430| loss: 0.27579 | val_0_auc: 0.89526 |  0:17:46s\n",
      "epoch 431| loss: 0.27805 | val_0_auc: 0.89497 |  0:17:48s\n",
      "epoch 432| loss: 0.27919 | val_0_auc: 0.89432 |  0:17:51s\n",
      "epoch 433| loss: 0.27716 | val_0_auc: 0.89541 |  0:17:53s\n",
      "epoch 434| loss: 0.27724 | val_0_auc: 0.89526 |  0:17:56s\n",
      "epoch 435| loss: 0.27865 | val_0_auc: 0.89649 |  0:17:58s\n",
      "epoch 436| loss: 0.27581 | val_0_auc: 0.89659 |  0:18:00s\n",
      "epoch 437| loss: 0.27807 | val_0_auc: 0.89723 |  0:18:03s\n",
      "epoch 438| loss: 0.27677 | val_0_auc: 0.89826 |  0:18:05s\n",
      "epoch 439| loss: 0.27744 | val_0_auc: 0.89707 |  0:18:07s\n",
      "epoch 440| loss: 0.27621 | val_0_auc: 0.8961  |  0:18:10s\n",
      "epoch 441| loss: 0.27679 | val_0_auc: 0.89658 |  0:18:12s\n",
      "epoch 442| loss: 0.27946 | val_0_auc: 0.89646 |  0:18:15s\n",
      "epoch 443| loss: 0.274   | val_0_auc: 0.89775 |  0:18:17s\n",
      "epoch 444| loss: 0.27711 | val_0_auc: 0.89768 |  0:18:20s\n",
      "epoch 445| loss: 0.27395 | val_0_auc: 0.89731 |  0:18:22s\n",
      "epoch 446| loss: 0.27227 | val_0_auc: 0.89643 |  0:18:24s\n",
      "epoch 447| loss: 0.27361 | val_0_auc: 0.8974  |  0:18:27s\n",
      "epoch 448| loss: 0.27313 | val_0_auc: 0.89787 |  0:18:29s\n",
      "epoch 449| loss: 0.27273 | val_0_auc: 0.89721 |  0:18:32s\n",
      "epoch 450| loss: 0.27308 | val_0_auc: 0.89796 |  0:18:34s\n",
      "epoch 451| loss: 0.27271 | val_0_auc: 0.89938 |  0:18:37s\n",
      "epoch 452| loss: 0.2709  | val_0_auc: 0.89942 |  0:18:39s\n",
      "epoch 453| loss: 0.27166 | val_0_auc: 0.89798 |  0:18:42s\n",
      "epoch 454| loss: 0.27395 | val_0_auc: 0.89875 |  0:18:45s\n",
      "epoch 455| loss: 0.2725  | val_0_auc: 0.89994 |  0:18:49s\n",
      "epoch 456| loss: 0.27205 | val_0_auc: 0.89938 |  0:18:52s\n",
      "epoch 457| loss: 0.27138 | val_0_auc: 0.90038 |  0:18:54s\n",
      "epoch 458| loss: 0.27198 | val_0_auc: 0.89901 |  0:18:57s\n",
      "epoch 459| loss: 0.27103 | val_0_auc: 0.89923 |  0:18:59s\n",
      "epoch 460| loss: 0.27009 | val_0_auc: 0.89958 |  0:19:02s\n",
      "epoch 461| loss: 0.2707  | val_0_auc: 0.89946 |  0:19:04s\n",
      "epoch 462| loss: 0.27014 | val_0_auc: 0.89886 |  0:19:06s\n",
      "epoch 463| loss: 0.27109 | val_0_auc: 0.89913 |  0:19:09s\n",
      "epoch 464| loss: 0.27032 | val_0_auc: 0.89891 |  0:19:11s\n",
      "epoch 465| loss: 0.2724  | val_0_auc: 0.89843 |  0:19:14s\n",
      "epoch 466| loss: 0.26922 | val_0_auc: 0.89915 |  0:19:16s\n",
      "epoch 467| loss: 0.26943 | val_0_auc: 0.89805 |  0:19:19s\n",
      "epoch 468| loss: 0.26882 | val_0_auc: 0.89945 |  0:19:21s\n",
      "epoch 469| loss: 0.27053 | val_0_auc: 0.89938 |  0:19:24s\n",
      "epoch 470| loss: 0.26796 | val_0_auc: 0.90014 |  0:19:26s\n",
      "epoch 471| loss: 0.26732 | val_0_auc: 0.89937 |  0:19:29s\n",
      "epoch 472| loss: 0.26699 | val_0_auc: 0.89941 |  0:19:31s\n",
      "epoch 473| loss: 0.26757 | val_0_auc: 0.89899 |  0:19:33s\n",
      "epoch 474| loss: 0.2666  | val_0_auc: 0.89936 |  0:19:36s\n",
      "epoch 475| loss: 0.26549 | val_0_auc: 0.8995  |  0:19:38s\n",
      "epoch 476| loss: 0.26655 | val_0_auc: 0.89968 |  0:19:40s\n",
      "epoch 477| loss: 0.26484 | val_0_auc: 0.9     |  0:19:43s\n",
      "\n",
      "Early stopping occurred at epoch 477 with best_epoch = 457 and best_val_0_auc = 0.90038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.77311 | val_0_auc: 0.56916 |  0:00:04s\n",
      "epoch 1  | loss: 0.60237 | val_0_auc: 0.60222 |  0:00:10s\n",
      "epoch 2  | loss: 0.52625 | val_0_auc: 0.6808  |  0:00:14s\n",
      "epoch 3  | loss: 0.50627 | val_0_auc: 0.69959 |  0:00:19s\n",
      "epoch 4  | loss: 0.49721 | val_0_auc: 0.70993 |  0:00:25s\n",
      "epoch 5  | loss: 0.48694 | val_0_auc: 0.73061 |  0:00:30s\n",
      "epoch 6  | loss: 0.48272 | val_0_auc: 0.72831 |  0:00:36s\n",
      "epoch 7  | loss: 0.48743 | val_0_auc: 0.74015 |  0:00:41s\n",
      "epoch 8  | loss: 0.4791  | val_0_auc: 0.74743 |  0:00:47s\n",
      "epoch 9  | loss: 0.47983 | val_0_auc: 0.73837 |  0:00:52s\n",
      "epoch 10 | loss: 0.47445 | val_0_auc: 0.77392 |  0:00:57s\n",
      "epoch 11 | loss: 0.46007 | val_0_auc: 0.79336 |  0:01:02s\n",
      "epoch 12 | loss: 0.4504  | val_0_auc: 0.76134 |  0:01:08s\n",
      "epoch 13 | loss: 0.44561 | val_0_auc: 0.80718 |  0:01:13s\n",
      "epoch 14 | loss: 0.41998 | val_0_auc: 0.81828 |  0:01:18s\n",
      "epoch 15 | loss: 0.40588 | val_0_auc: 0.82461 |  0:01:23s\n",
      "epoch 16 | loss: 0.41848 | val_0_auc: 0.81108 |  0:01:28s\n",
      "epoch 17 | loss: 0.41279 | val_0_auc: 0.82119 |  0:01:33s\n",
      "epoch 18 | loss: 0.40504 | val_0_auc: 0.8254  |  0:01:38s\n",
      "epoch 19 | loss: 0.39785 | val_0_auc: 0.83174 |  0:01:43s\n",
      "epoch 20 | loss: 0.38645 | val_0_auc: 0.84688 |  0:01:50s\n",
      "epoch 21 | loss: 0.38037 | val_0_auc: 0.85223 |  0:01:59s\n",
      "epoch 22 | loss: 0.37813 | val_0_auc: 0.85496 |  0:02:04s\n",
      "epoch 23 | loss: 0.37126 | val_0_auc: 0.85514 |  0:02:09s\n",
      "epoch 24 | loss: 0.36645 | val_0_auc: 0.86195 |  0:02:14s\n",
      "epoch 25 | loss: 0.36159 | val_0_auc: 0.8592  |  0:02:19s\n",
      "epoch 26 | loss: 0.36013 | val_0_auc: 0.85994 |  0:02:24s\n",
      "epoch 27 | loss: 0.35805 | val_0_auc: 0.86152 |  0:02:29s\n",
      "epoch 28 | loss: 0.35221 | val_0_auc: 0.8665  |  0:02:34s\n",
      "epoch 29 | loss: 0.34941 | val_0_auc: 0.87094 |  0:02:39s\n",
      "epoch 30 | loss: 0.33892 | val_0_auc: 0.87203 |  0:02:44s\n",
      "epoch 31 | loss: 0.33955 | val_0_auc: 0.87475 |  0:02:49s\n",
      "epoch 32 | loss: 0.33431 | val_0_auc: 0.88064 |  0:02:55s\n",
      "epoch 33 | loss: 0.32888 | val_0_auc: 0.88067 |  0:03:00s\n",
      "epoch 34 | loss: 0.32758 | val_0_auc: 0.87728 |  0:03:05s\n",
      "epoch 35 | loss: 0.32759 | val_0_auc: 0.8859  |  0:03:10s\n",
      "epoch 36 | loss: 0.32162 | val_0_auc: 0.88791 |  0:03:15s\n",
      "epoch 37 | loss: 0.32297 | val_0_auc: 0.89136 |  0:03:20s\n",
      "epoch 38 | loss: 0.31812 | val_0_auc: 0.89231 |  0:03:25s\n",
      "epoch 39 | loss: 0.31487 | val_0_auc: 0.89267 |  0:03:30s\n",
      "epoch 40 | loss: 0.31335 | val_0_auc: 0.89236 |  0:03:35s\n",
      "epoch 41 | loss: 0.31029 | val_0_auc: 0.8929  |  0:03:41s\n",
      "epoch 42 | loss: 0.3108  | val_0_auc: 0.8926  |  0:03:46s\n",
      "epoch 43 | loss: 0.31109 | val_0_auc: 0.89408 |  0:03:51s\n",
      "epoch 44 | loss: 0.30878 | val_0_auc: 0.89616 |  0:03:56s\n",
      "epoch 45 | loss: 0.30681 | val_0_auc: 0.89801 |  0:04:01s\n",
      "epoch 46 | loss: 0.30557 | val_0_auc: 0.89788 |  0:04:07s\n",
      "epoch 47 | loss: 0.30388 | val_0_auc: 0.89826 |  0:04:11s\n",
      "epoch 48 | loss: 0.30279 | val_0_auc: 0.89935 |  0:04:16s\n",
      "epoch 49 | loss: 0.29851 | val_0_auc: 0.89887 |  0:04:21s\n",
      "epoch 50 | loss: 0.30236 | val_0_auc: 0.89831 |  0:04:26s\n",
      "epoch 51 | loss: 0.30277 | val_0_auc: 0.89973 |  0:04:31s\n",
      "epoch 52 | loss: 0.29793 | val_0_auc: 0.9003  |  0:04:36s\n",
      "epoch 53 | loss: 0.29844 | val_0_auc: 0.89888 |  0:04:41s\n",
      "epoch 54 | loss: 0.30033 | val_0_auc: 0.90137 |  0:04:46s\n",
      "epoch 55 | loss: 0.29709 | val_0_auc: 0.8997  |  0:04:51s\n",
      "epoch 56 | loss: 0.29703 | val_0_auc: 0.90166 |  0:04:56s\n",
      "epoch 57 | loss: 0.29694 | val_0_auc: 0.90184 |  0:05:01s\n",
      "epoch 58 | loss: 0.29561 | val_0_auc: 0.90234 |  0:05:07s\n",
      "epoch 59 | loss: 0.29575 | val_0_auc: 0.90288 |  0:05:12s\n",
      "epoch 60 | loss: 0.29335 | val_0_auc: 0.90496 |  0:05:17s\n",
      "epoch 61 | loss: 0.29157 | val_0_auc: 0.90508 |  0:05:22s\n",
      "epoch 62 | loss: 0.2895  | val_0_auc: 0.90518 |  0:05:27s\n",
      "epoch 63 | loss: 0.29513 | val_0_auc: 0.90265 |  0:05:33s\n",
      "epoch 64 | loss: 0.29239 | val_0_auc: 0.90458 |  0:05:38s\n",
      "epoch 65 | loss: 0.29619 | val_0_auc: 0.89764 |  0:05:43s\n",
      "epoch 66 | loss: 0.2952  | val_0_auc: 0.90243 |  0:05:48s\n",
      "epoch 67 | loss: 0.29399 | val_0_auc: 0.90618 |  0:05:53s\n",
      "epoch 68 | loss: 0.28889 | val_0_auc: 0.90378 |  0:05:58s\n",
      "epoch 69 | loss: 0.28806 | val_0_auc: 0.90251 |  0:06:03s\n",
      "epoch 70 | loss: 0.29072 | val_0_auc: 0.90162 |  0:06:08s\n",
      "epoch 71 | loss: 0.28991 | val_0_auc: 0.90178 |  0:06:12s\n",
      "epoch 72 | loss: 0.29301 | val_0_auc: 0.90399 |  0:06:18s\n",
      "epoch 73 | loss: 0.29032 | val_0_auc: 0.90549 |  0:06:22s\n",
      "epoch 74 | loss: 0.29334 | val_0_auc: 0.90753 |  0:06:27s\n",
      "epoch 75 | loss: 0.29833 | val_0_auc: 0.89768 |  0:06:32s\n",
      "epoch 76 | loss: 0.30256 | val_0_auc: 0.89473 |  0:06:37s\n",
      "epoch 77 | loss: 0.30324 | val_0_auc: 0.89524 |  0:06:42s\n",
      "epoch 78 | loss: 0.29997 | val_0_auc: 0.89839 |  0:06:47s\n",
      "epoch 79 | loss: 0.29633 | val_0_auc: 0.902   |  0:06:52s\n",
      "epoch 80 | loss: 0.29306 | val_0_auc: 0.90364 |  0:06:57s\n",
      "epoch 81 | loss: 0.29104 | val_0_auc: 0.90315 |  0:07:01s\n",
      "epoch 82 | loss: 0.29167 | val_0_auc: 0.90269 |  0:07:06s\n",
      "epoch 83 | loss: 0.28931 | val_0_auc: 0.90465 |  0:07:11s\n",
      "epoch 84 | loss: 0.29284 | val_0_auc: 0.90114 |  0:07:16s\n",
      "epoch 85 | loss: 0.29472 | val_0_auc: 0.90578 |  0:07:21s\n",
      "epoch 86 | loss: 0.29207 | val_0_auc: 0.90458 |  0:07:26s\n",
      "epoch 87 | loss: 0.29278 | val_0_auc: 0.90339 |  0:07:31s\n",
      "epoch 88 | loss: 0.29256 | val_0_auc: 0.90491 |  0:07:36s\n",
      "epoch 89 | loss: 0.29162 | val_0_auc: 0.90619 |  0:07:41s\n",
      "epoch 90 | loss: 0.28773 | val_0_auc: 0.90762 |  0:07:46s\n",
      "epoch 91 | loss: 0.2895  | val_0_auc: 0.90773 |  0:07:51s\n",
      "epoch 92 | loss: 0.28706 | val_0_auc: 0.90859 |  0:07:56s\n",
      "epoch 93 | loss: 0.28549 | val_0_auc: 0.90805 |  0:08:01s\n",
      "epoch 94 | loss: 0.2834  | val_0_auc: 0.90874 |  0:08:06s\n",
      "epoch 95 | loss: 0.28263 | val_0_auc: 0.90941 |  0:08:11s\n",
      "epoch 96 | loss: 0.28359 | val_0_auc: 0.90763 |  0:08:16s\n",
      "epoch 97 | loss: 0.28327 | val_0_auc: 0.90442 |  0:08:21s\n",
      "epoch 98 | loss: 0.28086 | val_0_auc: 0.90902 |  0:08:27s\n",
      "epoch 99 | loss: 0.28226 | val_0_auc: 0.90821 |  0:08:32s\n",
      "epoch 100| loss: 0.28118 | val_0_auc: 0.9092  |  0:08:37s\n",
      "epoch 101| loss: 0.27955 | val_0_auc: 0.90993 |  0:08:42s\n",
      "epoch 102| loss: 0.27883 | val_0_auc: 0.91151 |  0:08:47s\n",
      "epoch 103| loss: 0.27988 | val_0_auc: 0.91149 |  0:08:51s\n",
      "epoch 104| loss: 0.27754 | val_0_auc: 0.91314 |  0:08:56s\n",
      "epoch 105| loss: 0.27666 | val_0_auc: 0.91302 |  0:09:02s\n",
      "epoch 106| loss: 0.27419 | val_0_auc: 0.914   |  0:09:08s\n",
      "epoch 107| loss: 0.27575 | val_0_auc: 0.91417 |  0:09:14s\n",
      "epoch 108| loss: 0.27515 | val_0_auc: 0.91481 |  0:09:20s\n",
      "epoch 109| loss: 0.27606 | val_0_auc: 0.91494 |  0:09:25s\n",
      "epoch 110| loss: 0.27055 | val_0_auc: 0.91582 |  0:09:30s\n",
      "epoch 111| loss: 0.27152 | val_0_auc: 0.91649 |  0:09:35s\n",
      "epoch 112| loss: 0.27271 | val_0_auc: 0.91546 |  0:09:40s\n",
      "epoch 113| loss: 0.27228 | val_0_auc: 0.91529 |  0:09:45s\n",
      "epoch 114| loss: 0.27268 | val_0_auc: 0.91615 |  0:09:50s\n",
      "epoch 115| loss: 0.26982 | val_0_auc: 0.91637 |  0:09:55s\n",
      "epoch 116| loss: 0.2702  | val_0_auc: 0.91772 |  0:10:00s\n",
      "epoch 117| loss: 0.26759 | val_0_auc: 0.91743 |  0:10:05s\n",
      "epoch 118| loss: 0.26778 | val_0_auc: 0.9162  |  0:10:10s\n",
      "epoch 119| loss: 0.26634 | val_0_auc: 0.91734 |  0:10:15s\n",
      "epoch 120| loss: 0.26655 | val_0_auc: 0.9181  |  0:10:21s\n",
      "epoch 121| loss: 0.27082 | val_0_auc: 0.91685 |  0:10:26s\n",
      "epoch 122| loss: 0.26567 | val_0_auc: 0.91701 |  0:10:30s\n",
      "epoch 123| loss: 0.26601 | val_0_auc: 0.91702 |  0:10:35s\n",
      "epoch 124| loss: 0.2666  | val_0_auc: 0.91456 |  0:10:40s\n",
      "epoch 125| loss: 0.27098 | val_0_auc: 0.91592 |  0:10:45s\n",
      "epoch 126| loss: 0.26986 | val_0_auc: 0.9151  |  0:10:50s\n",
      "epoch 127| loss: 0.26792 | val_0_auc: 0.91623 |  0:10:54s\n",
      "epoch 128| loss: 0.26476 | val_0_auc: 0.91641 |  0:11:00s\n",
      "epoch 129| loss: 0.26596 | val_0_auc: 0.91691 |  0:11:04s\n",
      "epoch 130| loss: 0.26385 | val_0_auc: 0.91833 |  0:11:10s\n",
      "epoch 131| loss: 0.26431 | val_0_auc: 0.91847 |  0:11:15s\n",
      "epoch 132| loss: 0.26546 | val_0_auc: 0.91884 |  0:11:20s\n",
      "epoch 133| loss: 0.26279 | val_0_auc: 0.91892 |  0:11:25s\n",
      "epoch 134| loss: 0.26636 | val_0_auc: 0.91761 |  0:11:30s\n",
      "epoch 135| loss: 0.26991 | val_0_auc: 0.91593 |  0:11:34s\n",
      "epoch 136| loss: 0.26643 | val_0_auc: 0.91495 |  0:11:39s\n",
      "epoch 137| loss: 0.26556 | val_0_auc: 0.91795 |  0:11:44s\n",
      "epoch 138| loss: 0.26391 | val_0_auc: 0.91766 |  0:11:49s\n",
      "epoch 139| loss: 0.26286 | val_0_auc: 0.91885 |  0:11:53s\n",
      "epoch 140| loss: 0.26232 | val_0_auc: 0.91804 |  0:11:58s\n",
      "epoch 141| loss: 0.26003 | val_0_auc: 0.91892 |  0:12:03s\n",
      "epoch 142| loss: 0.26262 | val_0_auc: 0.91584 |  0:12:08s\n",
      "epoch 143| loss: 0.26167 | val_0_auc: 0.91873 |  0:12:13s\n",
      "epoch 144| loss: 0.26554 | val_0_auc: 0.91875 |  0:12:18s\n",
      "epoch 145| loss: 0.26259 | val_0_auc: 0.91955 |  0:12:23s\n",
      "epoch 146| loss: 0.2599  | val_0_auc: 0.91975 |  0:12:29s\n",
      "epoch 147| loss: 0.25962 | val_0_auc: 0.91948 |  0:12:36s\n",
      "epoch 148| loss: 0.25929 | val_0_auc: 0.91899 |  0:12:42s\n",
      "epoch 149| loss: 0.25886 | val_0_auc: 0.91906 |  0:12:48s\n",
      "epoch 150| loss: 0.25804 | val_0_auc: 0.91827 |  0:12:54s\n",
      "epoch 151| loss: 0.25561 | val_0_auc: 0.91908 |  0:13:01s\n",
      "epoch 152| loss: 0.2573  | val_0_auc: 0.91898 |  0:13:07s\n",
      "epoch 153| loss: 0.25614 | val_0_auc: 0.91844 |  0:13:13s\n",
      "epoch 154| loss: 0.25685 | val_0_auc: 0.91847 |  0:13:19s\n",
      "epoch 155| loss: 0.25774 | val_0_auc: 0.91832 |  0:13:25s\n",
      "epoch 156| loss: 0.25691 | val_0_auc: 0.91817 |  0:13:32s\n",
      "epoch 157| loss: 0.25615 | val_0_auc: 0.91867 |  0:13:41s\n",
      "epoch 158| loss: 0.26002 | val_0_auc: 0.91739 |  0:13:50s\n",
      "epoch 159| loss: 0.25921 | val_0_auc: 0.91923 |  0:13:56s\n",
      "epoch 160| loss: 0.25782 | val_0_auc: 0.91917 |  0:14:02s\n",
      "epoch 161| loss: 0.25601 | val_0_auc: 0.92012 |  0:14:08s\n",
      "epoch 162| loss: 0.2547  | val_0_auc: 0.91995 |  0:14:15s\n",
      "epoch 163| loss: 0.25695 | val_0_auc: 0.92007 |  0:14:21s\n",
      "epoch 164| loss: 0.25521 | val_0_auc: 0.91975 |  0:14:36s\n",
      "epoch 165| loss: 0.25472 | val_0_auc: 0.91832 |  0:14:44s\n",
      "epoch 166| loss: 0.25691 | val_0_auc: 0.91781 |  0:14:53s\n",
      "epoch 167| loss: 0.25751 | val_0_auc: 0.91788 |  0:15:01s\n",
      "epoch 168| loss: 0.25657 | val_0_auc: 0.91861 |  0:15:08s\n",
      "epoch 169| loss: 0.25778 | val_0_auc: 0.91763 |  0:15:20s\n",
      "epoch 170| loss: 0.25787 | val_0_auc: 0.91468 |  0:15:27s\n",
      "epoch 171| loss: 0.27019 | val_0_auc: 0.913   |  0:15:33s\n",
      "epoch 172| loss: 0.26375 | val_0_auc: 0.91372 |  0:15:38s\n",
      "epoch 173| loss: 0.26262 | val_0_auc: 0.91636 |  0:15:42s\n",
      "epoch 174| loss: 0.26134 | val_0_auc: 0.91677 |  0:15:48s\n",
      "epoch 175| loss: 0.26145 | val_0_auc: 0.91646 |  0:15:54s\n",
      "epoch 176| loss: 0.25941 | val_0_auc: 0.91712 |  0:16:00s\n",
      "epoch 177| loss: 0.25915 | val_0_auc: 0.9172  |  0:16:05s\n",
      "epoch 178| loss: 0.25964 | val_0_auc: 0.91828 |  0:16:10s\n",
      "epoch 179| loss: 0.25894 | val_0_auc: 0.91742 |  0:16:15s\n",
      "epoch 180| loss: 0.25703 | val_0_auc: 0.9146  |  0:16:19s\n",
      "epoch 181| loss: 0.26191 | val_0_auc: 0.91027 |  0:16:24s\n",
      "\n",
      "Early stopping occurred at epoch 181 with best_epoch = 161 and best_val_0_auc = 0.92012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.77351 | val_0_auc: 0.56151 |  0:00:02s\n",
      "epoch 1  | loss: 0.72645 | val_0_auc: 0.57152 |  0:00:04s\n",
      "epoch 2  | loss: 0.67565 | val_0_auc: 0.5992  |  0:00:06s\n",
      "epoch 3  | loss: 0.64445 | val_0_auc: 0.62018 |  0:00:09s\n",
      "epoch 4  | loss: 0.6126  | val_0_auc: 0.64802 |  0:00:12s\n",
      "epoch 5  | loss: 0.59259 | val_0_auc: 0.67113 |  0:00:15s\n",
      "epoch 6  | loss: 0.57788 | val_0_auc: 0.69246 |  0:00:17s\n",
      "epoch 7  | loss: 0.55456 | val_0_auc: 0.71207 |  0:00:20s\n",
      "epoch 8  | loss: 0.54567 | val_0_auc: 0.73274 |  0:00:22s\n",
      "epoch 9  | loss: 0.53007 | val_0_auc: 0.74597 |  0:00:25s\n",
      "epoch 10 | loss: 0.52182 | val_0_auc: 0.75982 |  0:00:27s\n",
      "epoch 11 | loss: 0.50399 | val_0_auc: 0.76959 |  0:00:29s\n",
      "epoch 12 | loss: 0.49936 | val_0_auc: 0.78126 |  0:00:32s\n",
      "epoch 13 | loss: 0.49079 | val_0_auc: 0.79162 |  0:00:34s\n",
      "epoch 14 | loss: 0.48324 | val_0_auc: 0.798   |  0:00:37s\n",
      "epoch 15 | loss: 0.47985 | val_0_auc: 0.80169 |  0:00:39s\n",
      "epoch 16 | loss: 0.47011 | val_0_auc: 0.80739 |  0:00:41s\n",
      "epoch 17 | loss: 0.46232 | val_0_auc: 0.80817 |  0:00:44s\n",
      "epoch 18 | loss: 0.46043 | val_0_auc: 0.81333 |  0:00:46s\n",
      "epoch 19 | loss: 0.45148 | val_0_auc: 0.81984 |  0:00:48s\n",
      "epoch 20 | loss: 0.4447  | val_0_auc: 0.82543 |  0:00:50s\n",
      "epoch 21 | loss: 0.43858 | val_0_auc: 0.82793 |  0:00:52s\n",
      "epoch 22 | loss: 0.43589 | val_0_auc: 0.83112 |  0:00:54s\n",
      "epoch 23 | loss: 0.42904 | val_0_auc: 0.83379 |  0:00:56s\n",
      "epoch 24 | loss: 0.42505 | val_0_auc: 0.83421 |  0:00:58s\n",
      "epoch 25 | loss: 0.41909 | val_0_auc: 0.8377  |  0:01:00s\n",
      "epoch 26 | loss: 0.41772 | val_0_auc: 0.84025 |  0:01:02s\n",
      "epoch 27 | loss: 0.41145 | val_0_auc: 0.83736 |  0:01:04s\n",
      "epoch 28 | loss: 0.40988 | val_0_auc: 0.83952 |  0:01:06s\n",
      "epoch 29 | loss: 0.4088  | val_0_auc: 0.84157 |  0:01:07s\n",
      "epoch 30 | loss: 0.40606 | val_0_auc: 0.83983 |  0:01:09s\n",
      "epoch 31 | loss: 0.40008 | val_0_auc: 0.84258 |  0:01:11s\n",
      "epoch 32 | loss: 0.39977 | val_0_auc: 0.84255 |  0:01:13s\n",
      "epoch 33 | loss: 0.39544 | val_0_auc: 0.84797 |  0:01:15s\n",
      "epoch 34 | loss: 0.39386 | val_0_auc: 0.84951 |  0:01:17s\n",
      "epoch 35 | loss: 0.3919  | val_0_auc: 0.85057 |  0:01:20s\n",
      "epoch 36 | loss: 0.38868 | val_0_auc: 0.85163 |  0:01:23s\n",
      "epoch 37 | loss: 0.38825 | val_0_auc: 0.85108 |  0:01:25s\n",
      "epoch 38 | loss: 0.38736 | val_0_auc: 0.85296 |  0:01:27s\n",
      "epoch 39 | loss: 0.38472 | val_0_auc: 0.85351 |  0:01:29s\n",
      "epoch 40 | loss: 0.38086 | val_0_auc: 0.85493 |  0:01:31s\n",
      "epoch 41 | loss: 0.38439 | val_0_auc: 0.85726 |  0:01:33s\n",
      "epoch 42 | loss: 0.37668 | val_0_auc: 0.85707 |  0:01:36s\n",
      "epoch 43 | loss: 0.37838 | val_0_auc: 0.85597 |  0:01:39s\n",
      "epoch 44 | loss: 0.37614 | val_0_auc: 0.85703 |  0:01:41s\n",
      "epoch 45 | loss: 0.37529 | val_0_auc: 0.8601  |  0:01:43s\n",
      "epoch 46 | loss: 0.37076 | val_0_auc: 0.86044 |  0:01:44s\n",
      "epoch 47 | loss: 0.3711  | val_0_auc: 0.86197 |  0:01:46s\n",
      "epoch 48 | loss: 0.36948 | val_0_auc: 0.86225 |  0:01:48s\n",
      "epoch 49 | loss: 0.37166 | val_0_auc: 0.86253 |  0:01:51s\n",
      "epoch 50 | loss: 0.36808 | val_0_auc: 0.86371 |  0:01:53s\n",
      "epoch 51 | loss: 0.3678  | val_0_auc: 0.86445 |  0:01:56s\n",
      "epoch 52 | loss: 0.36497 | val_0_auc: 0.86473 |  0:01:58s\n",
      "epoch 53 | loss: 0.36356 | val_0_auc: 0.86446 |  0:02:00s\n",
      "epoch 54 | loss: 0.3638  | val_0_auc: 0.86537 |  0:02:02s\n",
      "epoch 55 | loss: 0.36284 | val_0_auc: 0.86603 |  0:02:05s\n",
      "epoch 56 | loss: 0.36205 | val_0_auc: 0.86805 |  0:02:07s\n",
      "epoch 57 | loss: 0.35989 | val_0_auc: 0.86665 |  0:02:09s\n",
      "epoch 58 | loss: 0.35864 | val_0_auc: 0.86659 |  0:02:11s\n",
      "epoch 59 | loss: 0.35682 | val_0_auc: 0.86736 |  0:02:13s\n",
      "epoch 60 | loss: 0.35677 | val_0_auc: 0.8692  |  0:02:15s\n",
      "epoch 61 | loss: 0.35548 | val_0_auc: 0.87044 |  0:02:17s\n",
      "epoch 62 | loss: 0.35619 | val_0_auc: 0.87126 |  0:02:19s\n",
      "epoch 63 | loss: 0.35475 | val_0_auc: 0.87203 |  0:02:20s\n",
      "epoch 64 | loss: 0.35409 | val_0_auc: 0.87234 |  0:02:23s\n",
      "epoch 65 | loss: 0.35099 | val_0_auc: 0.87311 |  0:02:25s\n",
      "epoch 66 | loss: 0.35279 | val_0_auc: 0.87244 |  0:02:26s\n",
      "epoch 67 | loss: 0.34935 | val_0_auc: 0.8742  |  0:02:28s\n",
      "epoch 68 | loss: 0.34865 | val_0_auc: 0.87516 |  0:02:30s\n",
      "epoch 69 | loss: 0.34915 | val_0_auc: 0.87564 |  0:02:32s\n",
      "epoch 70 | loss: 0.34707 | val_0_auc: 0.87609 |  0:02:35s\n",
      "epoch 71 | loss: 0.34563 | val_0_auc: 0.87692 |  0:02:37s\n",
      "epoch 72 | loss: 0.34367 | val_0_auc: 0.87727 |  0:02:38s\n",
      "epoch 73 | loss: 0.34324 | val_0_auc: 0.87677 |  0:02:40s\n",
      "epoch 74 | loss: 0.34064 | val_0_auc: 0.87733 |  0:02:42s\n",
      "epoch 75 | loss: 0.34435 | val_0_auc: 0.87765 |  0:02:46s\n",
      "epoch 76 | loss: 0.34354 | val_0_auc: 0.87718 |  0:02:48s\n",
      "epoch 77 | loss: 0.3433  | val_0_auc: 0.87807 |  0:02:50s\n",
      "epoch 78 | loss: 0.33989 | val_0_auc: 0.87812 |  0:02:52s\n",
      "epoch 79 | loss: 0.33915 | val_0_auc: 0.87822 |  0:02:54s\n",
      "epoch 80 | loss: 0.3381  | val_0_auc: 0.87921 |  0:02:56s\n",
      "epoch 81 | loss: 0.33833 | val_0_auc: 0.87903 |  0:02:58s\n",
      "epoch 82 | loss: 0.34072 | val_0_auc: 0.87991 |  0:03:00s\n",
      "epoch 83 | loss: 0.33918 | val_0_auc: 0.88038 |  0:03:02s\n",
      "epoch 84 | loss: 0.33471 | val_0_auc: 0.87994 |  0:03:04s\n",
      "epoch 85 | loss: 0.33562 | val_0_auc: 0.87972 |  0:03:06s\n",
      "epoch 86 | loss: 0.33453 | val_0_auc: 0.87964 |  0:03:08s\n",
      "epoch 87 | loss: 0.33516 | val_0_auc: 0.88073 |  0:03:10s\n",
      "epoch 88 | loss: 0.33292 | val_0_auc: 0.88129 |  0:03:12s\n",
      "epoch 89 | loss: 0.33199 | val_0_auc: 0.8809  |  0:03:14s\n",
      "epoch 90 | loss: 0.33178 | val_0_auc: 0.88197 |  0:03:15s\n",
      "epoch 91 | loss: 0.33277 | val_0_auc: 0.88285 |  0:03:17s\n",
      "epoch 92 | loss: 0.32958 | val_0_auc: 0.88378 |  0:03:19s\n",
      "epoch 93 | loss: 0.33192 | val_0_auc: 0.88329 |  0:03:22s\n",
      "epoch 94 | loss: 0.33103 | val_0_auc: 0.88392 |  0:03:23s\n",
      "epoch 95 | loss: 0.32869 | val_0_auc: 0.88346 |  0:03:25s\n",
      "epoch 96 | loss: 0.32896 | val_0_auc: 0.88308 |  0:03:27s\n",
      "epoch 97 | loss: 0.32721 | val_0_auc: 0.88298 |  0:03:29s\n",
      "epoch 98 | loss: 0.32612 | val_0_auc: 0.88394 |  0:03:31s\n",
      "epoch 99 | loss: 0.32776 | val_0_auc: 0.88423 |  0:03:33s\n",
      "epoch 100| loss: 0.32417 | val_0_auc: 0.88444 |  0:03:34s\n",
      "epoch 101| loss: 0.32658 | val_0_auc: 0.88407 |  0:03:36s\n",
      "epoch 102| loss: 0.32356 | val_0_auc: 0.8842  |  0:03:38s\n",
      "epoch 103| loss: 0.325   | val_0_auc: 0.8843  |  0:03:41s\n",
      "epoch 104| loss: 0.32414 | val_0_auc: 0.88556 |  0:03:42s\n",
      "epoch 105| loss: 0.32226 | val_0_auc: 0.88558 |  0:03:44s\n",
      "epoch 106| loss: 0.32109 | val_0_auc: 0.88545 |  0:03:46s\n",
      "epoch 107| loss: 0.32004 | val_0_auc: 0.8849  |  0:03:48s\n",
      "epoch 108| loss: 0.32069 | val_0_auc: 0.88489 |  0:03:50s\n",
      "epoch 109| loss: 0.31982 | val_0_auc: 0.88545 |  0:03:52s\n",
      "epoch 110| loss: 0.31847 | val_0_auc: 0.88596 |  0:03:54s\n",
      "epoch 111| loss: 0.31901 | val_0_auc: 0.88588 |  0:03:56s\n",
      "epoch 112| loss: 0.31844 | val_0_auc: 0.88623 |  0:03:58s\n",
      "epoch 113| loss: 0.31652 | val_0_auc: 0.88688 |  0:03:59s\n",
      "epoch 114| loss: 0.31581 | val_0_auc: 0.88649 |  0:04:01s\n",
      "epoch 115| loss: 0.32085 | val_0_auc: 0.88663 |  0:04:05s\n",
      "epoch 116| loss: 0.3163  | val_0_auc: 0.88691 |  0:04:07s\n",
      "epoch 117| loss: 0.31626 | val_0_auc: 0.88726 |  0:04:09s\n",
      "epoch 118| loss: 0.31371 | val_0_auc: 0.88731 |  0:04:11s\n",
      "epoch 119| loss: 0.31653 | val_0_auc: 0.88728 |  0:04:13s\n",
      "epoch 120| loss: 0.31673 | val_0_auc: 0.8872  |  0:04:15s\n",
      "epoch 121| loss: 0.31078 | val_0_auc: 0.88627 |  0:04:17s\n",
      "epoch 122| loss: 0.31294 | val_0_auc: 0.88686 |  0:04:19s\n",
      "epoch 123| loss: 0.31351 | val_0_auc: 0.88687 |  0:04:21s\n",
      "epoch 124| loss: 0.31044 | val_0_auc: 0.88741 |  0:04:24s\n",
      "epoch 125| loss: 0.31175 | val_0_auc: 0.88726 |  0:04:26s\n",
      "epoch 126| loss: 0.31055 | val_0_auc: 0.88736 |  0:04:28s\n",
      "epoch 127| loss: 0.31024 | val_0_auc: 0.88763 |  0:04:30s\n",
      "epoch 128| loss: 0.31033 | val_0_auc: 0.88785 |  0:04:32s\n",
      "epoch 129| loss: 0.30833 | val_0_auc: 0.88832 |  0:04:34s\n",
      "epoch 130| loss: 0.31056 | val_0_auc: 0.88819 |  0:04:36s\n",
      "epoch 131| loss: 0.30641 | val_0_auc: 0.88831 |  0:04:39s\n",
      "epoch 132| loss: 0.30918 | val_0_auc: 0.88874 |  0:04:41s\n",
      "epoch 133| loss: 0.30551 | val_0_auc: 0.88963 |  0:04:43s\n",
      "epoch 134| loss: 0.30457 | val_0_auc: 0.88943 |  0:04:45s\n",
      "epoch 135| loss: 0.30466 | val_0_auc: 0.8896  |  0:04:47s\n",
      "epoch 136| loss: 0.30597 | val_0_auc: 0.88866 |  0:04:48s\n",
      "epoch 137| loss: 0.30407 | val_0_auc: 0.88875 |  0:04:50s\n",
      "epoch 138| loss: 0.30621 | val_0_auc: 0.88939 |  0:04:52s\n",
      "epoch 139| loss: 0.30249 | val_0_auc: 0.8898  |  0:04:54s\n",
      "epoch 140| loss: 0.30383 | val_0_auc: 0.88996 |  0:04:56s\n",
      "epoch 141| loss: 0.30321 | val_0_auc: 0.88925 |  0:04:57s\n",
      "epoch 142| loss: 0.302   | val_0_auc: 0.889   |  0:04:59s\n",
      "epoch 143| loss: 0.30473 | val_0_auc: 0.88971 |  0:05:01s\n",
      "epoch 144| loss: 0.30229 | val_0_auc: 0.89016 |  0:05:03s\n",
      "epoch 145| loss: 0.29975 | val_0_auc: 0.89038 |  0:05:05s\n",
      "epoch 146| loss: 0.30205 | val_0_auc: 0.89102 |  0:05:06s\n",
      "epoch 147| loss: 0.29856 | val_0_auc: 0.89129 |  0:05:08s\n",
      "epoch 148| loss: 0.29922 | val_0_auc: 0.89064 |  0:05:10s\n",
      "epoch 149| loss: 0.30124 | val_0_auc: 0.89102 |  0:05:12s\n",
      "epoch 150| loss: 0.29942 | val_0_auc: 0.89139 |  0:05:14s\n",
      "epoch 151| loss: 0.30152 | val_0_auc: 0.89214 |  0:05:16s\n",
      "epoch 152| loss: 0.29857 | val_0_auc: 0.89127 |  0:05:18s\n",
      "epoch 153| loss: 0.29762 | val_0_auc: 0.89109 |  0:05:20s\n",
      "epoch 154| loss: 0.2964  | val_0_auc: 0.89148 |  0:05:22s\n",
      "epoch 155| loss: 0.29642 | val_0_auc: 0.89171 |  0:05:24s\n",
      "epoch 156| loss: 0.2975  | val_0_auc: 0.89162 |  0:05:26s\n",
      "epoch 157| loss: 0.29417 | val_0_auc: 0.89249 |  0:05:28s\n",
      "epoch 158| loss: 0.29544 | val_0_auc: 0.89246 |  0:05:30s\n",
      "epoch 159| loss: 0.29505 | val_0_auc: 0.8923  |  0:05:32s\n",
      "epoch 160| loss: 0.29526 | val_0_auc: 0.89302 |  0:05:34s\n",
      "epoch 161| loss: 0.29531 | val_0_auc: 0.89236 |  0:05:35s\n",
      "epoch 162| loss: 0.29196 | val_0_auc: 0.89304 |  0:05:37s\n",
      "epoch 163| loss: 0.29537 | val_0_auc: 0.89354 |  0:05:39s\n",
      "epoch 164| loss: 0.29233 | val_0_auc: 0.89344 |  0:05:41s\n",
      "epoch 165| loss: 0.2943  | val_0_auc: 0.89362 |  0:05:43s\n",
      "epoch 166| loss: 0.29119 | val_0_auc: 0.89316 |  0:05:45s\n",
      "epoch 167| loss: 0.29134 | val_0_auc: 0.89333 |  0:05:47s\n",
      "epoch 168| loss: 0.28854 | val_0_auc: 0.89431 |  0:05:49s\n",
      "epoch 169| loss: 0.29023 | val_0_auc: 0.89372 |  0:05:51s\n",
      "epoch 170| loss: 0.29101 | val_0_auc: 0.8943  |  0:05:52s\n",
      "epoch 171| loss: 0.29007 | val_0_auc: 0.89409 |  0:05:54s\n",
      "epoch 172| loss: 0.2904  | val_0_auc: 0.89398 |  0:05:56s\n",
      "epoch 173| loss: 0.28793 | val_0_auc: 0.89471 |  0:05:58s\n",
      "epoch 174| loss: 0.28477 | val_0_auc: 0.89466 |  0:06:00s\n",
      "epoch 175| loss: 0.28498 | val_0_auc: 0.89423 |  0:06:02s\n",
      "epoch 176| loss: 0.28522 | val_0_auc: 0.89439 |  0:06:04s\n",
      "epoch 177| loss: 0.28653 | val_0_auc: 0.89419 |  0:06:06s\n",
      "epoch 178| loss: 0.28412 | val_0_auc: 0.89366 |  0:06:08s\n",
      "epoch 179| loss: 0.28576 | val_0_auc: 0.89409 |  0:06:10s\n",
      "epoch 180| loss: 0.28617 | val_0_auc: 0.89355 |  0:06:12s\n",
      "epoch 181| loss: 0.28404 | val_0_auc: 0.89331 |  0:06:14s\n",
      "epoch 182| loss: 0.28585 | val_0_auc: 0.89306 |  0:06:19s\n",
      "epoch 183| loss: 0.28194 | val_0_auc: 0.89292 |  0:06:21s\n",
      "epoch 184| loss: 0.28467 | val_0_auc: 0.89375 |  0:06:23s\n",
      "epoch 185| loss: 0.28192 | val_0_auc: 0.89395 |  0:06:25s\n",
      "epoch 186| loss: 0.28321 | val_0_auc: 0.89288 |  0:06:26s\n",
      "epoch 187| loss: 0.28286 | val_0_auc: 0.89348 |  0:06:28s\n",
      "epoch 188| loss: 0.28337 | val_0_auc: 0.89331 |  0:06:30s\n",
      "epoch 189| loss: 0.2806  | val_0_auc: 0.89331 |  0:06:32s\n",
      "epoch 190| loss: 0.28005 | val_0_auc: 0.89325 |  0:06:35s\n",
      "epoch 191| loss: 0.28075 | val_0_auc: 0.89281 |  0:06:36s\n",
      "epoch 192| loss: 0.28163 | val_0_auc: 0.89306 |  0:06:38s\n",
      "epoch 193| loss: 0.28265 | val_0_auc: 0.89237 |  0:06:40s\n",
      "\n",
      "Early stopping occurred at epoch 193 with best_epoch = 173 and best_val_0_auc = 0.89471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64202 | val_0_auc: 0.53976 |  0:00:02s\n",
      "epoch 1  | loss: 0.62099 | val_0_auc: 0.57955 |  0:00:04s\n",
      "epoch 2  | loss: 0.59704 | val_0_auc: 0.61298 |  0:00:06s\n",
      "epoch 3  | loss: 0.58163 | val_0_auc: 0.6607  |  0:00:09s\n",
      "epoch 4  | loss: 0.56513 | val_0_auc: 0.68593 |  0:00:11s\n",
      "epoch 5  | loss: 0.55039 | val_0_auc: 0.7062  |  0:00:14s\n",
      "epoch 6  | loss: 0.53824 | val_0_auc: 0.72889 |  0:00:17s\n",
      "epoch 7  | loss: 0.52364 | val_0_auc: 0.73713 |  0:00:20s\n",
      "epoch 8  | loss: 0.51039 | val_0_auc: 0.74325 |  0:00:22s\n",
      "epoch 9  | loss: 0.50604 | val_0_auc: 0.75177 |  0:00:25s\n",
      "epoch 10 | loss: 0.49439 | val_0_auc: 0.75669 |  0:00:27s\n",
      "epoch 11 | loss: 0.49231 | val_0_auc: 0.75286 |  0:00:30s\n",
      "epoch 12 | loss: 0.48878 | val_0_auc: 0.76012 |  0:00:32s\n",
      "epoch 13 | loss: 0.48311 | val_0_auc: 0.76059 |  0:00:35s\n",
      "epoch 14 | loss: 0.47832 | val_0_auc: 0.76173 |  0:00:38s\n",
      "epoch 15 | loss: 0.47714 | val_0_auc: 0.7666  |  0:00:41s\n",
      "epoch 16 | loss: 0.47555 | val_0_auc: 0.75901 |  0:00:43s\n",
      "epoch 17 | loss: 0.4737  | val_0_auc: 0.76423 |  0:00:46s\n",
      "epoch 18 | loss: 0.46996 | val_0_auc: 0.76885 |  0:00:49s\n",
      "epoch 19 | loss: 0.46846 | val_0_auc: 0.77461 |  0:00:51s\n",
      "epoch 20 | loss: 0.467   | val_0_auc: 0.77406 |  0:00:54s\n",
      "epoch 21 | loss: 0.46858 | val_0_auc: 0.77246 |  0:00:56s\n",
      "epoch 22 | loss: 0.46764 | val_0_auc: 0.77215 |  0:00:58s\n",
      "epoch 23 | loss: 0.46235 | val_0_auc: 0.77229 |  0:01:00s\n",
      "epoch 24 | loss: 0.458   | val_0_auc: 0.7796  |  0:01:03s\n",
      "epoch 25 | loss: 0.4585  | val_0_auc: 0.78344 |  0:01:05s\n",
      "epoch 26 | loss: 0.45715 | val_0_auc: 0.78315 |  0:01:07s\n",
      "epoch 27 | loss: 0.45409 | val_0_auc: 0.77899 |  0:01:09s\n",
      "epoch 28 | loss: 0.45374 | val_0_auc: 0.78209 |  0:01:11s\n",
      "epoch 29 | loss: 0.45405 | val_0_auc: 0.78464 |  0:01:14s\n",
      "epoch 30 | loss: 0.45172 | val_0_auc: 0.78895 |  0:01:16s\n",
      "epoch 31 | loss: 0.45265 | val_0_auc: 0.7888  |  0:01:19s\n",
      "epoch 32 | loss: 0.45073 | val_0_auc: 0.79449 |  0:01:21s\n",
      "epoch 33 | loss: 0.44909 | val_0_auc: 0.79676 |  0:01:23s\n",
      "epoch 34 | loss: 0.44774 | val_0_auc: 0.79344 |  0:01:26s\n",
      "epoch 35 | loss: 0.44719 | val_0_auc: 0.79402 |  0:01:28s\n",
      "epoch 36 | loss: 0.44629 | val_0_auc: 0.79707 |  0:01:31s\n",
      "epoch 37 | loss: 0.44365 | val_0_auc: 0.80304 |  0:01:33s\n",
      "epoch 38 | loss: 0.44487 | val_0_auc: 0.80212 |  0:01:35s\n",
      "epoch 39 | loss: 0.44153 | val_0_auc: 0.802   |  0:01:38s\n",
      "epoch 40 | loss: 0.43949 | val_0_auc: 0.80393 |  0:01:40s\n",
      "epoch 41 | loss: 0.44055 | val_0_auc: 0.80382 |  0:01:43s\n",
      "epoch 42 | loss: 0.4361  | val_0_auc: 0.80353 |  0:01:45s\n",
      "epoch 43 | loss: 0.43708 | val_0_auc: 0.80632 |  0:01:47s\n",
      "epoch 44 | loss: 0.43713 | val_0_auc: 0.80392 |  0:01:49s\n",
      "epoch 45 | loss: 0.43818 | val_0_auc: 0.80866 |  0:01:52s\n",
      "epoch 46 | loss: 0.43385 | val_0_auc: 0.81002 |  0:01:54s\n",
      "epoch 47 | loss: 0.43223 | val_0_auc: 0.80835 |  0:01:56s\n",
      "epoch 48 | loss: 0.4324  | val_0_auc: 0.81514 |  0:01:59s\n",
      "epoch 49 | loss: 0.43319 | val_0_auc: 0.81616 |  0:02:01s\n",
      "epoch 50 | loss: 0.43144 | val_0_auc: 0.815   |  0:02:03s\n",
      "epoch 51 | loss: 0.42776 | val_0_auc: 0.81692 |  0:02:06s\n",
      "epoch 52 | loss: 0.42614 | val_0_auc: 0.81621 |  0:02:08s\n",
      "epoch 53 | loss: 0.42509 | val_0_auc: 0.81836 |  0:02:11s\n",
      "epoch 54 | loss: 0.42574 | val_0_auc: 0.82151 |  0:02:13s\n",
      "epoch 55 | loss: 0.42342 | val_0_auc: 0.81728 |  0:02:15s\n",
      "epoch 56 | loss: 0.42162 | val_0_auc: 0.82385 |  0:02:17s\n",
      "epoch 57 | loss: 0.42179 | val_0_auc: 0.82481 |  0:02:19s\n",
      "epoch 58 | loss: 0.41879 | val_0_auc: 0.82673 |  0:02:22s\n",
      "epoch 59 | loss: 0.41762 | val_0_auc: 0.82897 |  0:02:24s\n",
      "epoch 60 | loss: 0.41343 | val_0_auc: 0.83147 |  0:02:26s\n",
      "epoch 61 | loss: 0.41418 | val_0_auc: 0.83056 |  0:02:28s\n",
      "epoch 62 | loss: 0.41468 | val_0_auc: 0.83085 |  0:02:31s\n",
      "epoch 63 | loss: 0.4145  | val_0_auc: 0.83211 |  0:02:33s\n",
      "epoch 64 | loss: 0.40874 | val_0_auc: 0.83278 |  0:02:35s\n",
      "epoch 65 | loss: 0.41157 | val_0_auc: 0.82943 |  0:02:37s\n",
      "epoch 66 | loss: 0.41299 | val_0_auc: 0.835   |  0:02:40s\n",
      "epoch 67 | loss: 0.40633 | val_0_auc: 0.83579 |  0:02:42s\n",
      "epoch 68 | loss: 0.4078  | val_0_auc: 0.8347  |  0:02:44s\n",
      "epoch 69 | loss: 0.40909 | val_0_auc: 0.8354  |  0:02:46s\n",
      "epoch 70 | loss: 0.40805 | val_0_auc: 0.83542 |  0:02:49s\n",
      "epoch 71 | loss: 0.40521 | val_0_auc: 0.83543 |  0:02:51s\n",
      "epoch 72 | loss: 0.40566 | val_0_auc: 0.83681 |  0:02:53s\n",
      "epoch 73 | loss: 0.40594 | val_0_auc: 0.83708 |  0:02:55s\n",
      "epoch 74 | loss: 0.40479 | val_0_auc: 0.83808 |  0:02:58s\n",
      "epoch 75 | loss: 0.40326 | val_0_auc: 0.83971 |  0:03:00s\n",
      "epoch 76 | loss: 0.40263 | val_0_auc: 0.84113 |  0:03:02s\n",
      "epoch 77 | loss: 0.4017  | val_0_auc: 0.84082 |  0:03:04s\n",
      "epoch 78 | loss: 0.40268 | val_0_auc: 0.8407  |  0:03:07s\n",
      "epoch 79 | loss: 0.39883 | val_0_auc: 0.84386 |  0:03:09s\n",
      "epoch 80 | loss: 0.40037 | val_0_auc: 0.84518 |  0:03:11s\n",
      "epoch 81 | loss: 0.39746 | val_0_auc: 0.84438 |  0:03:14s\n",
      "epoch 82 | loss: 0.39377 | val_0_auc: 0.84658 |  0:03:16s\n",
      "epoch 83 | loss: 0.39254 | val_0_auc: 0.84761 |  0:03:18s\n",
      "epoch 84 | loss: 0.39578 | val_0_auc: 0.84738 |  0:03:20s\n",
      "epoch 85 | loss: 0.39215 | val_0_auc: 0.84836 |  0:03:23s\n",
      "epoch 86 | loss: 0.39406 | val_0_auc: 0.84978 |  0:03:25s\n",
      "epoch 87 | loss: 0.39384 | val_0_auc: 0.84785 |  0:03:27s\n",
      "epoch 88 | loss: 0.39225 | val_0_auc: 0.8487  |  0:03:29s\n",
      "epoch 89 | loss: 0.38764 | val_0_auc: 0.84831 |  0:03:32s\n",
      "epoch 90 | loss: 0.39096 | val_0_auc: 0.84814 |  0:03:34s\n",
      "epoch 91 | loss: 0.38839 | val_0_auc: 0.85237 |  0:03:36s\n",
      "epoch 92 | loss: 0.38974 | val_0_auc: 0.85009 |  0:03:38s\n",
      "epoch 93 | loss: 0.38802 | val_0_auc: 0.84978 |  0:03:41s\n",
      "epoch 94 | loss: 0.38886 | val_0_auc: 0.85008 |  0:03:43s\n",
      "epoch 95 | loss: 0.38906 | val_0_auc: 0.85196 |  0:03:45s\n",
      "epoch 96 | loss: 0.3898  | val_0_auc: 0.85209 |  0:03:47s\n",
      "epoch 97 | loss: 0.3874  | val_0_auc: 0.85197 |  0:03:49s\n",
      "epoch 98 | loss: 0.38585 | val_0_auc: 0.85226 |  0:03:52s\n",
      "epoch 99 | loss: 0.38879 | val_0_auc: 0.85211 |  0:03:54s\n",
      "epoch 100| loss: 0.38757 | val_0_auc: 0.85314 |  0:03:56s\n",
      "epoch 101| loss: 0.38593 | val_0_auc: 0.85278 |  0:03:58s\n",
      "epoch 102| loss: 0.38553 | val_0_auc: 0.85158 |  0:04:01s\n",
      "epoch 103| loss: 0.38773 | val_0_auc: 0.85155 |  0:04:03s\n",
      "epoch 104| loss: 0.38466 | val_0_auc: 0.85134 |  0:04:05s\n",
      "epoch 105| loss: 0.38644 | val_0_auc: 0.85161 |  0:04:07s\n",
      "epoch 106| loss: 0.3834  | val_0_auc: 0.85196 |  0:04:09s\n",
      "epoch 107| loss: 0.38553 | val_0_auc: 0.85189 |  0:04:12s\n",
      "epoch 108| loss: 0.38438 | val_0_auc: 0.85133 |  0:04:14s\n",
      "epoch 109| loss: 0.38293 | val_0_auc: 0.85304 |  0:04:16s\n",
      "epoch 110| loss: 0.38369 | val_0_auc: 0.85351 |  0:04:18s\n",
      "epoch 111| loss: 0.38281 | val_0_auc: 0.85307 |  0:04:20s\n",
      "epoch 112| loss: 0.38208 | val_0_auc: 0.85497 |  0:04:23s\n",
      "epoch 113| loss: 0.38202 | val_0_auc: 0.85615 |  0:04:25s\n",
      "epoch 114| loss: 0.38138 | val_0_auc: 0.85375 |  0:04:27s\n",
      "epoch 115| loss: 0.38058 | val_0_auc: 0.85346 |  0:04:29s\n",
      "epoch 116| loss: 0.3772  | val_0_auc: 0.85384 |  0:04:31s\n",
      "epoch 117| loss: 0.37852 | val_0_auc: 0.85466 |  0:04:34s\n",
      "epoch 118| loss: 0.38172 | val_0_auc: 0.85324 |  0:04:36s\n",
      "epoch 119| loss: 0.3819  | val_0_auc: 0.85428 |  0:04:38s\n",
      "epoch 120| loss: 0.37876 | val_0_auc: 0.85662 |  0:04:40s\n",
      "epoch 121| loss: 0.37684 | val_0_auc: 0.85636 |  0:04:42s\n",
      "epoch 122| loss: 0.37668 | val_0_auc: 0.85579 |  0:04:44s\n",
      "epoch 123| loss: 0.377   | val_0_auc: 0.85756 |  0:04:47s\n",
      "epoch 124| loss: 0.37531 | val_0_auc: 0.85756 |  0:04:49s\n",
      "epoch 125| loss: 0.37451 | val_0_auc: 0.85802 |  0:04:51s\n",
      "epoch 126| loss: 0.37272 | val_0_auc: 0.85856 |  0:04:53s\n",
      "epoch 127| loss: 0.37163 | val_0_auc: 0.85895 |  0:04:55s\n",
      "epoch 128| loss: 0.37182 | val_0_auc: 0.85883 |  0:04:58s\n",
      "epoch 129| loss: 0.37193 | val_0_auc: 0.85827 |  0:05:00s\n",
      "epoch 130| loss: 0.37254 | val_0_auc: 0.85846 |  0:05:02s\n",
      "epoch 131| loss: 0.37058 | val_0_auc: 0.85833 |  0:05:04s\n",
      "epoch 132| loss: 0.37116 | val_0_auc: 0.85926 |  0:05:07s\n",
      "epoch 133| loss: 0.37064 | val_0_auc: 0.86039 |  0:05:09s\n",
      "epoch 134| loss: 0.37069 | val_0_auc: 0.86117 |  0:05:11s\n",
      "epoch 135| loss: 0.36634 | val_0_auc: 0.85937 |  0:05:13s\n",
      "epoch 136| loss: 0.36896 | val_0_auc: 0.85969 |  0:05:15s\n",
      "epoch 137| loss: 0.36645 | val_0_auc: 0.85915 |  0:05:18s\n",
      "epoch 138| loss: 0.3686  | val_0_auc: 0.8592  |  0:05:20s\n",
      "epoch 139| loss: 0.36799 | val_0_auc: 0.8605  |  0:05:22s\n",
      "epoch 140| loss: 0.36421 | val_0_auc: 0.86126 |  0:05:24s\n",
      "epoch 141| loss: 0.36749 | val_0_auc: 0.8628  |  0:05:26s\n",
      "epoch 142| loss: 0.36583 | val_0_auc: 0.86079 |  0:05:29s\n",
      "epoch 143| loss: 0.36579 | val_0_auc: 0.86117 |  0:05:31s\n",
      "epoch 144| loss: 0.36585 | val_0_auc: 0.86097 |  0:05:33s\n",
      "epoch 145| loss: 0.36694 | val_0_auc: 0.86273 |  0:05:36s\n",
      "epoch 146| loss: 0.36236 | val_0_auc: 0.86269 |  0:05:38s\n",
      "epoch 147| loss: 0.36474 | val_0_auc: 0.86148 |  0:05:40s\n",
      "epoch 148| loss: 0.36566 | val_0_auc: 0.86244 |  0:05:42s\n",
      "epoch 149| loss: 0.36234 | val_0_auc: 0.86308 |  0:05:44s\n",
      "epoch 150| loss: 0.36015 | val_0_auc: 0.86315 |  0:05:47s\n",
      "epoch 151| loss: 0.36178 | val_0_auc: 0.86367 |  0:05:49s\n",
      "epoch 152| loss: 0.36312 | val_0_auc: 0.86267 |  0:05:51s\n",
      "epoch 153| loss: 0.36034 | val_0_auc: 0.8629  |  0:05:55s\n",
      "epoch 154| loss: 0.3621  | val_0_auc: 0.86168 |  0:05:57s\n",
      "epoch 155| loss: 0.35918 | val_0_auc: 0.86508 |  0:05:59s\n",
      "epoch 156| loss: 0.35861 | val_0_auc: 0.8656  |  0:06:01s\n",
      "epoch 157| loss: 0.35924 | val_0_auc: 0.86694 |  0:06:04s\n",
      "epoch 158| loss: 0.35776 | val_0_auc: 0.86434 |  0:06:06s\n",
      "epoch 159| loss: 0.35863 | val_0_auc: 0.86497 |  0:06:08s\n",
      "epoch 160| loss: 0.35631 | val_0_auc: 0.86434 |  0:06:10s\n",
      "epoch 161| loss: 0.3568  | val_0_auc: 0.86612 |  0:06:12s\n",
      "epoch 162| loss: 0.35455 | val_0_auc: 0.86742 |  0:06:15s\n",
      "epoch 163| loss: 0.35661 | val_0_auc: 0.86718 |  0:06:17s\n",
      "epoch 164| loss: 0.35623 | val_0_auc: 0.86888 |  0:06:20s\n",
      "epoch 165| loss: 0.35689 | val_0_auc: 0.86799 |  0:06:22s\n",
      "epoch 166| loss: 0.35544 | val_0_auc: 0.86819 |  0:06:24s\n",
      "epoch 167| loss: 0.35573 | val_0_auc: 0.86809 |  0:06:26s\n",
      "epoch 168| loss: 0.35553 | val_0_auc: 0.8693  |  0:06:28s\n",
      "epoch 169| loss: 0.35148 | val_0_auc: 0.86901 |  0:06:31s\n",
      "epoch 170| loss: 0.35337 | val_0_auc: 0.86891 |  0:06:33s\n",
      "epoch 171| loss: 0.35273 | val_0_auc: 0.8684  |  0:06:35s\n",
      "epoch 172| loss: 0.35372 | val_0_auc: 0.86737 |  0:06:37s\n",
      "epoch 173| loss: 0.35349 | val_0_auc: 0.86581 |  0:06:39s\n",
      "epoch 174| loss: 0.35138 | val_0_auc: 0.86876 |  0:06:42s\n",
      "epoch 175| loss: 0.35086 | val_0_auc: 0.8681  |  0:06:44s\n",
      "epoch 176| loss: 0.35101 | val_0_auc: 0.86913 |  0:06:46s\n",
      "epoch 177| loss: 0.35121 | val_0_auc: 0.8687  |  0:06:48s\n",
      "epoch 178| loss: 0.35038 | val_0_auc: 0.86914 |  0:06:51s\n",
      "epoch 179| loss: 0.35226 | val_0_auc: 0.87024 |  0:06:53s\n",
      "epoch 180| loss: 0.34919 | val_0_auc: 0.8711  |  0:06:55s\n",
      "epoch 181| loss: 0.34907 | val_0_auc: 0.87053 |  0:06:57s\n",
      "epoch 182| loss: 0.34851 | val_0_auc: 0.87027 |  0:07:00s\n",
      "epoch 183| loss: 0.34616 | val_0_auc: 0.87009 |  0:07:02s\n",
      "epoch 184| loss: 0.3475  | val_0_auc: 0.87027 |  0:07:04s\n",
      "epoch 185| loss: 0.34677 | val_0_auc: 0.87213 |  0:07:06s\n",
      "epoch 186| loss: 0.34765 | val_0_auc: 0.87023 |  0:07:08s\n",
      "epoch 187| loss: 0.34571 | val_0_auc: 0.87128 |  0:07:11s\n",
      "epoch 188| loss: 0.34999 | val_0_auc: 0.87287 |  0:07:13s\n",
      "epoch 189| loss: 0.34506 | val_0_auc: 0.87314 |  0:07:15s\n",
      "epoch 190| loss: 0.34276 | val_0_auc: 0.87293 |  0:07:17s\n",
      "epoch 191| loss: 0.34228 | val_0_auc: 0.8728  |  0:07:19s\n",
      "epoch 192| loss: 0.34399 | val_0_auc: 0.87351 |  0:07:22s\n",
      "epoch 193| loss: 0.34303 | val_0_auc: 0.87439 |  0:07:24s\n",
      "epoch 194| loss: 0.34224 | val_0_auc: 0.87553 |  0:07:26s\n",
      "epoch 195| loss: 0.34345 | val_0_auc: 0.87436 |  0:07:28s\n",
      "epoch 196| loss: 0.34165 | val_0_auc: 0.87364 |  0:07:30s\n",
      "epoch 197| loss: 0.34609 | val_0_auc: 0.87498 |  0:07:33s\n",
      "epoch 198| loss: 0.34366 | val_0_auc: 0.876   |  0:07:35s\n",
      "epoch 199| loss: 0.34224 | val_0_auc: 0.87535 |  0:07:37s\n",
      "epoch 200| loss: 0.34314 | val_0_auc: 0.87687 |  0:07:39s\n",
      "epoch 201| loss: 0.34113 | val_0_auc: 0.877   |  0:07:41s\n",
      "epoch 202| loss: 0.34259 | val_0_auc: 0.87692 |  0:07:43s\n",
      "epoch 203| loss: 0.34241 | val_0_auc: 0.87706 |  0:07:46s\n",
      "epoch 204| loss: 0.33806 | val_0_auc: 0.87794 |  0:07:48s\n",
      "epoch 205| loss: 0.33922 | val_0_auc: 0.87731 |  0:07:50s\n",
      "epoch 206| loss: 0.33803 | val_0_auc: 0.87754 |  0:07:52s\n",
      "epoch 207| loss: 0.33831 | val_0_auc: 0.87839 |  0:07:54s\n",
      "epoch 208| loss: 0.3398  | val_0_auc: 0.87733 |  0:07:57s\n",
      "epoch 209| loss: 0.3367  | val_0_auc: 0.87728 |  0:07:59s\n",
      "epoch 210| loss: 0.33738 | val_0_auc: 0.87884 |  0:08:01s\n",
      "epoch 211| loss: 0.33393 | val_0_auc: 0.87937 |  0:08:03s\n",
      "epoch 212| loss: 0.33715 | val_0_auc: 0.87907 |  0:08:05s\n",
      "epoch 213| loss: 0.33529 | val_0_auc: 0.88005 |  0:08:08s\n",
      "epoch 214| loss: 0.33796 | val_0_auc: 0.87839 |  0:08:10s\n",
      "epoch 215| loss: 0.33398 | val_0_auc: 0.8795  |  0:08:12s\n",
      "epoch 216| loss: 0.33413 | val_0_auc: 0.88028 |  0:08:14s\n",
      "epoch 217| loss: 0.33461 | val_0_auc: 0.88104 |  0:08:17s\n",
      "epoch 218| loss: 0.33666 | val_0_auc: 0.87932 |  0:08:19s\n",
      "epoch 219| loss: 0.33677 | val_0_auc: 0.88011 |  0:08:21s\n",
      "epoch 220| loss: 0.33435 | val_0_auc: 0.88069 |  0:08:23s\n",
      "epoch 221| loss: 0.33465 | val_0_auc: 0.88149 |  0:08:26s\n",
      "epoch 222| loss: 0.33619 | val_0_auc: 0.8813  |  0:08:28s\n",
      "epoch 223| loss: 0.33465 | val_0_auc: 0.88197 |  0:08:30s\n",
      "epoch 224| loss: 0.33297 | val_0_auc: 0.88419 |  0:08:32s\n",
      "epoch 225| loss: 0.33364 | val_0_auc: 0.88386 |  0:08:35s\n",
      "epoch 226| loss: 0.33116 | val_0_auc: 0.88199 |  0:08:37s\n",
      "epoch 227| loss: 0.33263 | val_0_auc: 0.88278 |  0:08:39s\n",
      "epoch 228| loss: 0.33076 | val_0_auc: 0.88176 |  0:08:41s\n",
      "epoch 229| loss: 0.33288 | val_0_auc: 0.88264 |  0:08:43s\n",
      "epoch 230| loss: 0.33166 | val_0_auc: 0.88208 |  0:08:46s\n",
      "epoch 231| loss: 0.33057 | val_0_auc: 0.88204 |  0:08:48s\n",
      "epoch 232| loss: 0.33037 | val_0_auc: 0.88261 |  0:08:50s\n",
      "epoch 233| loss: 0.33121 | val_0_auc: 0.88374 |  0:08:52s\n",
      "epoch 234| loss: 0.3291  | val_0_auc: 0.88366 |  0:08:54s\n",
      "epoch 235| loss: 0.32955 | val_0_auc: 0.88383 |  0:08:57s\n",
      "epoch 236| loss: 0.33166 | val_0_auc: 0.88433 |  0:08:59s\n",
      "epoch 237| loss: 0.32957 | val_0_auc: 0.88471 |  0:09:01s\n",
      "epoch 238| loss: 0.32904 | val_0_auc: 0.88569 |  0:09:03s\n",
      "epoch 239| loss: 0.3291  | val_0_auc: 0.88583 |  0:09:05s\n",
      "epoch 240| loss: 0.32939 | val_0_auc: 0.88433 |  0:09:08s\n",
      "epoch 241| loss: 0.32837 | val_0_auc: 0.88394 |  0:09:10s\n",
      "epoch 242| loss: 0.32807 | val_0_auc: 0.88505 |  0:09:12s\n",
      "epoch 243| loss: 0.32886 | val_0_auc: 0.88448 |  0:09:14s\n",
      "epoch 244| loss: 0.32644 | val_0_auc: 0.886   |  0:09:16s\n",
      "epoch 245| loss: 0.32781 | val_0_auc: 0.88637 |  0:09:19s\n",
      "epoch 246| loss: 0.32423 | val_0_auc: 0.88687 |  0:09:21s\n",
      "epoch 247| loss: 0.32609 | val_0_auc: 0.88527 |  0:09:23s\n",
      "epoch 248| loss: 0.32525 | val_0_auc: 0.8847  |  0:09:25s\n",
      "epoch 249| loss: 0.32455 | val_0_auc: 0.88624 |  0:09:27s\n",
      "epoch 250| loss: 0.32622 | val_0_auc: 0.8864  |  0:09:30s\n",
      "epoch 251| loss: 0.32604 | val_0_auc: 0.88947 |  0:09:32s\n",
      "epoch 252| loss: 0.32254 | val_0_auc: 0.88968 |  0:09:34s\n",
      "epoch 253| loss: 0.32374 | val_0_auc: 0.89012 |  0:09:36s\n",
      "epoch 254| loss: 0.32097 | val_0_auc: 0.88899 |  0:09:39s\n",
      "epoch 255| loss: 0.32312 | val_0_auc: 0.88884 |  0:09:41s\n",
      "epoch 256| loss: 0.32101 | val_0_auc: 0.88958 |  0:09:43s\n",
      "epoch 257| loss: 0.32193 | val_0_auc: 0.89036 |  0:09:45s\n",
      "epoch 258| loss: 0.31964 | val_0_auc: 0.89022 |  0:09:47s\n",
      "epoch 259| loss: 0.32249 | val_0_auc: 0.89027 |  0:09:49s\n",
      "epoch 260| loss: 0.32112 | val_0_auc: 0.89071 |  0:09:52s\n",
      "epoch 261| loss: 0.32051 | val_0_auc: 0.8907  |  0:09:54s\n",
      "epoch 262| loss: 0.3212  | val_0_auc: 0.89019 |  0:09:56s\n",
      "epoch 263| loss: 0.31925 | val_0_auc: 0.8909  |  0:09:58s\n",
      "epoch 264| loss: 0.31907 | val_0_auc: 0.89104 |  0:10:00s\n",
      "epoch 265| loss: 0.32097 | val_0_auc: 0.89016 |  0:10:03s\n",
      "epoch 266| loss: 0.32154 | val_0_auc: 0.88987 |  0:10:05s\n",
      "epoch 267| loss: 0.31802 | val_0_auc: 0.89155 |  0:10:07s\n",
      "epoch 268| loss: 0.31818 | val_0_auc: 0.89212 |  0:10:09s\n",
      "epoch 269| loss: 0.31899 | val_0_auc: 0.89258 |  0:10:11s\n",
      "epoch 270| loss: 0.31646 | val_0_auc: 0.89258 |  0:10:14s\n",
      "epoch 271| loss: 0.31791 | val_0_auc: 0.89262 |  0:10:16s\n",
      "epoch 272| loss: 0.31831 | val_0_auc: 0.89298 |  0:10:18s\n",
      "epoch 273| loss: 0.31831 | val_0_auc: 0.89241 |  0:10:20s\n",
      "epoch 274| loss: 0.31864 | val_0_auc: 0.89283 |  0:10:23s\n",
      "epoch 275| loss: 0.31919 | val_0_auc: 0.89257 |  0:10:25s\n",
      "epoch 276| loss: 0.318   | val_0_auc: 0.89133 |  0:10:27s\n",
      "epoch 277| loss: 0.31486 | val_0_auc: 0.8922  |  0:10:29s\n",
      "epoch 278| loss: 0.31718 | val_0_auc: 0.89332 |  0:10:31s\n",
      "epoch 279| loss: 0.31614 | val_0_auc: 0.89331 |  0:10:34s\n",
      "epoch 280| loss: 0.31494 | val_0_auc: 0.89394 |  0:10:36s\n",
      "epoch 281| loss: 0.31536 | val_0_auc: 0.89366 |  0:10:38s\n",
      "epoch 282| loss: 0.31748 | val_0_auc: 0.89427 |  0:10:40s\n",
      "epoch 283| loss: 0.31547 | val_0_auc: 0.89425 |  0:10:43s\n",
      "epoch 284| loss: 0.31512 | val_0_auc: 0.89386 |  0:10:45s\n",
      "epoch 285| loss: 0.3154  | val_0_auc: 0.89403 |  0:10:47s\n",
      "epoch 286| loss: 0.31589 | val_0_auc: 0.89408 |  0:10:49s\n",
      "epoch 287| loss: 0.31437 | val_0_auc: 0.89503 |  0:10:51s\n",
      "epoch 288| loss: 0.31814 | val_0_auc: 0.89484 |  0:10:54s\n",
      "epoch 289| loss: 0.31515 | val_0_auc: 0.8948  |  0:10:56s\n",
      "epoch 290| loss: 0.3132  | val_0_auc: 0.8943  |  0:10:58s\n",
      "epoch 291| loss: 0.31304 | val_0_auc: 0.89441 |  0:11:00s\n",
      "epoch 292| loss: 0.3125  | val_0_auc: 0.89343 |  0:11:02s\n",
      "epoch 293| loss: 0.31301 | val_0_auc: 0.89424 |  0:11:05s\n",
      "epoch 294| loss: 0.31283 | val_0_auc: 0.89434 |  0:11:07s\n",
      "epoch 295| loss: 0.31076 | val_0_auc: 0.89579 |  0:11:09s\n",
      "epoch 296| loss: 0.31302 | val_0_auc: 0.89553 |  0:11:11s\n",
      "epoch 297| loss: 0.31274 | val_0_auc: 0.89521 |  0:11:13s\n",
      "epoch 298| loss: 0.31224 | val_0_auc: 0.89573 |  0:11:16s\n",
      "epoch 299| loss: 0.31122 | val_0_auc: 0.89517 |  0:11:18s\n",
      "epoch 300| loss: 0.3107  | val_0_auc: 0.89581 |  0:11:20s\n",
      "epoch 301| loss: 0.3096  | val_0_auc: 0.89457 |  0:11:22s\n",
      "epoch 302| loss: 0.31187 | val_0_auc: 0.89554 |  0:11:24s\n",
      "epoch 303| loss: 0.31034 | val_0_auc: 0.89662 |  0:11:27s\n",
      "epoch 304| loss: 0.30869 | val_0_auc: 0.89707 |  0:11:29s\n",
      "epoch 305| loss: 0.31111 | val_0_auc: 0.89607 |  0:11:31s\n",
      "epoch 306| loss: 0.30967 | val_0_auc: 0.89615 |  0:11:33s\n",
      "epoch 307| loss: 0.30821 | val_0_auc: 0.89583 |  0:11:35s\n",
      "epoch 308| loss: 0.30778 | val_0_auc: 0.8952  |  0:11:38s\n",
      "epoch 309| loss: 0.31083 | val_0_auc: 0.89521 |  0:11:40s\n",
      "epoch 310| loss: 0.30854 | val_0_auc: 0.89629 |  0:11:42s\n",
      "epoch 311| loss: 0.30723 | val_0_auc: 0.89631 |  0:11:44s\n",
      "epoch 312| loss: 0.30994 | val_0_auc: 0.8959  |  0:11:46s\n",
      "epoch 313| loss: 0.30807 | val_0_auc: 0.89658 |  0:11:49s\n",
      "epoch 314| loss: 0.30785 | val_0_auc: 0.89592 |  0:11:51s\n",
      "epoch 315| loss: 0.30792 | val_0_auc: 0.89602 |  0:11:53s\n",
      "epoch 316| loss: 0.30711 | val_0_auc: 0.89679 |  0:11:55s\n",
      "epoch 317| loss: 0.30649 | val_0_auc: 0.89641 |  0:11:57s\n",
      "epoch 318| loss: 0.30517 | val_0_auc: 0.8961  |  0:12:00s\n",
      "epoch 319| loss: 0.30628 | val_0_auc: 0.89674 |  0:12:02s\n",
      "epoch 320| loss: 0.30528 | val_0_auc: 0.89781 |  0:12:04s\n",
      "epoch 321| loss: 0.30457 | val_0_auc: 0.89762 |  0:12:06s\n",
      "epoch 322| loss: 0.30397 | val_0_auc: 0.89658 |  0:12:08s\n",
      "epoch 323| loss: 0.30449 | val_0_auc: 0.89676 |  0:12:10s\n",
      "epoch 324| loss: 0.30437 | val_0_auc: 0.89669 |  0:12:13s\n",
      "epoch 325| loss: 0.30566 | val_0_auc: 0.89687 |  0:12:15s\n",
      "epoch 326| loss: 0.30494 | val_0_auc: 0.89766 |  0:12:17s\n",
      "epoch 327| loss: 0.30589 | val_0_auc: 0.89748 |  0:12:19s\n",
      "epoch 328| loss: 0.30423 | val_0_auc: 0.89823 |  0:12:21s\n",
      "epoch 329| loss: 0.30394 | val_0_auc: 0.89852 |  0:12:24s\n",
      "epoch 330| loss: 0.30508 | val_0_auc: 0.89844 |  0:12:26s\n",
      "epoch 331| loss: 0.30404 | val_0_auc: 0.89809 |  0:12:28s\n",
      "epoch 332| loss: 0.30457 | val_0_auc: 0.89946 |  0:12:30s\n",
      "epoch 333| loss: 0.30153 | val_0_auc: 0.89876 |  0:12:32s\n",
      "epoch 334| loss: 0.30245 | val_0_auc: 0.89856 |  0:12:35s\n",
      "epoch 335| loss: 0.30183 | val_0_auc: 0.89909 |  0:12:37s\n",
      "epoch 336| loss: 0.30438 | val_0_auc: 0.89872 |  0:12:39s\n",
      "epoch 337| loss: 0.30226 | val_0_auc: 0.89892 |  0:12:41s\n",
      "epoch 338| loss: 0.30311 | val_0_auc: 0.89864 |  0:12:44s\n",
      "epoch 339| loss: 0.30271 | val_0_auc: 0.89932 |  0:12:46s\n",
      "epoch 340| loss: 0.30072 | val_0_auc: 0.89855 |  0:12:48s\n",
      "epoch 341| loss: 0.29989 | val_0_auc: 0.89886 |  0:12:50s\n",
      "epoch 342| loss: 0.30083 | val_0_auc: 0.89925 |  0:12:52s\n",
      "epoch 343| loss: 0.30175 | val_0_auc: 0.89916 |  0:12:55s\n",
      "epoch 344| loss: 0.30109 | val_0_auc: 0.89943 |  0:12:57s\n",
      "epoch 345| loss: 0.30085 | val_0_auc: 0.89857 |  0:12:59s\n",
      "epoch 346| loss: 0.29995 | val_0_auc: 0.89936 |  0:13:01s\n",
      "epoch 347| loss: 0.2999  | val_0_auc: 0.89955 |  0:13:03s\n",
      "epoch 348| loss: 0.3024  | val_0_auc: 0.89854 |  0:13:06s\n",
      "epoch 349| loss: 0.30082 | val_0_auc: 0.89955 |  0:13:08s\n",
      "epoch 350| loss: 0.30191 | val_0_auc: 0.89934 |  0:13:10s\n",
      "epoch 351| loss: 0.29901 | val_0_auc: 0.89865 |  0:13:12s\n",
      "epoch 352| loss: 0.30156 | val_0_auc: 0.89888 |  0:13:14s\n",
      "epoch 353| loss: 0.29935 | val_0_auc: 0.89847 |  0:13:17s\n",
      "epoch 354| loss: 0.30088 | val_0_auc: 0.89892 |  0:13:19s\n",
      "epoch 355| loss: 0.29971 | val_0_auc: 0.89911 |  0:13:21s\n",
      "epoch 356| loss: 0.29864 | val_0_auc: 0.89981 |  0:13:23s\n",
      "epoch 357| loss: 0.29878 | val_0_auc: 0.89966 |  0:13:25s\n",
      "epoch 358| loss: 0.2976  | val_0_auc: 0.89935 |  0:13:28s\n",
      "epoch 359| loss: 0.29906 | val_0_auc: 0.89913 |  0:13:30s\n",
      "epoch 360| loss: 0.29827 | val_0_auc: 0.89991 |  0:13:32s\n",
      "epoch 361| loss: 0.29716 | val_0_auc: 0.90056 |  0:13:34s\n",
      "epoch 362| loss: 0.29905 | val_0_auc: 0.90037 |  0:13:36s\n",
      "epoch 363| loss: 0.29776 | val_0_auc: 0.89935 |  0:13:39s\n",
      "epoch 364| loss: 0.29777 | val_0_auc: 0.89977 |  0:13:41s\n",
      "epoch 365| loss: 0.29794 | val_0_auc: 0.89963 |  0:13:43s\n",
      "epoch 366| loss: 0.29855 | val_0_auc: 0.89954 |  0:13:45s\n",
      "epoch 367| loss: 0.29865 | val_0_auc: 0.89994 |  0:13:47s\n",
      "epoch 368| loss: 0.29473 | val_0_auc: 0.90029 |  0:13:49s\n",
      "epoch 369| loss: 0.29999 | val_0_auc: 0.90073 |  0:13:52s\n",
      "epoch 370| loss: 0.29861 | val_0_auc: 0.90148 |  0:13:54s\n",
      "epoch 371| loss: 0.29629 | val_0_auc: 0.90207 |  0:13:56s\n",
      "epoch 372| loss: 0.29493 | val_0_auc: 0.90187 |  0:13:58s\n",
      "epoch 373| loss: 0.29687 | val_0_auc: 0.90171 |  0:14:00s\n",
      "epoch 374| loss: 0.29666 | val_0_auc: 0.9022  |  0:14:03s\n",
      "epoch 375| loss: 0.29815 | val_0_auc: 0.90192 |  0:14:05s\n",
      "epoch 376| loss: 0.29612 | val_0_auc: 0.90173 |  0:14:07s\n",
      "epoch 377| loss: 0.29784 | val_0_auc: 0.90269 |  0:14:09s\n",
      "epoch 378| loss: 0.29487 | val_0_auc: 0.90274 |  0:14:11s\n",
      "epoch 379| loss: 0.29582 | val_0_auc: 0.90342 |  0:14:14s\n",
      "epoch 380| loss: 0.29559 | val_0_auc: 0.90318 |  0:14:16s\n",
      "epoch 381| loss: 0.29603 | val_0_auc: 0.90253 |  0:14:18s\n",
      "epoch 382| loss: 0.29579 | val_0_auc: 0.90248 |  0:14:21s\n",
      "epoch 383| loss: 0.294   | val_0_auc: 0.90216 |  0:14:23s\n",
      "epoch 384| loss: 0.29478 | val_0_auc: 0.90207 |  0:14:25s\n",
      "epoch 385| loss: 0.29507 | val_0_auc: 0.90234 |  0:14:27s\n",
      "epoch 386| loss: 0.29546 | val_0_auc: 0.90193 |  0:14:30s\n",
      "epoch 387| loss: 0.29508 | val_0_auc: 0.90246 |  0:14:32s\n",
      "epoch 388| loss: 0.2956  | val_0_auc: 0.90248 |  0:14:35s\n",
      "epoch 389| loss: 0.29438 | val_0_auc: 0.90276 |  0:14:37s\n",
      "epoch 390| loss: 0.29346 | val_0_auc: 0.90264 |  0:14:39s\n",
      "epoch 391| loss: 0.29356 | val_0_auc: 0.90272 |  0:14:42s\n",
      "epoch 392| loss: 0.29282 | val_0_auc: 0.90323 |  0:14:44s\n",
      "epoch 393| loss: 0.29535 | val_0_auc: 0.90337 |  0:14:46s\n",
      "epoch 394| loss: 0.29413 | val_0_auc: 0.90287 |  0:14:48s\n",
      "epoch 395| loss: 0.29143 | val_0_auc: 0.90323 |  0:14:51s\n",
      "epoch 396| loss: 0.2929  | val_0_auc: 0.90287 |  0:14:53s\n",
      "epoch 397| loss: 0.28827 | val_0_auc: 0.90279 |  0:14:55s\n",
      "epoch 398| loss: 0.2914  | val_0_auc: 0.90344 |  0:14:57s\n",
      "epoch 399| loss: 0.28937 | val_0_auc: 0.90356 |  0:15:00s\n",
      "epoch 400| loss: 0.29052 | val_0_auc: 0.90301 |  0:15:02s\n",
      "epoch 401| loss: 0.29316 | val_0_auc: 0.90303 |  0:15:04s\n",
      "epoch 402| loss: 0.29026 | val_0_auc: 0.90294 |  0:15:07s\n",
      "epoch 403| loss: 0.2926  | val_0_auc: 0.90359 |  0:15:09s\n",
      "epoch 404| loss: 0.29122 | val_0_auc: 0.90398 |  0:15:11s\n",
      "epoch 405| loss: 0.29057 | val_0_auc: 0.90371 |  0:15:14s\n",
      "epoch 406| loss: 0.29101 | val_0_auc: 0.90348 |  0:15:17s\n",
      "epoch 407| loss: 0.2912  | val_0_auc: 0.90311 |  0:15:19s\n",
      "epoch 408| loss: 0.29005 | val_0_auc: 0.9035  |  0:15:22s\n",
      "epoch 409| loss: 0.29044 | val_0_auc: 0.90406 |  0:15:24s\n",
      "epoch 410| loss: 0.28981 | val_0_auc: 0.9038  |  0:15:26s\n",
      "epoch 411| loss: 0.29132 | val_0_auc: 0.90384 |  0:15:28s\n",
      "epoch 412| loss: 0.29192 | val_0_auc: 0.90376 |  0:15:30s\n",
      "epoch 413| loss: 0.29009 | val_0_auc: 0.90392 |  0:15:32s\n",
      "epoch 414| loss: 0.29078 | val_0_auc: 0.90426 |  0:15:35s\n",
      "epoch 415| loss: 0.29025 | val_0_auc: 0.90409 |  0:15:37s\n",
      "epoch 416| loss: 0.28952 | val_0_auc: 0.90382 |  0:15:39s\n",
      "epoch 417| loss: 0.29213 | val_0_auc: 0.9044  |  0:15:41s\n",
      "epoch 418| loss: 0.29034 | val_0_auc: 0.90456 |  0:15:44s\n",
      "epoch 419| loss: 0.28782 | val_0_auc: 0.90466 |  0:15:46s\n",
      "epoch 420| loss: 0.29061 | val_0_auc: 0.90462 |  0:15:48s\n",
      "epoch 421| loss: 0.28915 | val_0_auc: 0.9042  |  0:15:50s\n",
      "epoch 422| loss: 0.28947 | val_0_auc: 0.90465 |  0:15:53s\n",
      "epoch 423| loss: 0.29246 | val_0_auc: 0.90459 |  0:15:55s\n",
      "epoch 424| loss: 0.29103 | val_0_auc: 0.90494 |  0:15:57s\n",
      "epoch 425| loss: 0.29014 | val_0_auc: 0.90397 |  0:15:59s\n",
      "epoch 426| loss: 0.28921 | val_0_auc: 0.90392 |  0:16:02s\n",
      "epoch 427| loss: 0.28973 | val_0_auc: 0.903   |  0:16:04s\n",
      "epoch 428| loss: 0.28873 | val_0_auc: 0.90313 |  0:16:06s\n",
      "epoch 429| loss: 0.28791 | val_0_auc: 0.90354 |  0:16:08s\n",
      "epoch 430| loss: 0.28808 | val_0_auc: 0.90381 |  0:16:10s\n",
      "epoch 431| loss: 0.28891 | val_0_auc: 0.90435 |  0:16:12s\n",
      "epoch 432| loss: 0.28923 | val_0_auc: 0.9042  |  0:16:15s\n",
      "epoch 433| loss: 0.28512 | val_0_auc: 0.90402 |  0:16:17s\n",
      "epoch 434| loss: 0.289   | val_0_auc: 0.90471 |  0:16:19s\n",
      "epoch 435| loss: 0.28793 | val_0_auc: 0.90516 |  0:16:21s\n",
      "epoch 436| loss: 0.28572 | val_0_auc: 0.90523 |  0:16:24s\n",
      "epoch 437| loss: 0.28737 | val_0_auc: 0.90487 |  0:16:26s\n",
      "epoch 438| loss: 0.28875 | val_0_auc: 0.90546 |  0:16:28s\n",
      "epoch 439| loss: 0.28696 | val_0_auc: 0.90526 |  0:16:30s\n",
      "epoch 440| loss: 0.28637 | val_0_auc: 0.90468 |  0:16:32s\n",
      "epoch 441| loss: 0.28739 | val_0_auc: 0.90465 |  0:16:34s\n",
      "epoch 442| loss: 0.28688 | val_0_auc: 0.90517 |  0:16:36s\n",
      "epoch 443| loss: 0.28637 | val_0_auc: 0.90551 |  0:16:38s\n",
      "epoch 444| loss: 0.28618 | val_0_auc: 0.90554 |  0:16:41s\n",
      "epoch 445| loss: 0.2882  | val_0_auc: 0.90518 |  0:16:43s\n",
      "epoch 446| loss: 0.28497 | val_0_auc: 0.90518 |  0:16:45s\n",
      "epoch 447| loss: 0.28532 | val_0_auc: 0.90485 |  0:16:47s\n",
      "epoch 448| loss: 0.28677 | val_0_auc: 0.90519 |  0:16:49s\n",
      "epoch 449| loss: 0.28618 | val_0_auc: 0.9052  |  0:16:51s\n",
      "epoch 450| loss: 0.28358 | val_0_auc: 0.90571 |  0:16:54s\n",
      "epoch 451| loss: 0.28663 | val_0_auc: 0.90524 |  0:16:56s\n",
      "epoch 452| loss: 0.28591 | val_0_auc: 0.90505 |  0:16:58s\n",
      "epoch 453| loss: 0.28613 | val_0_auc: 0.90449 |  0:17:00s\n",
      "epoch 454| loss: 0.28481 | val_0_auc: 0.90467 |  0:17:02s\n",
      "epoch 455| loss: 0.28279 | val_0_auc: 0.90514 |  0:17:04s\n",
      "epoch 456| loss: 0.28544 | val_0_auc: 0.9051  |  0:17:06s\n",
      "epoch 457| loss: 0.28188 | val_0_auc: 0.90549 |  0:17:09s\n",
      "epoch 458| loss: 0.28199 | val_0_auc: 0.90535 |  0:17:11s\n",
      "epoch 459| loss: 0.28229 | val_0_auc: 0.90472 |  0:17:13s\n",
      "epoch 460| loss: 0.28203 | val_0_auc: 0.90495 |  0:17:15s\n",
      "epoch 461| loss: 0.28395 | val_0_auc: 0.90541 |  0:17:17s\n",
      "epoch 462| loss: 0.28336 | val_0_auc: 0.90512 |  0:17:19s\n",
      "epoch 463| loss: 0.2851  | val_0_auc: 0.90513 |  0:17:21s\n",
      "epoch 464| loss: 0.28244 | val_0_auc: 0.90554 |  0:17:24s\n",
      "epoch 465| loss: 0.28402 | val_0_auc: 0.90486 |  0:17:26s\n",
      "epoch 466| loss: 0.28239 | val_0_auc: 0.90535 |  0:17:28s\n",
      "epoch 467| loss: 0.28459 | val_0_auc: 0.90526 |  0:17:30s\n",
      "epoch 468| loss: 0.28291 | val_0_auc: 0.90589 |  0:17:32s\n",
      "epoch 469| loss: 0.28211 | val_0_auc: 0.90542 |  0:17:34s\n",
      "epoch 470| loss: 0.28316 | val_0_auc: 0.90517 |  0:17:37s\n",
      "epoch 471| loss: 0.28142 | val_0_auc: 0.90502 |  0:17:39s\n",
      "epoch 472| loss: 0.28153 | val_0_auc: 0.90535 |  0:17:41s\n",
      "epoch 473| loss: 0.28042 | val_0_auc: 0.90566 |  0:17:43s\n",
      "epoch 474| loss: 0.28118 | val_0_auc: 0.90528 |  0:17:45s\n",
      "epoch 475| loss: 0.28194 | val_0_auc: 0.90479 |  0:17:47s\n",
      "epoch 476| loss: 0.28032 | val_0_auc: 0.90519 |  0:17:50s\n",
      "epoch 477| loss: 0.28076 | val_0_auc: 0.90505 |  0:17:52s\n",
      "epoch 478| loss: 0.28085 | val_0_auc: 0.90545 |  0:17:54s\n",
      "epoch 479| loss: 0.28072 | val_0_auc: 0.90582 |  0:17:56s\n",
      "epoch 480| loss: 0.28111 | val_0_auc: 0.9062  |  0:17:58s\n",
      "epoch 481| loss: 0.28156 | val_0_auc: 0.90535 |  0:18:00s\n",
      "epoch 482| loss: 0.28188 | val_0_auc: 0.90512 |  0:18:02s\n",
      "epoch 483| loss: 0.28167 | val_0_auc: 0.90543 |  0:18:05s\n",
      "epoch 484| loss: 0.28096 | val_0_auc: 0.90514 |  0:18:07s\n",
      "epoch 485| loss: 0.28144 | val_0_auc: 0.90538 |  0:18:09s\n",
      "epoch 486| loss: 0.28218 | val_0_auc: 0.9061  |  0:18:11s\n",
      "epoch 487| loss: 0.28194 | val_0_auc: 0.90596 |  0:18:13s\n",
      "epoch 488| loss: 0.27938 | val_0_auc: 0.90639 |  0:18:15s\n",
      "epoch 489| loss: 0.28003 | val_0_auc: 0.90665 |  0:18:18s\n",
      "epoch 490| loss: 0.28081 | val_0_auc: 0.9066  |  0:18:20s\n",
      "epoch 491| loss: 0.28035 | val_0_auc: 0.90635 |  0:18:22s\n",
      "epoch 492| loss: 0.27959 | val_0_auc: 0.90622 |  0:18:24s\n",
      "epoch 493| loss: 0.28005 | val_0_auc: 0.9059  |  0:18:26s\n",
      "epoch 494| loss: 0.27948 | val_0_auc: 0.90598 |  0:18:28s\n",
      "epoch 495| loss: 0.2815  | val_0_auc: 0.90586 |  0:18:31s\n",
      "epoch 496| loss: 0.27939 | val_0_auc: 0.90513 |  0:18:33s\n",
      "epoch 497| loss: 0.28081 | val_0_auc: 0.90518 |  0:18:35s\n",
      "epoch 498| loss: 0.27871 | val_0_auc: 0.90571 |  0:18:37s\n",
      "epoch 499| loss: 0.27825 | val_0_auc: 0.90605 |  0:18:39s\n",
      "epoch 500| loss: 0.27861 | val_0_auc: 0.90593 |  0:18:41s\n",
      "epoch 501| loss: 0.28054 | val_0_auc: 0.90553 |  0:18:43s\n",
      "epoch 502| loss: 0.27883 | val_0_auc: 0.9052  |  0:18:45s\n",
      "epoch 503| loss: 0.27811 | val_0_auc: 0.90498 |  0:18:48s\n",
      "epoch 504| loss: 0.28014 | val_0_auc: 0.90527 |  0:18:50s\n",
      "epoch 505| loss: 0.2776  | val_0_auc: 0.90523 |  0:18:52s\n",
      "epoch 506| loss: 0.28009 | val_0_auc: 0.90553 |  0:18:54s\n",
      "epoch 507| loss: 0.27953 | val_0_auc: 0.90519 |  0:18:56s\n",
      "epoch 508| loss: 0.27995 | val_0_auc: 0.90486 |  0:18:58s\n",
      "epoch 509| loss: 0.27807 | val_0_auc: 0.90462 |  0:19:00s\n",
      "\n",
      "Early stopping occurred at epoch 509 with best_epoch = 489 and best_val_0_auc = 0.90665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84484 | val_0_auc: 0.66765 |  0:00:01s\n",
      "epoch 1  | loss: 0.63994 | val_0_auc: 0.76946 |  0:00:02s\n",
      "epoch 2  | loss: 0.52631 | val_0_auc: 0.796   |  0:00:04s\n",
      "epoch 3  | loss: 0.45659 | val_0_auc: 0.82947 |  0:00:05s\n",
      "epoch 4  | loss: 0.40463 | val_0_auc: 0.85202 |  0:00:06s\n",
      "epoch 5  | loss: 0.37023 | val_0_auc: 0.86821 |  0:00:08s\n",
      "epoch 6  | loss: 0.34466 | val_0_auc: 0.88103 |  0:00:09s\n",
      "epoch 7  | loss: 0.33292 | val_0_auc: 0.88597 |  0:00:11s\n",
      "epoch 8  | loss: 0.32117 | val_0_auc: 0.89251 |  0:00:12s\n",
      "epoch 9  | loss: 0.31269 | val_0_auc: 0.89764 |  0:00:13s\n",
      "epoch 10 | loss: 0.30717 | val_0_auc: 0.89905 |  0:00:15s\n",
      "epoch 11 | loss: 0.30268 | val_0_auc: 0.90293 |  0:00:16s\n",
      "epoch 12 | loss: 0.30104 | val_0_auc: 0.90484 |  0:00:18s\n",
      "epoch 13 | loss: 0.29854 | val_0_auc: 0.90755 |  0:00:19s\n",
      "epoch 14 | loss: 0.29647 | val_0_auc: 0.90598 |  0:00:20s\n",
      "epoch 15 | loss: 0.29139 | val_0_auc: 0.90795 |  0:00:22s\n",
      "epoch 16 | loss: 0.291   | val_0_auc: 0.90998 |  0:00:23s\n",
      "epoch 17 | loss: 0.29125 | val_0_auc: 0.90942 |  0:00:25s\n",
      "epoch 18 | loss: 0.28615 | val_0_auc: 0.90739 |  0:00:26s\n",
      "epoch 19 | loss: 0.28772 | val_0_auc: 0.90575 |  0:00:27s\n",
      "epoch 20 | loss: 0.28622 | val_0_auc: 0.90377 |  0:00:29s\n",
      "epoch 21 | loss: 0.28457 | val_0_auc: 0.90651 |  0:00:30s\n",
      "epoch 22 | loss: 0.28225 | val_0_auc: 0.90461 |  0:00:32s\n",
      "epoch 23 | loss: 0.28056 | val_0_auc: 0.90578 |  0:00:33s\n",
      "epoch 24 | loss: 0.28285 | val_0_auc: 0.90606 |  0:00:34s\n",
      "epoch 25 | loss: 0.27852 | val_0_auc: 0.90642 |  0:00:36s\n",
      "epoch 26 | loss: 0.27916 | val_0_auc: 0.90599 |  0:00:37s\n",
      "epoch 27 | loss: 0.27801 | val_0_auc: 0.90682 |  0:00:39s\n",
      "epoch 28 | loss: 0.27731 | val_0_auc: 0.907   |  0:00:40s\n",
      "epoch 29 | loss: 0.27445 | val_0_auc: 0.90733 |  0:00:42s\n",
      "epoch 30 | loss: 0.2752  | val_0_auc: 0.90419 |  0:00:43s\n",
      "epoch 31 | loss: 0.27477 | val_0_auc: 0.90605 |  0:00:44s\n",
      "epoch 32 | loss: 0.27387 | val_0_auc: 0.90476 |  0:00:46s\n",
      "epoch 33 | loss: 0.27114 | val_0_auc: 0.9048  |  0:00:47s\n",
      "epoch 34 | loss: 0.27085 | val_0_auc: 0.90286 |  0:00:49s\n",
      "epoch 35 | loss: 0.27107 | val_0_auc: 0.90316 |  0:00:50s\n",
      "epoch 36 | loss: 0.27157 | val_0_auc: 0.90165 |  0:00:51s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_auc = 0.90998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.79126 | val_0_auc: 0.45548 |  0:00:02s\n",
      "epoch 1  | loss: 1.16177 | val_0_auc: 0.49811 |  0:00:04s\n",
      "epoch 2  | loss: 0.79999 | val_0_auc: 0.51141 |  0:00:06s\n",
      "epoch 3  | loss: 0.67224 | val_0_auc: 0.51939 |  0:00:08s\n",
      "epoch 4  | loss: 0.63182 | val_0_auc: 0.54819 |  0:00:11s\n",
      "epoch 5  | loss: 0.60867 | val_0_auc: 0.56963 |  0:00:13s\n",
      "epoch 6  | loss: 0.58794 | val_0_auc: 0.58284 |  0:00:15s\n",
      "epoch 7  | loss: 0.57903 | val_0_auc: 0.60586 |  0:00:17s\n",
      "epoch 8  | loss: 0.56433 | val_0_auc: 0.63057 |  0:00:19s\n",
      "epoch 9  | loss: 0.55154 | val_0_auc: 0.65346 |  0:00:22s\n",
      "epoch 10 | loss: 0.53971 | val_0_auc: 0.67247 |  0:00:24s\n",
      "epoch 11 | loss: 0.52954 | val_0_auc: 0.69099 |  0:00:26s\n",
      "epoch 12 | loss: 0.52359 | val_0_auc: 0.70683 |  0:00:28s\n",
      "epoch 13 | loss: 0.5168  | val_0_auc: 0.72453 |  0:00:31s\n",
      "epoch 14 | loss: 0.50672 | val_0_auc: 0.73785 |  0:00:33s\n",
      "epoch 15 | loss: 0.50462 | val_0_auc: 0.73713 |  0:00:35s\n",
      "epoch 16 | loss: 0.50063 | val_0_auc: 0.75502 |  0:00:37s\n",
      "epoch 17 | loss: 0.49273 | val_0_auc: 0.75892 |  0:00:40s\n",
      "epoch 18 | loss: 0.49127 | val_0_auc: 0.76653 |  0:00:42s\n",
      "epoch 19 | loss: 0.48328 | val_0_auc: 0.77254 |  0:00:44s\n",
      "epoch 20 | loss: 0.47679 | val_0_auc: 0.77447 |  0:00:46s\n",
      "epoch 21 | loss: 0.4699  | val_0_auc: 0.77686 |  0:00:49s\n",
      "epoch 22 | loss: 0.46875 | val_0_auc: 0.78826 |  0:00:51s\n",
      "epoch 23 | loss: 0.46777 | val_0_auc: 0.79127 |  0:00:54s\n",
      "epoch 24 | loss: 0.45896 | val_0_auc: 0.79387 |  0:00:56s\n",
      "epoch 25 | loss: 0.45672 | val_0_auc: 0.80063 |  0:00:58s\n",
      "epoch 26 | loss: 0.45701 | val_0_auc: 0.80304 |  0:01:00s\n",
      "epoch 27 | loss: 0.45548 | val_0_auc: 0.80639 |  0:01:03s\n",
      "epoch 28 | loss: 0.44899 | val_0_auc: 0.80922 |  0:01:05s\n",
      "epoch 29 | loss: 0.44667 | val_0_auc: 0.80586 |  0:01:07s\n",
      "epoch 30 | loss: 0.44303 | val_0_auc: 0.80743 |  0:01:09s\n",
      "epoch 31 | loss: 0.44353 | val_0_auc: 0.80695 |  0:01:11s\n",
      "epoch 32 | loss: 0.43952 | val_0_auc: 0.80968 |  0:01:14s\n",
      "epoch 33 | loss: 0.43688 | val_0_auc: 0.81325 |  0:01:16s\n",
      "epoch 34 | loss: 0.43619 | val_0_auc: 0.81518 |  0:01:18s\n",
      "epoch 35 | loss: 0.43458 | val_0_auc: 0.81798 |  0:01:20s\n",
      "epoch 36 | loss: 0.43075 | val_0_auc: 0.8187  |  0:01:23s\n",
      "epoch 37 | loss: 0.4347  | val_0_auc: 0.81771 |  0:01:25s\n",
      "epoch 38 | loss: 0.43118 | val_0_auc: 0.81993 |  0:01:27s\n",
      "epoch 39 | loss: 0.42968 | val_0_auc: 0.82232 |  0:01:29s\n",
      "epoch 40 | loss: 0.42612 | val_0_auc: 0.8232  |  0:01:31s\n",
      "epoch 41 | loss: 0.43027 | val_0_auc: 0.82619 |  0:01:34s\n",
      "epoch 42 | loss: 0.42556 | val_0_auc: 0.82569 |  0:01:36s\n",
      "epoch 43 | loss: 0.42394 | val_0_auc: 0.82844 |  0:01:38s\n",
      "epoch 44 | loss: 0.4225  | val_0_auc: 0.82969 |  0:01:40s\n",
      "epoch 45 | loss: 0.42027 | val_0_auc: 0.82669 |  0:01:42s\n",
      "epoch 46 | loss: 0.42213 | val_0_auc: 0.82941 |  0:01:45s\n",
      "epoch 47 | loss: 0.41824 | val_0_auc: 0.82585 |  0:01:47s\n",
      "epoch 48 | loss: 0.41744 | val_0_auc: 0.8286  |  0:01:49s\n",
      "epoch 49 | loss: 0.42284 | val_0_auc: 0.82744 |  0:01:51s\n",
      "epoch 50 | loss: 0.41832 | val_0_auc: 0.82881 |  0:01:53s\n",
      "epoch 51 | loss: 0.41856 | val_0_auc: 0.82714 |  0:01:56s\n",
      "epoch 52 | loss: 0.41628 | val_0_auc: 0.82626 |  0:01:58s\n",
      "epoch 53 | loss: 0.41675 | val_0_auc: 0.83087 |  0:02:00s\n",
      "epoch 54 | loss: 0.42049 | val_0_auc: 0.8316  |  0:02:02s\n",
      "epoch 55 | loss: 0.41681 | val_0_auc: 0.83054 |  0:02:04s\n",
      "epoch 56 | loss: 0.41533 | val_0_auc: 0.83158 |  0:02:08s\n",
      "epoch 57 | loss: 0.41408 | val_0_auc: 0.8292  |  0:02:11s\n",
      "epoch 58 | loss: 0.4127  | val_0_auc: 0.83217 |  0:02:14s\n",
      "epoch 59 | loss: 0.41456 | val_0_auc: 0.83366 |  0:02:16s\n",
      "epoch 60 | loss: 0.41352 | val_0_auc: 0.82977 |  0:02:19s\n",
      "epoch 61 | loss: 0.40781 | val_0_auc: 0.8294  |  0:02:21s\n",
      "epoch 62 | loss: 0.41032 | val_0_auc: 0.82807 |  0:02:24s\n",
      "epoch 63 | loss: 0.40961 | val_0_auc: 0.82648 |  0:02:26s\n",
      "epoch 64 | loss: 0.41063 | val_0_auc: 0.83209 |  0:02:28s\n",
      "epoch 65 | loss: 0.40471 | val_0_auc: 0.83237 |  0:02:31s\n",
      "epoch 66 | loss: 0.40722 | val_0_auc: 0.82979 |  0:02:33s\n",
      "epoch 67 | loss: 0.40192 | val_0_auc: 0.83101 |  0:02:35s\n",
      "epoch 68 | loss: 0.40495 | val_0_auc: 0.83191 |  0:02:37s\n",
      "epoch 69 | loss: 0.40458 | val_0_auc: 0.8315  |  0:02:39s\n",
      "epoch 70 | loss: 0.4037  | val_0_auc: 0.83261 |  0:02:42s\n",
      "epoch 71 | loss: 0.40336 | val_0_auc: 0.83509 |  0:02:44s\n",
      "epoch 72 | loss: 0.40328 | val_0_auc: 0.83593 |  0:02:46s\n",
      "epoch 73 | loss: 0.40218 | val_0_auc: 0.84041 |  0:02:48s\n",
      "epoch 74 | loss: 0.40149 | val_0_auc: 0.839   |  0:02:50s\n",
      "epoch 75 | loss: 0.40077 | val_0_auc: 0.83717 |  0:02:52s\n",
      "epoch 76 | loss: 0.40004 | val_0_auc: 0.84173 |  0:02:55s\n",
      "epoch 77 | loss: 0.40256 | val_0_auc: 0.83961 |  0:02:57s\n",
      "epoch 78 | loss: 0.39956 | val_0_auc: 0.84029 |  0:02:59s\n",
      "epoch 79 | loss: 0.39693 | val_0_auc: 0.84171 |  0:03:01s\n",
      "epoch 80 | loss: 0.39866 | val_0_auc: 0.84172 |  0:03:04s\n",
      "epoch 81 | loss: 0.40026 | val_0_auc: 0.84079 |  0:03:06s\n",
      "epoch 82 | loss: 0.39853 | val_0_auc: 0.84064 |  0:03:09s\n",
      "epoch 83 | loss: 0.39762 | val_0_auc: 0.84132 |  0:03:12s\n",
      "epoch 84 | loss: 0.39643 | val_0_auc: 0.83884 |  0:03:15s\n",
      "epoch 85 | loss: 0.39525 | val_0_auc: 0.83965 |  0:03:17s\n",
      "epoch 86 | loss: 0.394   | val_0_auc: 0.83941 |  0:03:20s\n",
      "epoch 87 | loss: 0.39859 | val_0_auc: 0.83905 |  0:03:22s\n",
      "epoch 88 | loss: 0.39674 | val_0_auc: 0.84003 |  0:03:25s\n",
      "epoch 89 | loss: 0.39494 | val_0_auc: 0.84017 |  0:03:27s\n",
      "epoch 90 | loss: 0.39591 | val_0_auc: 0.84136 |  0:03:35s\n",
      "epoch 91 | loss: 0.39464 | val_0_auc: 0.84265 |  0:03:40s\n",
      "epoch 92 | loss: 0.39431 | val_0_auc: 0.84392 |  0:03:43s\n",
      "epoch 93 | loss: 0.39332 | val_0_auc: 0.84321 |  0:03:46s\n",
      "epoch 94 | loss: 0.39225 | val_0_auc: 0.84404 |  0:03:49s\n",
      "epoch 95 | loss: 0.39269 | val_0_auc: 0.84546 |  0:03:53s\n",
      "epoch 96 | loss: 0.3911  | val_0_auc: 0.84707 |  0:03:56s\n",
      "epoch 97 | loss: 0.3909  | val_0_auc: 0.84492 |  0:03:59s\n",
      "epoch 98 | loss: 0.39195 | val_0_auc: 0.84322 |  0:04:01s\n",
      "epoch 99 | loss: 0.39057 | val_0_auc: 0.84561 |  0:04:04s\n",
      "epoch 100| loss: 0.38915 | val_0_auc: 0.84441 |  0:04:07s\n",
      "epoch 101| loss: 0.38736 | val_0_auc: 0.8452  |  0:04:09s\n",
      "epoch 102| loss: 0.38688 | val_0_auc: 0.8476  |  0:04:12s\n",
      "epoch 103| loss: 0.38734 | val_0_auc: 0.84876 |  0:04:15s\n",
      "epoch 104| loss: 0.38854 | val_0_auc: 0.84805 |  0:04:18s\n",
      "epoch 105| loss: 0.38844 | val_0_auc: 0.84812 |  0:04:21s\n",
      "epoch 106| loss: 0.38421 | val_0_auc: 0.84976 |  0:04:24s\n",
      "epoch 107| loss: 0.38349 | val_0_auc: 0.84898 |  0:04:26s\n",
      "epoch 108| loss: 0.38613 | val_0_auc: 0.85106 |  0:04:29s\n",
      "epoch 109| loss: 0.38635 | val_0_auc: 0.85304 |  0:04:33s\n",
      "epoch 110| loss: 0.38563 | val_0_auc: 0.85112 |  0:04:35s\n",
      "epoch 111| loss: 0.3848  | val_0_auc: 0.85412 |  0:04:37s\n",
      "epoch 112| loss: 0.38302 | val_0_auc: 0.85289 |  0:04:40s\n",
      "epoch 113| loss: 0.38303 | val_0_auc: 0.85196 |  0:04:42s\n",
      "epoch 114| loss: 0.38219 | val_0_auc: 0.85436 |  0:04:44s\n",
      "epoch 115| loss: 0.3831  | val_0_auc: 0.85419 |  0:04:46s\n",
      "epoch 116| loss: 0.38    | val_0_auc: 0.85653 |  0:04:49s\n",
      "epoch 117| loss: 0.37652 | val_0_auc: 0.85398 |  0:04:51s\n",
      "epoch 118| loss: 0.38241 | val_0_auc: 0.85535 |  0:04:53s\n",
      "epoch 119| loss: 0.37831 | val_0_auc: 0.85406 |  0:04:55s\n",
      "epoch 120| loss: 0.37804 | val_0_auc: 0.85776 |  0:04:57s\n",
      "epoch 121| loss: 0.37847 | val_0_auc: 0.85979 |  0:05:00s\n",
      "epoch 122| loss: 0.37622 | val_0_auc: 0.85605 |  0:05:02s\n",
      "epoch 123| loss: 0.37535 | val_0_auc: 0.85763 |  0:05:04s\n",
      "epoch 124| loss: 0.37721 | val_0_auc: 0.85851 |  0:05:07s\n",
      "epoch 125| loss: 0.37939 | val_0_auc: 0.86065 |  0:05:09s\n",
      "epoch 126| loss: 0.3736  | val_0_auc: 0.86175 |  0:05:11s\n",
      "epoch 127| loss: 0.37292 | val_0_auc: 0.86075 |  0:05:13s\n",
      "epoch 128| loss: 0.37657 | val_0_auc: 0.86186 |  0:05:16s\n",
      "epoch 129| loss: 0.37369 | val_0_auc: 0.86192 |  0:05:18s\n",
      "epoch 130| loss: 0.37206 | val_0_auc: 0.86243 |  0:05:20s\n",
      "epoch 131| loss: 0.37302 | val_0_auc: 0.85774 |  0:05:23s\n",
      "epoch 132| loss: 0.37217 | val_0_auc: 0.86077 |  0:05:25s\n",
      "epoch 133| loss: 0.3728  | val_0_auc: 0.86059 |  0:05:27s\n",
      "epoch 134| loss: 0.36883 | val_0_auc: 0.86181 |  0:05:29s\n",
      "epoch 135| loss: 0.37194 | val_0_auc: 0.86266 |  0:05:32s\n",
      "epoch 136| loss: 0.37359 | val_0_auc: 0.8635  |  0:05:34s\n",
      "epoch 137| loss: 0.37387 | val_0_auc: 0.86253 |  0:05:36s\n",
      "epoch 138| loss: 0.37114 | val_0_auc: 0.8644  |  0:05:39s\n",
      "epoch 139| loss: 0.37071 | val_0_auc: 0.86558 |  0:05:41s\n",
      "epoch 140| loss: 0.36652 | val_0_auc: 0.86495 |  0:05:43s\n",
      "epoch 141| loss: 0.36991 | val_0_auc: 0.86567 |  0:05:45s\n",
      "epoch 142| loss: 0.36862 | val_0_auc: 0.86464 |  0:05:48s\n",
      "epoch 143| loss: 0.3662  | val_0_auc: 0.86528 |  0:05:50s\n",
      "epoch 144| loss: 0.36798 | val_0_auc: 0.86603 |  0:05:52s\n",
      "epoch 145| loss: 0.36463 | val_0_auc: 0.86617 |  0:05:54s\n",
      "epoch 146| loss: 0.36421 | val_0_auc: 0.86661 |  0:05:57s\n",
      "epoch 147| loss: 0.36749 | val_0_auc: 0.8676  |  0:06:00s\n",
      "epoch 148| loss: 0.36426 | val_0_auc: 0.86674 |  0:06:02s\n",
      "epoch 149| loss: 0.36542 | val_0_auc: 0.86855 |  0:06:04s\n",
      "epoch 150| loss: 0.36391 | val_0_auc: 0.86628 |  0:06:07s\n",
      "epoch 151| loss: 0.36578 | val_0_auc: 0.86793 |  0:06:09s\n",
      "epoch 152| loss: 0.36261 | val_0_auc: 0.86728 |  0:06:11s\n",
      "epoch 153| loss: 0.36353 | val_0_auc: 0.86866 |  0:06:14s\n",
      "epoch 154| loss: 0.36224 | val_0_auc: 0.86764 |  0:06:16s\n",
      "epoch 155| loss: 0.36183 | val_0_auc: 0.86842 |  0:06:19s\n",
      "epoch 156| loss: 0.36294 | val_0_auc: 0.87041 |  0:06:21s\n",
      "epoch 157| loss: 0.36365 | val_0_auc: 0.86921 |  0:06:24s\n",
      "epoch 158| loss: 0.36333 | val_0_auc: 0.86858 |  0:06:26s\n",
      "epoch 159| loss: 0.36067 | val_0_auc: 0.8688  |  0:06:28s\n",
      "epoch 160| loss: 0.35834 | val_0_auc: 0.87018 |  0:06:30s\n",
      "epoch 161| loss: 0.35986 | val_0_auc: 0.86854 |  0:06:33s\n",
      "epoch 162| loss: 0.36267 | val_0_auc: 0.87029 |  0:06:35s\n",
      "epoch 163| loss: 0.35987 | val_0_auc: 0.87036 |  0:06:37s\n",
      "epoch 164| loss: 0.36047 | val_0_auc: 0.86931 |  0:06:39s\n",
      "epoch 165| loss: 0.35965 | val_0_auc: 0.86964 |  0:06:41s\n",
      "epoch 166| loss: 0.3603  | val_0_auc: 0.86975 |  0:06:44s\n",
      "epoch 167| loss: 0.35874 | val_0_auc: 0.8691  |  0:06:46s\n",
      "epoch 168| loss: 0.3593  | val_0_auc: 0.87003 |  0:06:49s\n",
      "epoch 169| loss: 0.35804 | val_0_auc: 0.86972 |  0:06:51s\n",
      "epoch 170| loss: 0.35721 | val_0_auc: 0.87135 |  0:06:53s\n",
      "epoch 171| loss: 0.35777 | val_0_auc: 0.87121 |  0:06:55s\n",
      "epoch 172| loss: 0.35698 | val_0_auc: 0.87167 |  0:06:57s\n",
      "epoch 173| loss: 0.35679 | val_0_auc: 0.87139 |  0:07:00s\n",
      "epoch 174| loss: 0.35654 | val_0_auc: 0.87369 |  0:07:02s\n",
      "epoch 175| loss: 0.35788 | val_0_auc: 0.87429 |  0:07:04s\n",
      "epoch 176| loss: 0.35614 | val_0_auc: 0.87174 |  0:07:07s\n",
      "epoch 177| loss: 0.35646 | val_0_auc: 0.87206 |  0:07:09s\n",
      "epoch 178| loss: 0.35796 | val_0_auc: 0.87296 |  0:07:11s\n",
      "epoch 179| loss: 0.35274 | val_0_auc: 0.87419 |  0:07:13s\n",
      "epoch 180| loss: 0.35469 | val_0_auc: 0.87449 |  0:07:15s\n",
      "epoch 181| loss: 0.35464 | val_0_auc: 0.87513 |  0:07:18s\n",
      "epoch 182| loss: 0.35314 | val_0_auc: 0.87289 |  0:07:20s\n",
      "epoch 183| loss: 0.35462 | val_0_auc: 0.87267 |  0:07:22s\n",
      "epoch 184| loss: 0.35326 | val_0_auc: 0.87205 |  0:07:24s\n",
      "epoch 185| loss: 0.35658 | val_0_auc: 0.87361 |  0:07:27s\n",
      "epoch 186| loss: 0.35375 | val_0_auc: 0.87264 |  0:07:29s\n",
      "epoch 187| loss: 0.35289 | val_0_auc: 0.87154 |  0:07:32s\n",
      "epoch 188| loss: 0.35432 | val_0_auc: 0.87243 |  0:07:34s\n",
      "epoch 189| loss: 0.3542  | val_0_auc: 0.87446 |  0:07:36s\n",
      "epoch 190| loss: 0.35277 | val_0_auc: 0.87363 |  0:07:38s\n",
      "epoch 191| loss: 0.35264 | val_0_auc: 0.8728  |  0:07:41s\n",
      "epoch 192| loss: 0.35484 | val_0_auc: 0.87166 |  0:07:43s\n",
      "epoch 193| loss: 0.35527 | val_0_auc: 0.87096 |  0:07:45s\n",
      "epoch 194| loss: 0.35013 | val_0_auc: 0.87319 |  0:07:47s\n",
      "epoch 195| loss: 0.35379 | val_0_auc: 0.87425 |  0:07:50s\n",
      "epoch 196| loss: 0.35172 | val_0_auc: 0.8727  |  0:07:52s\n",
      "epoch 197| loss: 0.35286 | val_0_auc: 0.87382 |  0:07:54s\n",
      "epoch 198| loss: 0.35244 | val_0_auc: 0.87113 |  0:07:56s\n",
      "epoch 199| loss: 0.34953 | val_0_auc: 0.87302 |  0:07:58s\n",
      "epoch 200| loss: 0.3502  | val_0_auc: 0.87249 |  0:08:01s\n",
      "epoch 201| loss: 0.35256 | val_0_auc: 0.87301 |  0:08:03s\n",
      "\n",
      "Early stopping occurred at epoch 201 with best_epoch = 181 and best_val_0_auc = 0.87513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.53096 | val_0_auc: 0.74553 |  0:00:01s\n",
      "epoch 1  | loss: 0.43852 | val_0_auc: 0.77868 |  0:00:03s\n",
      "epoch 2  | loss: 0.41501 | val_0_auc: 0.81354 |  0:00:04s\n",
      "epoch 3  | loss: 0.40056 | val_0_auc: 0.80547 |  0:00:06s\n",
      "epoch 4  | loss: 0.39473 | val_0_auc: 0.82006 |  0:00:08s\n",
      "epoch 5  | loss: 0.38623 | val_0_auc: 0.83432 |  0:00:09s\n",
      "epoch 6  | loss: 0.38438 | val_0_auc: 0.81088 |  0:00:11s\n",
      "epoch 7  | loss: 0.37968 | val_0_auc: 0.8452  |  0:00:13s\n",
      "epoch 8  | loss: 0.37793 | val_0_auc: 0.85945 |  0:00:14s\n",
      "epoch 9  | loss: 0.36759 | val_0_auc: 0.86571 |  0:00:16s\n",
      "epoch 10 | loss: 0.36334 | val_0_auc: 0.86908 |  0:00:18s\n",
      "epoch 11 | loss: 0.35818 | val_0_auc: 0.86876 |  0:00:19s\n",
      "epoch 12 | loss: 0.35579 | val_0_auc: 0.87097 |  0:00:21s\n",
      "epoch 13 | loss: 0.35302 | val_0_auc: 0.87303 |  0:00:23s\n",
      "epoch 14 | loss: 0.34645 | val_0_auc: 0.87826 |  0:00:24s\n",
      "epoch 15 | loss: 0.34149 | val_0_auc: 0.88017 |  0:00:26s\n",
      "epoch 16 | loss: 0.33648 | val_0_auc: 0.8866  |  0:00:29s\n",
      "epoch 17 | loss: 0.32784 | val_0_auc: 0.88947 |  0:00:30s\n",
      "epoch 18 | loss: 0.32593 | val_0_auc: 0.88936 |  0:00:32s\n",
      "epoch 19 | loss: 0.32089 | val_0_auc: 0.89377 |  0:00:34s\n",
      "epoch 20 | loss: 0.31564 | val_0_auc: 0.89509 |  0:00:35s\n",
      "epoch 21 | loss: 0.31797 | val_0_auc: 0.89517 |  0:00:37s\n",
      "epoch 22 | loss: 0.31251 | val_0_auc: 0.89435 |  0:00:39s\n",
      "epoch 23 | loss: 0.31115 | val_0_auc: 0.89287 |  0:00:40s\n",
      "epoch 24 | loss: 0.30798 | val_0_auc: 0.89881 |  0:00:42s\n",
      "epoch 25 | loss: 0.30908 | val_0_auc: 0.89666 |  0:00:44s\n",
      "epoch 26 | loss: 0.30766 | val_0_auc: 0.89637 |  0:00:45s\n",
      "epoch 27 | loss: 0.3062  | val_0_auc: 0.90048 |  0:00:47s\n",
      "epoch 28 | loss: 0.30237 | val_0_auc: 0.89756 |  0:00:48s\n",
      "epoch 29 | loss: 0.30216 | val_0_auc: 0.89983 |  0:00:50s\n",
      "epoch 30 | loss: 0.30385 | val_0_auc: 0.90168 |  0:00:51s\n",
      "epoch 31 | loss: 0.30498 | val_0_auc: 0.90039 |  0:00:53s\n",
      "epoch 32 | loss: 0.30071 | val_0_auc: 0.90325 |  0:00:55s\n",
      "epoch 33 | loss: 0.30231 | val_0_auc: 0.89697 |  0:00:56s\n",
      "epoch 34 | loss: 0.30393 | val_0_auc: 0.90497 |  0:00:58s\n",
      "epoch 35 | loss: 0.29987 | val_0_auc: 0.9057  |  0:01:00s\n",
      "epoch 36 | loss: 0.29901 | val_0_auc: 0.90768 |  0:01:02s\n",
      "epoch 37 | loss: 0.29836 | val_0_auc: 0.90773 |  0:01:03s\n",
      "epoch 38 | loss: 0.29789 | val_0_auc: 0.90588 |  0:01:05s\n",
      "epoch 39 | loss: 0.29591 | val_0_auc: 0.90786 |  0:01:07s\n",
      "epoch 40 | loss: 0.29408 | val_0_auc: 0.90918 |  0:01:08s\n",
      "epoch 41 | loss: 0.29273 | val_0_auc: 0.90943 |  0:01:10s\n",
      "epoch 42 | loss: 0.29386 | val_0_auc: 0.91004 |  0:01:11s\n",
      "epoch 43 | loss: 0.29229 | val_0_auc: 0.91008 |  0:01:13s\n",
      "epoch 44 | loss: 0.29479 | val_0_auc: 0.91025 |  0:01:15s\n",
      "epoch 45 | loss: 0.29192 | val_0_auc: 0.90949 |  0:01:17s\n",
      "epoch 46 | loss: 0.29214 | val_0_auc: 0.90815 |  0:01:19s\n",
      "epoch 47 | loss: 0.29033 | val_0_auc: 0.90998 |  0:01:20s\n",
      "epoch 48 | loss: 0.29266 | val_0_auc: 0.90446 |  0:01:22s\n",
      "epoch 49 | loss: 0.29233 | val_0_auc: 0.9063  |  0:01:24s\n",
      "epoch 50 | loss: 0.28914 | val_0_auc: 0.91026 |  0:01:26s\n",
      "epoch 51 | loss: 0.28712 | val_0_auc: 0.91045 |  0:01:28s\n",
      "epoch 52 | loss: 0.28935 | val_0_auc: 0.90686 |  0:01:29s\n",
      "epoch 53 | loss: 0.28806 | val_0_auc: 0.9082  |  0:01:32s\n",
      "epoch 54 | loss: 0.28725 | val_0_auc: 0.90914 |  0:01:34s\n",
      "epoch 55 | loss: 0.28824 | val_0_auc: 0.90995 |  0:01:36s\n",
      "epoch 56 | loss: 0.28737 | val_0_auc: 0.91228 |  0:01:38s\n",
      "epoch 57 | loss: 0.28598 | val_0_auc: 0.91195 |  0:01:41s\n",
      "epoch 58 | loss: 0.2842  | val_0_auc: 0.9117  |  0:01:44s\n",
      "epoch 59 | loss: 0.2858  | val_0_auc: 0.91149 |  0:01:47s\n",
      "epoch 60 | loss: 0.28577 | val_0_auc: 0.9102  |  0:01:50s\n",
      "epoch 61 | loss: 0.28583 | val_0_auc: 0.91168 |  0:01:53s\n",
      "epoch 62 | loss: 0.28364 | val_0_auc: 0.91343 |  0:01:56s\n",
      "epoch 63 | loss: 0.28155 | val_0_auc: 0.91369 |  0:01:59s\n",
      "epoch 64 | loss: 0.28344 | val_0_auc: 0.91104 |  0:02:01s\n",
      "epoch 65 | loss: 0.28155 | val_0_auc: 0.91127 |  0:02:03s\n",
      "epoch 66 | loss: 0.28148 | val_0_auc: 0.91045 |  0:02:06s\n",
      "epoch 67 | loss: 0.28143 | val_0_auc: 0.91175 |  0:02:08s\n",
      "epoch 68 | loss: 0.27959 | val_0_auc: 0.91292 |  0:02:11s\n",
      "epoch 69 | loss: 0.27975 | val_0_auc: 0.9136  |  0:02:13s\n",
      "epoch 70 | loss: 0.27916 | val_0_auc: 0.91345 |  0:02:15s\n",
      "epoch 71 | loss: 0.2775  | val_0_auc: 0.91275 |  0:02:17s\n",
      "epoch 72 | loss: 0.27841 | val_0_auc: 0.91311 |  0:02:19s\n",
      "epoch 73 | loss: 0.2775  | val_0_auc: 0.91243 |  0:02:21s\n",
      "epoch 74 | loss: 0.27643 | val_0_auc: 0.91193 |  0:02:23s\n",
      "epoch 75 | loss: 0.27642 | val_0_auc: 0.91271 |  0:02:25s\n",
      "epoch 76 | loss: 0.27526 | val_0_auc: 0.90411 |  0:02:27s\n",
      "epoch 77 | loss: 0.27765 | val_0_auc: 0.91097 |  0:02:29s\n",
      "epoch 78 | loss: 0.27634 | val_0_auc: 0.91122 |  0:02:31s\n",
      "epoch 79 | loss: 0.27619 | val_0_auc: 0.90399 |  0:02:33s\n",
      "epoch 80 | loss: 0.27563 | val_0_auc: 0.91164 |  0:02:35s\n",
      "epoch 81 | loss: 0.27861 | val_0_auc: 0.91344 |  0:02:37s\n",
      "epoch 82 | loss: 0.27408 | val_0_auc: 0.91224 |  0:02:39s\n",
      "epoch 83 | loss: 0.27585 | val_0_auc: 0.91218 |  0:02:41s\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 63 and best_val_0_auc = 0.91369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.80279 | val_0_auc: 0.60041 |  0:00:01s\n",
      "epoch 1  | loss: 0.61138 | val_0_auc: 0.69586 |  0:00:01s\n",
      "epoch 2  | loss: 0.50771 | val_0_auc: 0.79076 |  0:00:02s\n",
      "epoch 3  | loss: 0.43116 | val_0_auc: 0.81908 |  0:00:03s\n",
      "epoch 4  | loss: 0.40323 | val_0_auc: 0.82918 |  0:00:04s\n",
      "epoch 5  | loss: 0.38274 | val_0_auc: 0.83639 |  0:00:06s\n",
      "epoch 6  | loss: 0.3679  | val_0_auc: 0.84231 |  0:00:07s\n",
      "epoch 7  | loss: 0.35922 | val_0_auc: 0.86043 |  0:00:09s\n",
      "epoch 8  | loss: 0.34403 | val_0_auc: 0.86876 |  0:00:10s\n",
      "epoch 9  | loss: 0.33403 | val_0_auc: 0.87421 |  0:00:11s\n",
      "epoch 10 | loss: 0.32937 | val_0_auc: 0.87693 |  0:00:12s\n",
      "epoch 11 | loss: 0.3241  | val_0_auc: 0.88203 |  0:00:13s\n",
      "epoch 12 | loss: 0.32173 | val_0_auc: 0.88386 |  0:00:14s\n",
      "epoch 13 | loss: 0.31855 | val_0_auc: 0.88702 |  0:00:15s\n",
      "epoch 14 | loss: 0.31345 | val_0_auc: 0.8904  |  0:00:15s\n",
      "epoch 15 | loss: 0.31274 | val_0_auc: 0.89381 |  0:00:16s\n",
      "epoch 16 | loss: 0.30859 | val_0_auc: 0.89439 |  0:00:17s\n",
      "epoch 17 | loss: 0.30549 | val_0_auc: 0.89784 |  0:00:18s\n",
      "epoch 18 | loss: 0.30695 | val_0_auc: 0.89778 |  0:00:19s\n",
      "epoch 19 | loss: 0.30367 | val_0_auc: 0.89758 |  0:00:20s\n",
      "epoch 20 | loss: 0.2997  | val_0_auc: 0.89818 |  0:00:21s\n",
      "epoch 21 | loss: 0.29862 | val_0_auc: 0.89622 |  0:00:21s\n",
      "epoch 22 | loss: 0.29682 | val_0_auc: 0.89972 |  0:00:22s\n",
      "epoch 23 | loss: 0.29626 | val_0_auc: 0.89904 |  0:00:23s\n",
      "epoch 24 | loss: 0.29619 | val_0_auc: 0.89837 |  0:00:24s\n",
      "epoch 25 | loss: 0.29456 | val_0_auc: 0.89976 |  0:00:25s\n",
      "epoch 26 | loss: 0.29293 | val_0_auc: 0.90037 |  0:00:26s\n",
      "epoch 27 | loss: 0.29287 | val_0_auc: 0.90045 |  0:00:27s\n",
      "epoch 28 | loss: 0.2913  | val_0_auc: 0.89985 |  0:00:27s\n",
      "epoch 29 | loss: 0.29173 | val_0_auc: 0.90108 |  0:00:28s\n",
      "epoch 30 | loss: 0.29001 | val_0_auc: 0.90125 |  0:00:29s\n",
      "epoch 31 | loss: 0.28884 | val_0_auc: 0.90047 |  0:00:30s\n",
      "epoch 32 | loss: 0.28824 | val_0_auc: 0.90158 |  0:00:31s\n",
      "epoch 33 | loss: 0.28742 | val_0_auc: 0.9017  |  0:00:32s\n",
      "epoch 34 | loss: 0.28683 | val_0_auc: 0.90107 |  0:00:33s\n",
      "epoch 35 | loss: 0.28664 | val_0_auc: 0.90278 |  0:00:34s\n",
      "epoch 36 | loss: 0.28628 | val_0_auc: 0.90148 |  0:00:34s\n",
      "epoch 37 | loss: 0.28441 | val_0_auc: 0.90317 |  0:00:35s\n",
      "epoch 38 | loss: 0.28354 | val_0_auc: 0.90348 |  0:00:36s\n",
      "epoch 39 | loss: 0.28261 | val_0_auc: 0.90325 |  0:00:37s\n",
      "epoch 40 | loss: 0.28194 | val_0_auc: 0.90367 |  0:00:38s\n",
      "epoch 41 | loss: 0.28043 | val_0_auc: 0.90095 |  0:00:39s\n",
      "epoch 42 | loss: 0.28208 | val_0_auc: 0.89817 |  0:00:40s\n",
      "epoch 43 | loss: 0.27887 | val_0_auc: 0.90268 |  0:00:41s\n",
      "epoch 44 | loss: 0.27844 | val_0_auc: 0.90051 |  0:00:42s\n",
      "epoch 45 | loss: 0.27818 | val_0_auc: 0.90051 |  0:00:43s\n",
      "epoch 46 | loss: 0.27763 | val_0_auc: 0.9015  |  0:00:43s\n",
      "epoch 47 | loss: 0.27848 | val_0_auc: 0.90125 |  0:00:44s\n",
      "epoch 48 | loss: 0.2781  | val_0_auc: 0.89852 |  0:00:45s\n",
      "epoch 49 | loss: 0.27588 | val_0_auc: 0.90063 |  0:00:46s\n",
      "epoch 50 | loss: 0.27567 | val_0_auc: 0.90265 |  0:00:47s\n",
      "epoch 51 | loss: 0.27306 | val_0_auc: 0.8999  |  0:00:48s\n",
      "epoch 52 | loss: 0.27326 | val_0_auc: 0.89828 |  0:00:48s\n",
      "epoch 53 | loss: 0.2738  | val_0_auc: 0.90124 |  0:00:49s\n",
      "epoch 54 | loss: 0.27242 | val_0_auc: 0.90374 |  0:00:50s\n",
      "epoch 55 | loss: 0.27309 | val_0_auc: 0.90506 |  0:00:51s\n",
      "epoch 56 | loss: 0.27128 | val_0_auc: 0.89698 |  0:00:52s\n",
      "epoch 57 | loss: 0.27274 | val_0_auc: 0.90235 |  0:00:53s\n",
      "epoch 58 | loss: 0.27075 | val_0_auc: 0.90294 |  0:00:54s\n",
      "epoch 59 | loss: 0.27002 | val_0_auc: 0.89978 |  0:00:55s\n",
      "epoch 60 | loss: 0.26951 | val_0_auc: 0.90066 |  0:00:56s\n",
      "epoch 61 | loss: 0.27072 | val_0_auc: 0.903   |  0:00:56s\n",
      "epoch 62 | loss: 0.2677  | val_0_auc: 0.89834 |  0:00:57s\n",
      "epoch 63 | loss: 0.27148 | val_0_auc: 0.90119 |  0:00:58s\n",
      "epoch 64 | loss: 0.27008 | val_0_auc: 0.89607 |  0:00:59s\n",
      "epoch 65 | loss: 0.269   | val_0_auc: 0.9003  |  0:01:00s\n",
      "epoch 66 | loss: 0.26767 | val_0_auc: 0.89819 |  0:01:01s\n",
      "epoch 67 | loss: 0.2673  | val_0_auc: 0.9004  |  0:01:02s\n",
      "epoch 68 | loss: 0.26641 | val_0_auc: 0.89599 |  0:01:03s\n",
      "epoch 69 | loss: 0.26692 | val_0_auc: 0.90093 |  0:01:03s\n",
      "epoch 70 | loss: 0.26507 | val_0_auc: 0.89326 |  0:01:04s\n",
      "epoch 71 | loss: 0.2666  | val_0_auc: 0.89991 |  0:01:05s\n",
      "epoch 72 | loss: 0.26677 | val_0_auc: 0.89901 |  0:01:06s\n",
      "epoch 73 | loss: 0.26528 | val_0_auc: 0.89633 |  0:01:07s\n",
      "epoch 74 | loss: 0.26445 | val_0_auc: 0.90094 |  0:01:08s\n",
      "epoch 75 | loss: 0.26315 | val_0_auc: 0.89645 |  0:01:08s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_val_0_auc = 0.90506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.75784 | val_0_auc: 0.47569 |  0:00:02s\n",
      "epoch 1  | loss: 2.32629 | val_0_auc: 0.50031 |  0:00:04s\n",
      "epoch 2  | loss: 1.91702 | val_0_auc: 0.50453 |  0:00:07s\n",
      "epoch 3  | loss: 1.63081 | val_0_auc: 0.53033 |  0:00:09s\n",
      "epoch 4  | loss: 1.39735 | val_0_auc: 0.5313  |  0:00:11s\n",
      "epoch 5  | loss: 1.26386 | val_0_auc: 0.52587 |  0:00:13s\n",
      "epoch 6  | loss: 1.10942 | val_0_auc: 0.5265  |  0:00:16s\n",
      "epoch 7  | loss: 1.01044 | val_0_auc: 0.53028 |  0:00:18s\n",
      "epoch 8  | loss: 0.92744 | val_0_auc: 0.54491 |  0:00:20s\n",
      "epoch 9  | loss: 0.89044 | val_0_auc: 0.54057 |  0:00:23s\n",
      "epoch 10 | loss: 0.84294 | val_0_auc: 0.54016 |  0:00:25s\n",
      "epoch 11 | loss: 0.80412 | val_0_auc: 0.54719 |  0:00:27s\n",
      "epoch 12 | loss: 0.77933 | val_0_auc: 0.55993 |  0:00:30s\n",
      "epoch 13 | loss: 0.75802 | val_0_auc: 0.5556  |  0:00:33s\n",
      "epoch 14 | loss: 0.74899 | val_0_auc: 0.5693  |  0:00:35s\n",
      "epoch 15 | loss: 0.72118 | val_0_auc: 0.57144 |  0:00:37s\n",
      "epoch 16 | loss: 0.71657 | val_0_auc: 0.57407 |  0:00:40s\n",
      "epoch 17 | loss: 0.69625 | val_0_auc: 0.58615 |  0:00:42s\n",
      "epoch 18 | loss: 0.67223 | val_0_auc: 0.60514 |  0:00:44s\n",
      "epoch 19 | loss: 0.66218 | val_0_auc: 0.59184 |  0:00:47s\n",
      "epoch 20 | loss: 0.64922 | val_0_auc: 0.59962 |  0:00:49s\n",
      "epoch 21 | loss: 0.6429  | val_0_auc: 0.62584 |  0:00:52s\n",
      "epoch 22 | loss: 0.635   | val_0_auc: 0.62526 |  0:00:54s\n",
      "epoch 23 | loss: 0.61836 | val_0_auc: 0.64427 |  0:00:56s\n",
      "epoch 24 | loss: 0.60235 | val_0_auc: 0.6597  |  0:00:59s\n",
      "epoch 25 | loss: 0.58852 | val_0_auc: 0.67629 |  0:01:01s\n",
      "epoch 26 | loss: 0.58379 | val_0_auc: 0.68933 |  0:01:04s\n",
      "epoch 27 | loss: 0.57095 | val_0_auc: 0.68885 |  0:01:06s\n",
      "epoch 28 | loss: 0.55794 | val_0_auc: 0.71843 |  0:01:08s\n",
      "epoch 29 | loss: 0.54586 | val_0_auc: 0.73465 |  0:01:11s\n",
      "epoch 30 | loss: 0.53831 | val_0_auc: 0.72337 |  0:01:14s\n",
      "epoch 31 | loss: 0.52949 | val_0_auc: 0.7398  |  0:01:16s\n",
      "epoch 32 | loss: 0.52072 | val_0_auc: 0.75328 |  0:01:19s\n",
      "epoch 33 | loss: 0.51133 | val_0_auc: 0.74737 |  0:01:21s\n",
      "epoch 34 | loss: 0.505   | val_0_auc: 0.7587  |  0:01:23s\n",
      "epoch 35 | loss: 0.49652 | val_0_auc: 0.75433 |  0:01:26s\n",
      "epoch 36 | loss: 0.49564 | val_0_auc: 0.75495 |  0:01:28s\n",
      "epoch 37 | loss: 0.49238 | val_0_auc: 0.76533 |  0:01:31s\n",
      "epoch 38 | loss: 0.48607 | val_0_auc: 0.7703  |  0:01:33s\n",
      "epoch 39 | loss: 0.48497 | val_0_auc: 0.7711  |  0:01:36s\n",
      "epoch 40 | loss: 0.48465 | val_0_auc: 0.77927 |  0:01:38s\n",
      "epoch 41 | loss: 0.48393 | val_0_auc: 0.77902 |  0:01:41s\n",
      "epoch 42 | loss: 0.48019 | val_0_auc: 0.78135 |  0:01:43s\n",
      "epoch 43 | loss: 0.47378 | val_0_auc: 0.78771 |  0:01:45s\n",
      "epoch 44 | loss: 0.47202 | val_0_auc: 0.78335 |  0:01:48s\n",
      "epoch 45 | loss: 0.47155 | val_0_auc: 0.78994 |  0:01:50s\n",
      "epoch 46 | loss: 0.46841 | val_0_auc: 0.79305 |  0:01:52s\n",
      "epoch 47 | loss: 0.46679 | val_0_auc: 0.78225 |  0:01:55s\n",
      "epoch 48 | loss: 0.46387 | val_0_auc: 0.79015 |  0:01:57s\n",
      "epoch 49 | loss: 0.46313 | val_0_auc: 0.79536 |  0:02:00s\n",
      "epoch 50 | loss: 0.46122 | val_0_auc: 0.79497 |  0:02:03s\n",
      "epoch 51 | loss: 0.46169 | val_0_auc: 0.79539 |  0:02:05s\n",
      "epoch 52 | loss: 0.46255 | val_0_auc: 0.79487 |  0:02:08s\n",
      "epoch 53 | loss: 0.45971 | val_0_auc: 0.79638 |  0:02:10s\n",
      "epoch 54 | loss: 0.46039 | val_0_auc: 0.7938  |  0:02:14s\n",
      "epoch 55 | loss: 0.46105 | val_0_auc: 0.80135 |  0:02:16s\n",
      "epoch 56 | loss: 0.45843 | val_0_auc: 0.79797 |  0:02:19s\n",
      "epoch 57 | loss: 0.45568 | val_0_auc: 0.80514 |  0:02:22s\n",
      "epoch 58 | loss: 0.45372 | val_0_auc: 0.80799 |  0:02:25s\n",
      "epoch 59 | loss: 0.4553  | val_0_auc: 0.79787 |  0:02:27s\n",
      "epoch 60 | loss: 0.45489 | val_0_auc: 0.80529 |  0:02:30s\n",
      "epoch 61 | loss: 0.45226 | val_0_auc: 0.80398 |  0:02:33s\n",
      "epoch 62 | loss: 0.45029 | val_0_auc: 0.81279 |  0:02:36s\n",
      "epoch 63 | loss: 0.45091 | val_0_auc: 0.80669 |  0:02:38s\n",
      "epoch 64 | loss: 0.45085 | val_0_auc: 0.80341 |  0:02:41s\n",
      "epoch 65 | loss: 0.44963 | val_0_auc: 0.80458 |  0:02:44s\n",
      "epoch 66 | loss: 0.45078 | val_0_auc: 0.81242 |  0:02:47s\n",
      "epoch 67 | loss: 0.44949 | val_0_auc: 0.8047  |  0:02:49s\n",
      "epoch 68 | loss: 0.44839 | val_0_auc: 0.80745 |  0:02:51s\n",
      "epoch 69 | loss: 0.44479 | val_0_auc: 0.8032  |  0:02:53s\n",
      "epoch 70 | loss: 0.44828 | val_0_auc: 0.80424 |  0:02:56s\n",
      "epoch 71 | loss: 0.44403 | val_0_auc: 0.8112  |  0:02:58s\n",
      "epoch 72 | loss: 0.44498 | val_0_auc: 0.80885 |  0:03:00s\n",
      "epoch 73 | loss: 0.44976 | val_0_auc: 0.80496 |  0:03:03s\n",
      "epoch 74 | loss: 0.44319 | val_0_auc: 0.81139 |  0:03:05s\n",
      "epoch 75 | loss: 0.44699 | val_0_auc: 0.81365 |  0:03:07s\n",
      "epoch 76 | loss: 0.44396 | val_0_auc: 0.81395 |  0:03:10s\n",
      "epoch 77 | loss: 0.44517 | val_0_auc: 0.80946 |  0:03:12s\n",
      "epoch 78 | loss: 0.44215 | val_0_auc: 0.81516 |  0:03:14s\n",
      "epoch 79 | loss: 0.44475 | val_0_auc: 0.81103 |  0:03:17s\n",
      "epoch 80 | loss: 0.44275 | val_0_auc: 0.80938 |  0:03:21s\n",
      "epoch 81 | loss: 0.44192 | val_0_auc: 0.8095  |  0:03:25s\n",
      "epoch 82 | loss: 0.44147 | val_0_auc: 0.81609 |  0:03:30s\n",
      "epoch 83 | loss: 0.43946 | val_0_auc: 0.81071 |  0:03:34s\n",
      "epoch 84 | loss: 0.44458 | val_0_auc: 0.81406 |  0:03:42s\n",
      "epoch 85 | loss: 0.44269 | val_0_auc: 0.81401 |  0:03:47s\n",
      "epoch 86 | loss: 0.44064 | val_0_auc: 0.81466 |  0:03:50s\n",
      "epoch 87 | loss: 0.44145 | val_0_auc: 0.81277 |  0:03:53s\n",
      "epoch 88 | loss: 0.44368 | val_0_auc: 0.81549 |  0:03:55s\n",
      "epoch 89 | loss: 0.44051 | val_0_auc: 0.81709 |  0:03:58s\n",
      "epoch 90 | loss: 0.43963 | val_0_auc: 0.81576 |  0:04:01s\n",
      "epoch 91 | loss: 0.43627 | val_0_auc: 0.81718 |  0:04:04s\n",
      "epoch 92 | loss: 0.43846 | val_0_auc: 0.8149  |  0:04:07s\n",
      "epoch 93 | loss: 0.43791 | val_0_auc: 0.8193  |  0:04:10s\n",
      "epoch 94 | loss: 0.43857 | val_0_auc: 0.81942 |  0:04:12s\n",
      "epoch 95 | loss: 0.43868 | val_0_auc: 0.81826 |  0:04:14s\n",
      "epoch 96 | loss: 0.43705 | val_0_auc: 0.82184 |  0:04:17s\n",
      "epoch 97 | loss: 0.43785 | val_0_auc: 0.82194 |  0:04:19s\n",
      "epoch 98 | loss: 0.43716 | val_0_auc: 0.82147 |  0:04:23s\n",
      "epoch 99 | loss: 0.43816 | val_0_auc: 0.81799 |  0:04:25s\n",
      "epoch 100| loss: 0.43662 | val_0_auc: 0.82139 |  0:04:28s\n",
      "epoch 101| loss: 0.43452 | val_0_auc: 0.82123 |  0:04:31s\n",
      "epoch 102| loss: 0.4313  | val_0_auc: 0.81843 |  0:04:34s\n",
      "epoch 103| loss: 0.43365 | val_0_auc: 0.82239 |  0:04:37s\n",
      "epoch 104| loss: 0.43527 | val_0_auc: 0.81971 |  0:04:40s\n",
      "epoch 105| loss: 0.43508 | val_0_auc: 0.82739 |  0:04:43s\n",
      "epoch 106| loss: 0.43112 | val_0_auc: 0.8224  |  0:04:46s\n",
      "epoch 107| loss: 0.4294  | val_0_auc: 0.8224  |  0:04:48s\n",
      "epoch 108| loss: 0.43039 | val_0_auc: 0.82133 |  0:04:50s\n",
      "epoch 109| loss: 0.42944 | val_0_auc: 0.82519 |  0:04:54s\n",
      "epoch 110| loss: 0.43026 | val_0_auc: 0.82152 |  0:04:57s\n",
      "epoch 111| loss: 0.43187 | val_0_auc: 0.82091 |  0:05:00s\n",
      "epoch 112| loss: 0.42889 | val_0_auc: 0.81966 |  0:05:03s\n",
      "epoch 113| loss: 0.43032 | val_0_auc: 0.82613 |  0:05:06s\n",
      "epoch 114| loss: 0.42969 | val_0_auc: 0.82421 |  0:05:09s\n",
      "epoch 115| loss: 0.4281  | val_0_auc: 0.82378 |  0:05:12s\n",
      "epoch 116| loss: 0.43179 | val_0_auc: 0.82504 |  0:05:15s\n",
      "epoch 117| loss: 0.42862 | val_0_auc: 0.82726 |  0:05:19s\n",
      "epoch 118| loss: 0.42944 | val_0_auc: 0.82284 |  0:05:22s\n",
      "epoch 119| loss: 0.42637 | val_0_auc: 0.82333 |  0:05:26s\n",
      "epoch 120| loss: 0.42969 | val_0_auc: 0.82318 |  0:05:29s\n",
      "epoch 121| loss: 0.43048 | val_0_auc: 0.82356 |  0:05:33s\n",
      "epoch 122| loss: 0.42754 | val_0_auc: 0.82178 |  0:05:36s\n",
      "epoch 123| loss: 0.42606 | val_0_auc: 0.8222  |  0:05:39s\n",
      "epoch 124| loss: 0.42928 | val_0_auc: 0.82142 |  0:05:42s\n",
      "epoch 125| loss: 0.42986 | val_0_auc: 0.82166 |  0:05:45s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 105 and best_val_0_auc = 0.82739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.65346 | val_0_auc: 0.57269 |  0:00:02s\n",
      "epoch 1  | loss: 0.53267 | val_0_auc: 0.6288  |  0:00:03s\n",
      "epoch 2  | loss: 0.49031 | val_0_auc: 0.67723 |  0:00:05s\n",
      "epoch 3  | loss: 0.47149 | val_0_auc: 0.70894 |  0:00:07s\n",
      "epoch 4  | loss: 0.45988 | val_0_auc: 0.74523 |  0:00:09s\n",
      "epoch 5  | loss: 0.44843 | val_0_auc: 0.76066 |  0:00:10s\n",
      "epoch 6  | loss: 0.43997 | val_0_auc: 0.77726 |  0:00:12s\n",
      "epoch 7  | loss: 0.4341  | val_0_auc: 0.78946 |  0:00:14s\n",
      "epoch 8  | loss: 0.42907 | val_0_auc: 0.79866 |  0:00:17s\n",
      "epoch 9  | loss: 0.4251  | val_0_auc: 0.80349 |  0:00:19s\n",
      "epoch 10 | loss: 0.4187  | val_0_auc: 0.81062 |  0:00:21s\n",
      "epoch 11 | loss: 0.41323 | val_0_auc: 0.81624 |  0:00:23s\n",
      "epoch 12 | loss: 0.41134 | val_0_auc: 0.82006 |  0:00:26s\n",
      "epoch 13 | loss: 0.40651 | val_0_auc: 0.82361 |  0:00:28s\n",
      "epoch 14 | loss: 0.40242 | val_0_auc: 0.82674 |  0:00:30s\n",
      "epoch 15 | loss: 0.39869 | val_0_auc: 0.83041 |  0:00:33s\n",
      "epoch 16 | loss: 0.39656 | val_0_auc: 0.83293 |  0:00:35s\n",
      "epoch 17 | loss: 0.39556 | val_0_auc: 0.83623 |  0:00:37s\n",
      "epoch 18 | loss: 0.39253 | val_0_auc: 0.83914 |  0:00:39s\n",
      "epoch 19 | loss: 0.39186 | val_0_auc: 0.83969 |  0:00:42s\n",
      "epoch 20 | loss: 0.38884 | val_0_auc: 0.84106 |  0:00:44s\n",
      "epoch 21 | loss: 0.38563 | val_0_auc: 0.84172 |  0:00:47s\n",
      "epoch 22 | loss: 0.38502 | val_0_auc: 0.84282 |  0:00:50s\n",
      "epoch 23 | loss: 0.38458 | val_0_auc: 0.84321 |  0:00:52s\n",
      "epoch 24 | loss: 0.38278 | val_0_auc: 0.84452 |  0:00:54s\n",
      "epoch 25 | loss: 0.38106 | val_0_auc: 0.84453 |  0:00:56s\n",
      "epoch 26 | loss: 0.3787  | val_0_auc: 0.8455  |  0:00:59s\n",
      "epoch 27 | loss: 0.37591 | val_0_auc: 0.84841 |  0:01:01s\n",
      "epoch 28 | loss: 0.37456 | val_0_auc: 0.84817 |  0:01:03s\n",
      "epoch 29 | loss: 0.37272 | val_0_auc: 0.84989 |  0:01:05s\n",
      "epoch 30 | loss: 0.37454 | val_0_auc: 0.85115 |  0:01:08s\n",
      "epoch 31 | loss: 0.36975 | val_0_auc: 0.85189 |  0:01:10s\n",
      "epoch 32 | loss: 0.36889 | val_0_auc: 0.852   |  0:01:12s\n",
      "epoch 33 | loss: 0.36805 | val_0_auc: 0.85345 |  0:01:14s\n",
      "epoch 34 | loss: 0.36583 | val_0_auc: 0.85454 |  0:01:16s\n",
      "epoch 35 | loss: 0.36561 | val_0_auc: 0.85412 |  0:01:18s\n",
      "epoch 36 | loss: 0.36555 | val_0_auc: 0.85521 |  0:01:20s\n",
      "epoch 37 | loss: 0.36445 | val_0_auc: 0.85521 |  0:01:22s\n",
      "epoch 38 | loss: 0.36146 | val_0_auc: 0.85603 |  0:01:24s\n",
      "epoch 39 | loss: 0.36275 | val_0_auc: 0.85632 |  0:01:26s\n",
      "epoch 40 | loss: 0.36129 | val_0_auc: 0.85653 |  0:01:28s\n",
      "epoch 41 | loss: 0.35832 | val_0_auc: 0.85851 |  0:01:30s\n",
      "epoch 42 | loss: 0.35695 | val_0_auc: 0.85973 |  0:01:32s\n",
      "epoch 43 | loss: 0.35936 | val_0_auc: 0.8582  |  0:01:34s\n",
      "epoch 44 | loss: 0.35718 | val_0_auc: 0.85908 |  0:01:36s\n",
      "epoch 45 | loss: 0.35453 | val_0_auc: 0.8588  |  0:01:38s\n",
      "epoch 46 | loss: 0.35529 | val_0_auc: 0.85882 |  0:01:41s\n",
      "epoch 47 | loss: 0.3531  | val_0_auc: 0.8601  |  0:01:43s\n",
      "epoch 48 | loss: 0.35391 | val_0_auc: 0.86108 |  0:01:45s\n",
      "epoch 49 | loss: 0.35349 | val_0_auc: 0.86227 |  0:01:48s\n",
      "epoch 50 | loss: 0.35123 | val_0_auc: 0.86223 |  0:01:50s\n",
      "epoch 51 | loss: 0.35128 | val_0_auc: 0.86292 |  0:01:53s\n",
      "epoch 52 | loss: 0.35247 | val_0_auc: 0.86499 |  0:01:55s\n",
      "epoch 53 | loss: 0.34848 | val_0_auc: 0.865   |  0:01:58s\n",
      "epoch 54 | loss: 0.34598 | val_0_auc: 0.86525 |  0:02:00s\n",
      "epoch 55 | loss: 0.34697 | val_0_auc: 0.86556 |  0:02:02s\n",
      "epoch 56 | loss: 0.34915 | val_0_auc: 0.86502 |  0:02:04s\n",
      "epoch 57 | loss: 0.34695 | val_0_auc: 0.86614 |  0:02:06s\n",
      "epoch 58 | loss: 0.34482 | val_0_auc: 0.86659 |  0:02:07s\n",
      "epoch 59 | loss: 0.34541 | val_0_auc: 0.86522 |  0:02:09s\n",
      "epoch 60 | loss: 0.34317 | val_0_auc: 0.86639 |  0:02:12s\n",
      "epoch 61 | loss: 0.34309 | val_0_auc: 0.86767 |  0:02:14s\n",
      "epoch 62 | loss: 0.33938 | val_0_auc: 0.8677  |  0:02:16s\n",
      "epoch 63 | loss: 0.34005 | val_0_auc: 0.86729 |  0:02:19s\n",
      "epoch 64 | loss: 0.34079 | val_0_auc: 0.8683  |  0:02:21s\n",
      "epoch 65 | loss: 0.33683 | val_0_auc: 0.86744 |  0:02:24s\n",
      "epoch 66 | loss: 0.33892 | val_0_auc: 0.8685  |  0:02:25s\n",
      "epoch 67 | loss: 0.33634 | val_0_auc: 0.86873 |  0:02:27s\n",
      "epoch 68 | loss: 0.33863 | val_0_auc: 0.87034 |  0:02:29s\n",
      "epoch 69 | loss: 0.33786 | val_0_auc: 0.87037 |  0:02:30s\n",
      "epoch 70 | loss: 0.33638 | val_0_auc: 0.8724  |  0:02:32s\n",
      "epoch 71 | loss: 0.33505 | val_0_auc: 0.87214 |  0:02:34s\n",
      "epoch 72 | loss: 0.33504 | val_0_auc: 0.87201 |  0:02:36s\n",
      "epoch 73 | loss: 0.33143 | val_0_auc: 0.87244 |  0:02:39s\n",
      "epoch 74 | loss: 0.3306  | val_0_auc: 0.87335 |  0:02:41s\n",
      "epoch 75 | loss: 0.33152 | val_0_auc: 0.87303 |  0:02:44s\n",
      "epoch 76 | loss: 0.33051 | val_0_auc: 0.87303 |  0:02:46s\n",
      "epoch 77 | loss: 0.33069 | val_0_auc: 0.8723  |  0:02:49s\n",
      "epoch 78 | loss: 0.32955 | val_0_auc: 0.87238 |  0:02:51s\n",
      "epoch 79 | loss: 0.32938 | val_0_auc: 0.87413 |  0:02:53s\n",
      "epoch 80 | loss: 0.32777 | val_0_auc: 0.87366 |  0:02:55s\n",
      "epoch 81 | loss: 0.328   | val_0_auc: 0.87371 |  0:02:56s\n",
      "epoch 82 | loss: 0.3259  | val_0_auc: 0.87427 |  0:02:58s\n",
      "epoch 83 | loss: 0.32524 | val_0_auc: 0.87524 |  0:03:00s\n",
      "epoch 84 | loss: 0.32683 | val_0_auc: 0.87638 |  0:03:02s\n",
      "epoch 85 | loss: 0.3245  | val_0_auc: 0.87612 |  0:03:04s\n",
      "epoch 86 | loss: 0.32435 | val_0_auc: 0.87743 |  0:03:06s\n",
      "epoch 87 | loss: 0.32328 | val_0_auc: 0.87615 |  0:03:08s\n",
      "epoch 88 | loss: 0.32328 | val_0_auc: 0.87736 |  0:03:10s\n",
      "epoch 89 | loss: 0.32182 | val_0_auc: 0.87709 |  0:03:12s\n",
      "epoch 90 | loss: 0.31987 | val_0_auc: 0.87821 |  0:03:14s\n",
      "epoch 91 | loss: 0.32286 | val_0_auc: 0.87765 |  0:03:16s\n",
      "epoch 92 | loss: 0.31993 | val_0_auc: 0.87721 |  0:03:18s\n",
      "epoch 93 | loss: 0.31866 | val_0_auc: 0.87782 |  0:03:20s\n",
      "epoch 94 | loss: 0.31929 | val_0_auc: 0.87788 |  0:03:22s\n",
      "epoch 95 | loss: 0.3196  | val_0_auc: 0.87759 |  0:03:24s\n",
      "epoch 96 | loss: 0.32006 | val_0_auc: 0.87852 |  0:03:27s\n",
      "epoch 97 | loss: 0.31786 | val_0_auc: 0.87911 |  0:03:29s\n",
      "epoch 98 | loss: 0.31806 | val_0_auc: 0.87968 |  0:03:32s\n",
      "epoch 99 | loss: 0.31766 | val_0_auc: 0.88021 |  0:03:34s\n",
      "epoch 100| loss: 0.3174  | val_0_auc: 0.87839 |  0:03:37s\n",
      "epoch 101| loss: 0.31571 | val_0_auc: 0.8793  |  0:03:39s\n",
      "epoch 102| loss: 0.31453 | val_0_auc: 0.88137 |  0:03:42s\n",
      "epoch 103| loss: 0.31479 | val_0_auc: 0.8808  |  0:03:44s\n",
      "epoch 104| loss: 0.31405 | val_0_auc: 0.88106 |  0:03:48s\n",
      "epoch 105| loss: 0.31325 | val_0_auc: 0.88115 |  0:03:50s\n",
      "epoch 106| loss: 0.31238 | val_0_auc: 0.88075 |  0:03:57s\n",
      "epoch 107| loss: 0.31201 | val_0_auc: 0.87921 |  0:04:00s\n",
      "epoch 108| loss: 0.3115  | val_0_auc: 0.87951 |  0:04:02s\n",
      "epoch 109| loss: 0.31303 | val_0_auc: 0.88121 |  0:04:05s\n",
      "epoch 110| loss: 0.31167 | val_0_auc: 0.88177 |  0:04:07s\n",
      "epoch 111| loss: 0.30821 | val_0_auc: 0.88219 |  0:04:09s\n",
      "epoch 112| loss: 0.30847 | val_0_auc: 0.88218 |  0:04:11s\n",
      "epoch 113| loss: 0.30883 | val_0_auc: 0.88237 |  0:04:14s\n",
      "epoch 114| loss: 0.31002 | val_0_auc: 0.88289 |  0:04:16s\n",
      "epoch 115| loss: 0.30788 | val_0_auc: 0.8836  |  0:04:18s\n",
      "epoch 116| loss: 0.31089 | val_0_auc: 0.88214 |  0:04:20s\n",
      "epoch 117| loss: 0.30592 | val_0_auc: 0.88201 |  0:04:22s\n",
      "epoch 118| loss: 0.30656 | val_0_auc: 0.88315 |  0:04:24s\n",
      "epoch 119| loss: 0.30655 | val_0_auc: 0.88334 |  0:04:26s\n",
      "epoch 120| loss: 0.30758 | val_0_auc: 0.88349 |  0:04:27s\n",
      "epoch 121| loss: 0.30697 | val_0_auc: 0.88258 |  0:04:29s\n",
      "epoch 122| loss: 0.30373 | val_0_auc: 0.88421 |  0:04:31s\n",
      "epoch 123| loss: 0.30337 | val_0_auc: 0.885   |  0:04:32s\n",
      "epoch 124| loss: 0.30334 | val_0_auc: 0.88384 |  0:04:34s\n",
      "epoch 125| loss: 0.30375 | val_0_auc: 0.88432 |  0:04:36s\n",
      "epoch 126| loss: 0.3036  | val_0_auc: 0.88411 |  0:04:38s\n",
      "epoch 127| loss: 0.30447 | val_0_auc: 0.88292 |  0:04:40s\n",
      "epoch 128| loss: 0.30586 | val_0_auc: 0.88434 |  0:04:42s\n",
      "epoch 129| loss: 0.30262 | val_0_auc: 0.88355 |  0:04:44s\n",
      "epoch 130| loss: 0.30287 | val_0_auc: 0.88473 |  0:04:46s\n",
      "epoch 131| loss: 0.30066 | val_0_auc: 0.8852  |  0:04:48s\n",
      "epoch 132| loss: 0.30013 | val_0_auc: 0.88494 |  0:04:51s\n",
      "epoch 133| loss: 0.30118 | val_0_auc: 0.88604 |  0:04:53s\n",
      "epoch 134| loss: 0.30042 | val_0_auc: 0.88568 |  0:04:55s\n",
      "epoch 135| loss: 0.29995 | val_0_auc: 0.88604 |  0:04:58s\n",
      "epoch 136| loss: 0.29777 | val_0_auc: 0.88592 |  0:05:00s\n",
      "epoch 137| loss: 0.30007 | val_0_auc: 0.88625 |  0:05:02s\n",
      "epoch 138| loss: 0.29627 | val_0_auc: 0.88666 |  0:05:04s\n",
      "epoch 139| loss: 0.29735 | val_0_auc: 0.88699 |  0:05:06s\n",
      "epoch 140| loss: 0.29732 | val_0_auc: 0.88769 |  0:05:09s\n",
      "epoch 141| loss: 0.29584 | val_0_auc: 0.88668 |  0:05:12s\n",
      "epoch 142| loss: 0.29933 | val_0_auc: 0.88835 |  0:05:15s\n",
      "epoch 143| loss: 0.29547 | val_0_auc: 0.88773 |  0:05:18s\n",
      "epoch 144| loss: 0.29391 | val_0_auc: 0.88765 |  0:05:20s\n",
      "epoch 145| loss: 0.29427 | val_0_auc: 0.88729 |  0:05:22s\n",
      "epoch 146| loss: 0.29451 | val_0_auc: 0.88781 |  0:05:24s\n",
      "epoch 147| loss: 0.2957  | val_0_auc: 0.88715 |  0:05:26s\n",
      "epoch 148| loss: 0.29262 | val_0_auc: 0.88748 |  0:05:28s\n",
      "epoch 149| loss: 0.29359 | val_0_auc: 0.88875 |  0:05:29s\n",
      "epoch 150| loss: 0.2936  | val_0_auc: 0.89041 |  0:05:31s\n",
      "epoch 151| loss: 0.29274 | val_0_auc: 0.88842 |  0:05:33s\n",
      "epoch 152| loss: 0.29143 | val_0_auc: 0.88924 |  0:05:35s\n",
      "epoch 153| loss: 0.28965 | val_0_auc: 0.88894 |  0:05:37s\n",
      "epoch 154| loss: 0.29206 | val_0_auc: 0.88845 |  0:05:39s\n",
      "epoch 155| loss: 0.29079 | val_0_auc: 0.8901  |  0:05:41s\n",
      "epoch 156| loss: 0.28891 | val_0_auc: 0.88947 |  0:05:43s\n",
      "epoch 157| loss: 0.2863  | val_0_auc: 0.88987 |  0:05:45s\n",
      "epoch 158| loss: 0.28654 | val_0_auc: 0.88861 |  0:05:47s\n",
      "epoch 159| loss: 0.28856 | val_0_auc: 0.88843 |  0:05:49s\n",
      "epoch 160| loss: 0.28692 | val_0_auc: 0.88923 |  0:05:51s\n",
      "epoch 161| loss: 0.28594 | val_0_auc: 0.88922 |  0:05:53s\n",
      "epoch 162| loss: 0.28622 | val_0_auc: 0.88979 |  0:05:54s\n",
      "epoch 163| loss: 0.28657 | val_0_auc: 0.88901 |  0:05:56s\n",
      "epoch 164| loss: 0.28522 | val_0_auc: 0.88883 |  0:05:58s\n",
      "epoch 165| loss: 0.28469 | val_0_auc: 0.88917 |  0:05:59s\n",
      "epoch 166| loss: 0.28348 | val_0_auc: 0.88904 |  0:06:01s\n",
      "epoch 167| loss: 0.28671 | val_0_auc: 0.89018 |  0:06:03s\n",
      "epoch 168| loss: 0.28412 | val_0_auc: 0.89024 |  0:06:04s\n",
      "epoch 169| loss: 0.28631 | val_0_auc: 0.88854 |  0:06:06s\n",
      "epoch 170| loss: 0.2827  | val_0_auc: 0.88933 |  0:06:08s\n",
      "\n",
      "Early stopping occurred at epoch 170 with best_epoch = 150 and best_val_0_auc = 0.89041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67729 | val_0_auc: 0.52781 |  0:00:00s\n",
      "epoch 1  | loss: 0.62104 | val_0_auc: 0.56745 |  0:00:01s\n",
      "epoch 2  | loss: 0.58075 | val_0_auc: 0.61719 |  0:00:02s\n",
      "epoch 3  | loss: 0.54949 | val_0_auc: 0.65278 |  0:00:03s\n",
      "epoch 4  | loss: 0.52078 | val_0_auc: 0.68952 |  0:00:04s\n",
      "epoch 5  | loss: 0.49701 | val_0_auc: 0.72278 |  0:00:05s\n",
      "epoch 6  | loss: 0.4721  | val_0_auc: 0.74875 |  0:00:06s\n",
      "epoch 7  | loss: 0.45594 | val_0_auc: 0.77203 |  0:00:07s\n",
      "epoch 8  | loss: 0.44093 | val_0_auc: 0.78885 |  0:00:09s\n",
      "epoch 9  | loss: 0.42663 | val_0_auc: 0.80274 |  0:00:10s\n",
      "epoch 10 | loss: 0.41597 | val_0_auc: 0.81612 |  0:00:11s\n",
      "epoch 11 | loss: 0.40322 | val_0_auc: 0.82522 |  0:00:12s\n",
      "epoch 12 | loss: 0.39299 | val_0_auc: 0.83079 |  0:00:13s\n",
      "epoch 13 | loss: 0.38355 | val_0_auc: 0.8374  |  0:00:14s\n",
      "epoch 14 | loss: 0.37554 | val_0_auc: 0.84343 |  0:00:15s\n",
      "epoch 15 | loss: 0.36689 | val_0_auc: 0.84774 |  0:00:16s\n",
      "epoch 16 | loss: 0.36455 | val_0_auc: 0.85161 |  0:00:17s\n",
      "epoch 17 | loss: 0.3581  | val_0_auc: 0.8565  |  0:00:18s\n",
      "epoch 18 | loss: 0.35018 | val_0_auc: 0.86018 |  0:00:20s\n",
      "epoch 19 | loss: 0.34491 | val_0_auc: 0.86237 |  0:00:21s\n",
      "epoch 20 | loss: 0.34174 | val_0_auc: 0.86589 |  0:00:22s\n",
      "epoch 21 | loss: 0.33519 | val_0_auc: 0.86731 |  0:00:23s\n",
      "epoch 22 | loss: 0.33043 | val_0_auc: 0.8705  |  0:00:24s\n",
      "epoch 23 | loss: 0.32693 | val_0_auc: 0.87231 |  0:00:25s\n",
      "epoch 24 | loss: 0.32276 | val_0_auc: 0.87323 |  0:00:26s\n",
      "epoch 25 | loss: 0.32    | val_0_auc: 0.87465 |  0:00:27s\n",
      "epoch 26 | loss: 0.31729 | val_0_auc: 0.87677 |  0:00:28s\n",
      "epoch 27 | loss: 0.31441 | val_0_auc: 0.87818 |  0:00:29s\n",
      "epoch 28 | loss: 0.31005 | val_0_auc: 0.87934 |  0:00:29s\n",
      "epoch 29 | loss: 0.3101  | val_0_auc: 0.88039 |  0:00:30s\n",
      "epoch 30 | loss: 0.30904 | val_0_auc: 0.88172 |  0:00:31s\n",
      "epoch 31 | loss: 0.30278 | val_0_auc: 0.88135 |  0:00:33s\n",
      "epoch 32 | loss: 0.30372 | val_0_auc: 0.88383 |  0:00:34s\n",
      "epoch 33 | loss: 0.29948 | val_0_auc: 0.88597 |  0:00:34s\n",
      "epoch 34 | loss: 0.29596 | val_0_auc: 0.88637 |  0:00:35s\n",
      "epoch 35 | loss: 0.29451 | val_0_auc: 0.88601 |  0:00:36s\n",
      "epoch 36 | loss: 0.29367 | val_0_auc: 0.88696 |  0:00:37s\n",
      "epoch 37 | loss: 0.29306 | val_0_auc: 0.88817 |  0:00:38s\n",
      "epoch 38 | loss: 0.28872 | val_0_auc: 0.88787 |  0:00:39s\n",
      "epoch 39 | loss: 0.28487 | val_0_auc: 0.88885 |  0:00:40s\n",
      "epoch 40 | loss: 0.28624 | val_0_auc: 0.88976 |  0:00:41s\n",
      "epoch 41 | loss: 0.28414 | val_0_auc: 0.88992 |  0:00:42s\n",
      "epoch 42 | loss: 0.2822  | val_0_auc: 0.89061 |  0:00:43s\n",
      "epoch 43 | loss: 0.2802  | val_0_auc: 0.89156 |  0:00:44s\n",
      "epoch 44 | loss: 0.27963 | val_0_auc: 0.89145 |  0:00:45s\n",
      "epoch 45 | loss: 0.27797 | val_0_auc: 0.89199 |  0:00:46s\n",
      "epoch 46 | loss: 0.27674 | val_0_auc: 0.89181 |  0:00:47s\n",
      "epoch 47 | loss: 0.2774  | val_0_auc: 0.89151 |  0:00:48s\n",
      "epoch 48 | loss: 0.27477 | val_0_auc: 0.89153 |  0:00:49s\n",
      "epoch 49 | loss: 0.27203 | val_0_auc: 0.89262 |  0:00:50s\n",
      "epoch 50 | loss: 0.27179 | val_0_auc: 0.89248 |  0:00:51s\n",
      "epoch 51 | loss: 0.27155 | val_0_auc: 0.89232 |  0:00:52s\n",
      "epoch 52 | loss: 0.27101 | val_0_auc: 0.89325 |  0:00:53s\n",
      "epoch 53 | loss: 0.26695 | val_0_auc: 0.89371 |  0:00:54s\n",
      "epoch 54 | loss: 0.2666  | val_0_auc: 0.89341 |  0:00:55s\n",
      "epoch 55 | loss: 0.26553 | val_0_auc: 0.89322 |  0:00:56s\n",
      "epoch 56 | loss: 0.26433 | val_0_auc: 0.89413 |  0:00:57s\n",
      "epoch 57 | loss: 0.26367 | val_0_auc: 0.89406 |  0:00:58s\n",
      "epoch 58 | loss: 0.26104 | val_0_auc: 0.89463 |  0:00:59s\n",
      "epoch 59 | loss: 0.26118 | val_0_auc: 0.89366 |  0:01:00s\n",
      "epoch 60 | loss: 0.26051 | val_0_auc: 0.89384 |  0:01:01s\n",
      "epoch 61 | loss: 0.25889 | val_0_auc: 0.89363 |  0:01:02s\n",
      "epoch 62 | loss: 0.2596  | val_0_auc: 0.89375 |  0:01:03s\n",
      "epoch 63 | loss: 0.25459 | val_0_auc: 0.89394 |  0:01:04s\n",
      "epoch 64 | loss: 0.25575 | val_0_auc: 0.89253 |  0:01:05s\n",
      "epoch 65 | loss: 0.25255 | val_0_auc: 0.89368 |  0:01:06s\n",
      "epoch 66 | loss: 0.24996 | val_0_auc: 0.89288 |  0:01:07s\n",
      "epoch 67 | loss: 0.25131 | val_0_auc: 0.89336 |  0:01:08s\n",
      "epoch 68 | loss: 0.25234 | val_0_auc: 0.89256 |  0:01:09s\n",
      "epoch 69 | loss: 0.25097 | val_0_auc: 0.89286 |  0:01:10s\n",
      "epoch 70 | loss: 0.24729 | val_0_auc: 0.8924  |  0:01:11s\n",
      "epoch 71 | loss: 0.24953 | val_0_auc: 0.89171 |  0:01:12s\n",
      "epoch 72 | loss: 0.24582 | val_0_auc: 0.89172 |  0:01:13s\n",
      "epoch 73 | loss: 0.24965 | val_0_auc: 0.89197 |  0:01:14s\n",
      "epoch 74 | loss: 0.24714 | val_0_auc: 0.89167 |  0:01:15s\n",
      "epoch 75 | loss: 0.24367 | val_0_auc: 0.89302 |  0:01:16s\n",
      "epoch 76 | loss: 0.2429  | val_0_auc: 0.89117 |  0:01:17s\n",
      "epoch 77 | loss: 0.24324 | val_0_auc: 0.89178 |  0:01:18s\n",
      "epoch 78 | loss: 0.24446 | val_0_auc: 0.8925  |  0:01:19s\n",
      "\n",
      "Early stopping occurred at epoch 78 with best_epoch = 58 and best_val_0_auc = 0.89463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.14989 | val_0_auc: 0.58075 |  0:00:03s\n",
      "epoch 1  | loss: 0.85401 | val_0_auc: 0.60675 |  0:00:08s\n",
      "epoch 2  | loss: 0.77682 | val_0_auc: 0.65383 |  0:00:13s\n",
      "epoch 3  | loss: 0.73849 | val_0_auc: 0.69747 |  0:00:18s\n",
      "epoch 4  | loss: 0.71929 | val_0_auc: 0.73061 |  0:00:22s\n",
      "epoch 5  | loss: 0.7003  | val_0_auc: 0.74588 |  0:00:26s\n",
      "epoch 6  | loss: 0.68629 | val_0_auc: 0.7625  |  0:00:30s\n",
      "epoch 7  | loss: 0.67943 | val_0_auc: 0.77203 |  0:00:33s\n",
      "epoch 8  | loss: 0.66709 | val_0_auc: 0.78051 |  0:00:37s\n",
      "epoch 9  | loss: 0.66091 | val_0_auc: 0.7851  |  0:00:40s\n",
      "epoch 10 | loss: 0.65478 | val_0_auc: 0.79366 |  0:00:45s\n",
      "epoch 11 | loss: 0.64978 | val_0_auc: 0.7979  |  0:00:49s\n",
      "epoch 12 | loss: 0.64577 | val_0_auc: 0.80207 |  0:00:52s\n",
      "epoch 13 | loss: 0.63718 | val_0_auc: 0.80546 |  0:00:56s\n",
      "epoch 14 | loss: 0.63471 | val_0_auc: 0.80739 |  0:00:59s\n",
      "epoch 15 | loss: 0.63309 | val_0_auc: 0.81199 |  0:01:03s\n",
      "epoch 16 | loss: 0.62915 | val_0_auc: 0.81448 |  0:01:07s\n",
      "epoch 17 | loss: 0.62894 | val_0_auc: 0.81617 |  0:01:11s\n",
      "epoch 18 | loss: 0.62277 | val_0_auc: 0.81731 |  0:01:16s\n",
      "epoch 19 | loss: 0.61901 | val_0_auc: 0.81821 |  0:01:21s\n",
      "epoch 20 | loss: 0.61629 | val_0_auc: 0.82122 |  0:01:25s\n",
      "epoch 21 | loss: 0.61511 | val_0_auc: 0.82185 |  0:01:29s\n",
      "epoch 22 | loss: 0.61301 | val_0_auc: 0.82086 |  0:01:32s\n",
      "epoch 23 | loss: 0.61348 | val_0_auc: 0.82318 |  0:01:35s\n",
      "epoch 24 | loss: 0.61108 | val_0_auc: 0.82168 |  0:01:39s\n",
      "epoch 25 | loss: 0.60553 | val_0_auc: 0.82583 |  0:01:42s\n",
      "epoch 26 | loss: 0.6067  | val_0_auc: 0.83078 |  0:01:46s\n",
      "epoch 27 | loss: 0.60402 | val_0_auc: 0.83128 |  0:01:49s\n",
      "epoch 28 | loss: 0.60197 | val_0_auc: 0.83101 |  0:01:53s\n",
      "epoch 29 | loss: 0.5969  | val_0_auc: 0.83367 |  0:01:56s\n",
      "epoch 30 | loss: 0.59814 | val_0_auc: 0.83407 |  0:02:00s\n",
      "epoch 31 | loss: 0.59535 | val_0_auc: 0.83669 |  0:02:03s\n",
      "epoch 32 | loss: 0.59244 | val_0_auc: 0.83899 |  0:02:07s\n",
      "epoch 33 | loss: 0.59063 | val_0_auc: 0.84089 |  0:02:10s\n",
      "epoch 34 | loss: 0.58776 | val_0_auc: 0.84273 |  0:02:14s\n",
      "epoch 35 | loss: 0.58593 | val_0_auc: 0.84259 |  0:02:17s\n",
      "epoch 36 | loss: 0.58493 | val_0_auc: 0.84419 |  0:02:21s\n",
      "epoch 37 | loss: 0.58325 | val_0_auc: 0.84457 |  0:02:24s\n",
      "epoch 38 | loss: 0.57957 | val_0_auc: 0.84645 |  0:02:28s\n",
      "epoch 39 | loss: 0.57861 | val_0_auc: 0.8474  |  0:02:31s\n",
      "epoch 40 | loss: 0.57662 | val_0_auc: 0.84716 |  0:02:34s\n",
      "epoch 41 | loss: 0.57858 | val_0_auc: 0.85066 |  0:02:38s\n",
      "epoch 42 | loss: 0.57366 | val_0_auc: 0.85251 |  0:02:41s\n",
      "epoch 43 | loss: 0.57088 | val_0_auc: 0.85183 |  0:02:45s\n",
      "epoch 44 | loss: 0.57137 | val_0_auc: 0.85358 |  0:02:48s\n",
      "epoch 45 | loss: 0.56909 | val_0_auc: 0.85311 |  0:02:51s\n",
      "epoch 46 | loss: 0.5637  | val_0_auc: 0.8533  |  0:02:55s\n",
      "epoch 47 | loss: 0.56345 | val_0_auc: 0.85493 |  0:02:58s\n",
      "epoch 48 | loss: 0.56083 | val_0_auc: 0.8552  |  0:03:02s\n",
      "epoch 49 | loss: 0.56042 | val_0_auc: 0.85669 |  0:03:05s\n",
      "epoch 50 | loss: 0.55816 | val_0_auc: 0.85889 |  0:03:09s\n",
      "epoch 51 | loss: 0.55788 | val_0_auc: 0.85865 |  0:03:12s\n",
      "epoch 52 | loss: 0.55312 | val_0_auc: 0.85998 |  0:03:16s\n",
      "epoch 53 | loss: 0.5556  | val_0_auc: 0.86073 |  0:03:19s\n",
      "epoch 54 | loss: 0.55098 | val_0_auc: 0.86086 |  0:03:23s\n",
      "epoch 55 | loss: 0.55239 | val_0_auc: 0.86101 |  0:03:26s\n",
      "epoch 56 | loss: 0.54815 | val_0_auc: 0.86277 |  0:03:30s\n",
      "epoch 57 | loss: 0.55105 | val_0_auc: 0.862   |  0:03:33s\n",
      "epoch 58 | loss: 0.54778 | val_0_auc: 0.86298 |  0:03:37s\n",
      "epoch 59 | loss: 0.54431 | val_0_auc: 0.86329 |  0:03:40s\n",
      "epoch 60 | loss: 0.54438 | val_0_auc: 0.86417 |  0:03:43s\n",
      "epoch 61 | loss: 0.54225 | val_0_auc: 0.86522 |  0:03:47s\n",
      "epoch 62 | loss: 0.54148 | val_0_auc: 0.86573 |  0:03:50s\n",
      "epoch 63 | loss: 0.54119 | val_0_auc: 0.86365 |  0:03:54s\n",
      "epoch 64 | loss: 0.53904 | val_0_auc: 0.86535 |  0:03:57s\n",
      "epoch 65 | loss: 0.53695 | val_0_auc: 0.86512 |  0:04:00s\n",
      "epoch 66 | loss: 0.53546 | val_0_auc: 0.86568 |  0:04:04s\n",
      "epoch 67 | loss: 0.5366  | val_0_auc: 0.8641  |  0:04:07s\n",
      "epoch 68 | loss: 0.53466 | val_0_auc: 0.86514 |  0:04:11s\n",
      "epoch 69 | loss: 0.53148 | val_0_auc: 0.86471 |  0:04:15s\n",
      "epoch 70 | loss: 0.5312  | val_0_auc: 0.86602 |  0:04:18s\n",
      "epoch 71 | loss: 0.53071 | val_0_auc: 0.86621 |  0:04:21s\n",
      "epoch 72 | loss: 0.52858 | val_0_auc: 0.86728 |  0:04:25s\n",
      "epoch 73 | loss: 0.52824 | val_0_auc: 0.86691 |  0:04:28s\n",
      "epoch 74 | loss: 0.52323 | val_0_auc: 0.86656 |  0:04:32s\n",
      "epoch 75 | loss: 0.52733 | val_0_auc: 0.86796 |  0:04:35s\n",
      "epoch 76 | loss: 0.52661 | val_0_auc: 0.86682 |  0:04:38s\n",
      "epoch 77 | loss: 0.52159 | val_0_auc: 0.86748 |  0:04:42s\n",
      "epoch 78 | loss: 0.5215  | val_0_auc: 0.86772 |  0:04:45s\n",
      "epoch 79 | loss: 0.52203 | val_0_auc: 0.86778 |  0:04:49s\n",
      "epoch 80 | loss: 0.51914 | val_0_auc: 0.86791 |  0:04:52s\n",
      "epoch 81 | loss: 0.52195 | val_0_auc: 0.86821 |  0:04:55s\n",
      "epoch 82 | loss: 0.51638 | val_0_auc: 0.86795 |  0:04:59s\n",
      "epoch 83 | loss: 0.5147  | val_0_auc: 0.86881 |  0:05:02s\n",
      "epoch 84 | loss: 0.51733 | val_0_auc: 0.86889 |  0:05:06s\n",
      "epoch 85 | loss: 0.51368 | val_0_auc: 0.86953 |  0:05:09s\n",
      "epoch 86 | loss: 0.5132  | val_0_auc: 0.86901 |  0:05:13s\n",
      "epoch 87 | loss: 0.51564 | val_0_auc: 0.86812 |  0:05:16s\n",
      "epoch 88 | loss: 0.51151 | val_0_auc: 0.86934 |  0:05:20s\n",
      "epoch 89 | loss: 0.51075 | val_0_auc: 0.87047 |  0:05:23s\n",
      "epoch 90 | loss: 0.50833 | val_0_auc: 0.87015 |  0:05:27s\n",
      "epoch 91 | loss: 0.51111 | val_0_auc: 0.87011 |  0:05:30s\n",
      "epoch 92 | loss: 0.50722 | val_0_auc: 0.87165 |  0:05:33s\n",
      "epoch 93 | loss: 0.50697 | val_0_auc: 0.87011 |  0:05:37s\n",
      "epoch 94 | loss: 0.50407 | val_0_auc: 0.87159 |  0:05:40s\n",
      "epoch 95 | loss: 0.50472 | val_0_auc: 0.87198 |  0:05:47s\n",
      "epoch 96 | loss: 0.50368 | val_0_auc: 0.87037 |  0:05:53s\n",
      "epoch 97 | loss: 0.50486 | val_0_auc: 0.87116 |  0:05:58s\n",
      "epoch 98 | loss: 0.50125 | val_0_auc: 0.87197 |  0:06:01s\n",
      "epoch 99 | loss: 0.50234 | val_0_auc: 0.87123 |  0:06:05s\n",
      "epoch 100| loss: 0.49828 | val_0_auc: 0.87252 |  0:06:11s\n",
      "epoch 101| loss: 0.49884 | val_0_auc: 0.8729  |  0:06:16s\n",
      "epoch 102| loss: 0.49751 | val_0_auc: 0.87336 |  0:06:22s\n",
      "epoch 103| loss: 0.49758 | val_0_auc: 0.87273 |  0:06:28s\n",
      "epoch 104| loss: 0.49557 | val_0_auc: 0.87347 |  0:06:32s\n",
      "epoch 105| loss: 0.49548 | val_0_auc: 0.87284 |  0:06:35s\n",
      "epoch 106| loss: 0.49389 | val_0_auc: 0.87399 |  0:06:39s\n",
      "epoch 107| loss: 0.49369 | val_0_auc: 0.87337 |  0:06:42s\n",
      "epoch 108| loss: 0.49371 | val_0_auc: 0.87333 |  0:06:46s\n",
      "epoch 109| loss: 0.49218 | val_0_auc: 0.87283 |  0:06:49s\n",
      "epoch 110| loss: 0.49085 | val_0_auc: 0.8741  |  0:06:52s\n",
      "epoch 111| loss: 0.48989 | val_0_auc: 0.87309 |  0:06:56s\n",
      "epoch 112| loss: 0.48851 | val_0_auc: 0.87422 |  0:06:59s\n",
      "epoch 113| loss: 0.48841 | val_0_auc: 0.87288 |  0:07:03s\n",
      "epoch 114| loss: 0.48533 | val_0_auc: 0.87363 |  0:07:06s\n",
      "epoch 115| loss: 0.48587 | val_0_auc: 0.87288 |  0:07:10s\n",
      "epoch 116| loss: 0.48538 | val_0_auc: 0.87402 |  0:07:13s\n",
      "epoch 117| loss: 0.48487 | val_0_auc: 0.87422 |  0:07:16s\n",
      "epoch 118| loss: 0.48453 | val_0_auc: 0.87414 |  0:07:20s\n",
      "epoch 119| loss: 0.48143 | val_0_auc: 0.87357 |  0:07:23s\n",
      "epoch 120| loss: 0.48281 | val_0_auc: 0.8747  |  0:07:26s\n",
      "epoch 121| loss: 0.48186 | val_0_auc: 0.87478 |  0:07:30s\n",
      "epoch 122| loss: 0.47967 | val_0_auc: 0.87503 |  0:07:33s\n",
      "epoch 123| loss: 0.48059 | val_0_auc: 0.87448 |  0:07:37s\n",
      "epoch 124| loss: 0.47823 | val_0_auc: 0.87521 |  0:07:40s\n",
      "epoch 125| loss: 0.4762  | val_0_auc: 0.87406 |  0:07:44s\n",
      "epoch 126| loss: 0.47634 | val_0_auc: 0.87448 |  0:07:48s\n",
      "epoch 127| loss: 0.47492 | val_0_auc: 0.87639 |  0:07:53s\n",
      "epoch 128| loss: 0.47472 | val_0_auc: 0.87516 |  0:07:59s\n",
      "epoch 129| loss: 0.47603 | val_0_auc: 0.87575 |  0:08:05s\n",
      "epoch 130| loss: 0.47449 | val_0_auc: 0.87633 |  0:08:11s\n",
      "epoch 131| loss: 0.47064 | val_0_auc: 0.87613 |  0:08:16s\n",
      "epoch 132| loss: 0.47503 | val_0_auc: 0.87581 |  0:08:21s\n",
      "epoch 133| loss: 0.47286 | val_0_auc: 0.8747  |  0:08:26s\n",
      "epoch 134| loss: 0.47099 | val_0_auc: 0.87406 |  0:08:32s\n",
      "epoch 135| loss: 0.47001 | val_0_auc: 0.87492 |  0:08:36s\n",
      "epoch 136| loss: 0.46956 | val_0_auc: 0.87489 |  0:08:40s\n",
      "epoch 137| loss: 0.46921 | val_0_auc: 0.87614 |  0:08:44s\n",
      "epoch 138| loss: 0.46522 | val_0_auc: 0.87562 |  0:08:47s\n",
      "epoch 139| loss: 0.46567 | val_0_auc: 0.87675 |  0:08:51s\n",
      "epoch 140| loss: 0.46709 | val_0_auc: 0.8772  |  0:08:54s\n",
      "epoch 141| loss: 0.46454 | val_0_auc: 0.8768  |  0:08:58s\n",
      "epoch 142| loss: 0.46546 | val_0_auc: 0.87784 |  0:09:01s\n",
      "epoch 143| loss: 0.46304 | val_0_auc: 0.87886 |  0:09:05s\n",
      "epoch 144| loss: 0.46349 | val_0_auc: 0.87849 |  0:09:08s\n",
      "epoch 145| loss: 0.46367 | val_0_auc: 0.87829 |  0:09:11s\n",
      "epoch 146| loss: 0.4612  | val_0_auc: 0.87823 |  0:09:15s\n",
      "epoch 147| loss: 0.4625  | val_0_auc: 0.87745 |  0:09:22s\n",
      "epoch 148| loss: 0.45988 | val_0_auc: 0.87774 |  0:09:29s\n",
      "epoch 149| loss: 0.45979 | val_0_auc: 0.8794  |  0:09:35s\n",
      "epoch 150| loss: 0.46189 | val_0_auc: 0.87916 |  0:09:39s\n",
      "epoch 151| loss: 0.46144 | val_0_auc: 0.87933 |  0:09:43s\n",
      "epoch 152| loss: 0.45991 | val_0_auc: 0.87988 |  0:09:46s\n",
      "epoch 153| loss: 0.45773 | val_0_auc: 0.88009 |  0:09:50s\n",
      "epoch 154| loss: 0.45797 | val_0_auc: 0.87905 |  0:09:53s\n",
      "epoch 155| loss: 0.45644 | val_0_auc: 0.87884 |  0:09:57s\n",
      "epoch 156| loss: 0.45722 | val_0_auc: 0.87983 |  0:10:00s\n",
      "epoch 157| loss: 0.45372 | val_0_auc: 0.87943 |  0:10:04s\n",
      "epoch 158| loss: 0.45752 | val_0_auc: 0.87927 |  0:10:08s\n",
      "epoch 159| loss: 0.45624 | val_0_auc: 0.88    |  0:10:11s\n",
      "epoch 160| loss: 0.45618 | val_0_auc: 0.87898 |  0:10:15s\n",
      "epoch 161| loss: 0.45276 | val_0_auc: 0.88068 |  0:10:18s\n",
      "epoch 162| loss: 0.45513 | val_0_auc: 0.88042 |  0:10:22s\n",
      "epoch 163| loss: 0.45488 | val_0_auc: 0.88101 |  0:10:25s\n",
      "epoch 164| loss: 0.45383 | val_0_auc: 0.88106 |  0:10:29s\n",
      "epoch 165| loss: 0.45187 | val_0_auc: 0.88111 |  0:10:32s\n",
      "epoch 166| loss: 0.45575 | val_0_auc: 0.88151 |  0:10:36s\n",
      "epoch 167| loss: 0.45181 | val_0_auc: 0.8815  |  0:10:40s\n",
      "epoch 168| loss: 0.44947 | val_0_auc: 0.88256 |  0:10:49s\n",
      "epoch 169| loss: 0.44987 | val_0_auc: 0.8813  |  0:10:53s\n",
      "epoch 170| loss: 0.44852 | val_0_auc: 0.88317 |  0:10:56s\n",
      "epoch 171| loss: 0.44722 | val_0_auc: 0.88261 |  0:11:00s\n",
      "epoch 172| loss: 0.44903 | val_0_auc: 0.8814  |  0:11:03s\n",
      "epoch 173| loss: 0.44583 | val_0_auc: 0.88164 |  0:11:08s\n",
      "epoch 174| loss: 0.44692 | val_0_auc: 0.88057 |  0:11:11s\n",
      "epoch 175| loss: 0.44767 | val_0_auc: 0.88098 |  0:11:15s\n",
      "epoch 176| loss: 0.4475  | val_0_auc: 0.88191 |  0:11:18s\n",
      "epoch 177| loss: 0.44701 | val_0_auc: 0.88302 |  0:11:21s\n",
      "epoch 178| loss: 0.44485 | val_0_auc: 0.88177 |  0:11:25s\n",
      "epoch 179| loss: 0.44585 | val_0_auc: 0.88101 |  0:11:28s\n",
      "epoch 180| loss: 0.44108 | val_0_auc: 0.8817  |  0:11:32s\n",
      "epoch 181| loss: 0.4438  | val_0_auc: 0.88149 |  0:11:35s\n",
      "epoch 182| loss: 0.44129 | val_0_auc: 0.88041 |  0:11:39s\n",
      "epoch 183| loss: 0.44391 | val_0_auc: 0.88077 |  0:11:42s\n",
      "epoch 184| loss: 0.44254 | val_0_auc: 0.88161 |  0:11:46s\n",
      "epoch 185| loss: 0.44215 | val_0_auc: 0.8817  |  0:11:49s\n",
      "epoch 186| loss: 0.44159 | val_0_auc: 0.8801  |  0:11:53s\n",
      "epoch 187| loss: 0.44082 | val_0_auc: 0.88114 |  0:12:01s\n",
      "epoch 188| loss: 0.44253 | val_0_auc: 0.88151 |  0:12:07s\n",
      "epoch 189| loss: 0.4396  | val_0_auc: 0.88293 |  0:12:12s\n",
      "epoch 190| loss: 0.44193 | val_0_auc: 0.88373 |  0:12:17s\n",
      "epoch 191| loss: 0.44098 | val_0_auc: 0.88254 |  0:12:22s\n",
      "epoch 192| loss: 0.44042 | val_0_auc: 0.88259 |  0:12:26s\n",
      "epoch 193| loss: 0.43973 | val_0_auc: 0.88202 |  0:12:29s\n",
      "epoch 194| loss: 0.438   | val_0_auc: 0.88219 |  0:12:33s\n",
      "epoch 195| loss: 0.43959 | val_0_auc: 0.88171 |  0:12:36s\n",
      "epoch 196| loss: 0.43858 | val_0_auc: 0.88204 |  0:12:40s\n",
      "epoch 197| loss: 0.43553 | val_0_auc: 0.88319 |  0:12:43s\n",
      "epoch 198| loss: 0.43993 | val_0_auc: 0.88291 |  0:12:46s\n",
      "epoch 199| loss: 0.43715 | val_0_auc: 0.88328 |  0:12:50s\n",
      "epoch 200| loss: 0.43566 | val_0_auc: 0.88194 |  0:12:53s\n",
      "epoch 201| loss: 0.43582 | val_0_auc: 0.88268 |  0:12:57s\n",
      "epoch 202| loss: 0.43592 | val_0_auc: 0.88208 |  0:13:00s\n",
      "epoch 203| loss: 0.43205 | val_0_auc: 0.88298 |  0:13:04s\n",
      "epoch 204| loss: 0.43328 | val_0_auc: 0.88256 |  0:13:07s\n",
      "epoch 205| loss: 0.43439 | val_0_auc: 0.88306 |  0:13:10s\n",
      "epoch 206| loss: 0.43219 | val_0_auc: 0.88261 |  0:13:14s\n",
      "epoch 207| loss: 0.4325  | val_0_auc: 0.8825  |  0:13:17s\n",
      "epoch 208| loss: 0.43238 | val_0_auc: 0.88262 |  0:13:20s\n",
      "epoch 209| loss: 0.43319 | val_0_auc: 0.88236 |  0:13:24s\n",
      "epoch 210| loss: 0.4313  | val_0_auc: 0.88291 |  0:13:27s\n",
      "\n",
      "Early stopping occurred at epoch 210 with best_epoch = 190 and best_val_0_auc = 0.88373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.62745 | val_0_auc: 0.58815 |  0:00:02s\n",
      "epoch 1  | loss: 0.53791 | val_0_auc: 0.68749 |  0:00:05s\n",
      "epoch 2  | loss: 0.48873 | val_0_auc: 0.77518 |  0:00:08s\n",
      "epoch 3  | loss: 0.46334 | val_0_auc: 0.75333 |  0:00:11s\n",
      "epoch 4  | loss: 0.46009 | val_0_auc: 0.78421 |  0:00:14s\n",
      "epoch 5  | loss: 0.4369  | val_0_auc: 0.79977 |  0:00:16s\n",
      "epoch 6  | loss: 0.43854 | val_0_auc: 0.79795 |  0:00:19s\n",
      "epoch 7  | loss: 0.43799 | val_0_auc: 0.79342 |  0:00:22s\n",
      "epoch 8  | loss: 0.43344 | val_0_auc: 0.81515 |  0:00:25s\n",
      "epoch 9  | loss: 0.42231 | val_0_auc: 0.82304 |  0:00:27s\n",
      "epoch 10 | loss: 0.41467 | val_0_auc: 0.83368 |  0:00:30s\n",
      "epoch 11 | loss: 0.40804 | val_0_auc: 0.83966 |  0:00:33s\n",
      "epoch 12 | loss: 0.39458 | val_0_auc: 0.84913 |  0:00:36s\n",
      "epoch 13 | loss: 0.38767 | val_0_auc: 0.84777 |  0:00:38s\n",
      "epoch 14 | loss: 0.38362 | val_0_auc: 0.85903 |  0:00:41s\n",
      "epoch 15 | loss: 0.37155 | val_0_auc: 0.86298 |  0:00:44s\n",
      "epoch 16 | loss: 0.36801 | val_0_auc: 0.86713 |  0:00:47s\n",
      "epoch 17 | loss: 0.35926 | val_0_auc: 0.86802 |  0:00:49s\n",
      "epoch 18 | loss: 0.35582 | val_0_auc: 0.86862 |  0:00:52s\n",
      "epoch 19 | loss: 0.36166 | val_0_auc: 0.86575 |  0:00:55s\n",
      "epoch 20 | loss: 0.36866 | val_0_auc: 0.86941 |  0:00:58s\n",
      "epoch 21 | loss: 0.36449 | val_0_auc: 0.87202 |  0:01:01s\n",
      "epoch 22 | loss: 0.35464 | val_0_auc: 0.87597 |  0:01:03s\n",
      "epoch 23 | loss: 0.35009 | val_0_auc: 0.87514 |  0:01:06s\n",
      "epoch 24 | loss: 0.34636 | val_0_auc: 0.87811 |  0:01:09s\n",
      "epoch 25 | loss: 0.34583 | val_0_auc: 0.87607 |  0:01:12s\n",
      "epoch 26 | loss: 0.347   | val_0_auc: 0.8787  |  0:01:14s\n",
      "epoch 27 | loss: 0.3412  | val_0_auc: 0.88104 |  0:01:17s\n",
      "epoch 28 | loss: 0.34017 | val_0_auc: 0.88525 |  0:01:20s\n",
      "epoch 29 | loss: 0.34075 | val_0_auc: 0.88713 |  0:01:22s\n",
      "epoch 30 | loss: 0.33547 | val_0_auc: 0.88682 |  0:01:25s\n",
      "epoch 31 | loss: 0.33551 | val_0_auc: 0.88411 |  0:01:28s\n",
      "epoch 32 | loss: 0.33728 | val_0_auc: 0.88084 |  0:01:31s\n",
      "epoch 33 | loss: 0.3379  | val_0_auc: 0.88724 |  0:01:33s\n",
      "epoch 34 | loss: 0.33371 | val_0_auc: 0.88555 |  0:01:36s\n",
      "epoch 35 | loss: 0.33141 | val_0_auc: 0.89037 |  0:01:39s\n",
      "epoch 36 | loss: 0.3276  | val_0_auc: 0.88425 |  0:01:41s\n",
      "epoch 37 | loss: 0.33866 | val_0_auc: 0.88305 |  0:01:44s\n",
      "epoch 38 | loss: 0.32768 | val_0_auc: 0.8813  |  0:01:47s\n",
      "epoch 39 | loss: 0.32074 | val_0_auc: 0.88925 |  0:01:50s\n",
      "epoch 40 | loss: 0.32134 | val_0_auc: 0.89006 |  0:01:52s\n",
      "epoch 41 | loss: 0.3184  | val_0_auc: 0.88874 |  0:01:55s\n",
      "epoch 42 | loss: 0.31494 | val_0_auc: 0.89444 |  0:01:58s\n",
      "epoch 43 | loss: 0.31611 | val_0_auc: 0.89366 |  0:02:01s\n",
      "epoch 44 | loss: 0.325   | val_0_auc: 0.88305 |  0:02:03s\n",
      "epoch 45 | loss: 0.32112 | val_0_auc: 0.89171 |  0:02:06s\n",
      "epoch 46 | loss: 0.31512 | val_0_auc: 0.89513 |  0:02:09s\n",
      "epoch 47 | loss: 0.32139 | val_0_auc: 0.89376 |  0:02:12s\n",
      "epoch 48 | loss: 0.32485 | val_0_auc: 0.89347 |  0:02:14s\n",
      "epoch 49 | loss: 0.31359 | val_0_auc: 0.89562 |  0:02:17s\n",
      "epoch 50 | loss: 0.30617 | val_0_auc: 0.90073 |  0:02:20s\n",
      "epoch 51 | loss: 0.30325 | val_0_auc: 0.89823 |  0:02:23s\n",
      "epoch 52 | loss: 0.30448 | val_0_auc: 0.90091 |  0:02:25s\n",
      "epoch 53 | loss: 0.30251 | val_0_auc: 0.9031  |  0:02:28s\n",
      "epoch 54 | loss: 0.29837 | val_0_auc: 0.90382 |  0:02:31s\n",
      "epoch 55 | loss: 0.29719 | val_0_auc: 0.90615 |  0:02:33s\n",
      "epoch 56 | loss: 0.29653 | val_0_auc: 0.90709 |  0:02:36s\n",
      "epoch 57 | loss: 0.29653 | val_0_auc: 0.90306 |  0:02:39s\n",
      "epoch 58 | loss: 0.29905 | val_0_auc: 0.90786 |  0:02:42s\n",
      "epoch 59 | loss: 0.29518 | val_0_auc: 0.90884 |  0:02:45s\n",
      "epoch 60 | loss: 0.29318 | val_0_auc: 0.91098 |  0:02:47s\n",
      "epoch 61 | loss: 0.28933 | val_0_auc: 0.91057 |  0:02:50s\n",
      "epoch 62 | loss: 0.28675 | val_0_auc: 0.91098 |  0:02:53s\n",
      "epoch 63 | loss: 0.30197 | val_0_auc: 0.90774 |  0:02:56s\n",
      "epoch 64 | loss: 0.298   | val_0_auc: 0.91133 |  0:02:59s\n",
      "epoch 65 | loss: 0.29095 | val_0_auc: 0.91325 |  0:03:01s\n",
      "epoch 66 | loss: 0.28716 | val_0_auc: 0.91365 |  0:03:04s\n",
      "epoch 67 | loss: 0.28754 | val_0_auc: 0.91294 |  0:03:07s\n",
      "epoch 68 | loss: 0.28729 | val_0_auc: 0.91308 |  0:03:10s\n",
      "epoch 69 | loss: 0.2853  | val_0_auc: 0.91407 |  0:03:12s\n",
      "epoch 70 | loss: 0.29028 | val_0_auc: 0.90938 |  0:03:15s\n",
      "epoch 71 | loss: 0.28902 | val_0_auc: 0.91292 |  0:03:18s\n",
      "epoch 72 | loss: 0.28478 | val_0_auc: 0.91448 |  0:03:21s\n",
      "epoch 73 | loss: 0.28107 | val_0_auc: 0.91514 |  0:03:24s\n",
      "epoch 74 | loss: 0.28013 | val_0_auc: 0.91603 |  0:03:27s\n",
      "epoch 75 | loss: 0.27915 | val_0_auc: 0.91583 |  0:03:31s\n",
      "epoch 76 | loss: 0.27676 | val_0_auc: 0.91745 |  0:03:35s\n",
      "epoch 77 | loss: 0.27638 | val_0_auc: 0.91534 |  0:03:40s\n",
      "epoch 78 | loss: 0.28336 | val_0_auc: 0.91375 |  0:03:44s\n",
      "epoch 79 | loss: 0.28266 | val_0_auc: 0.91549 |  0:03:48s\n",
      "epoch 80 | loss: 0.27964 | val_0_auc: 0.91527 |  0:03:52s\n",
      "epoch 81 | loss: 0.27938 | val_0_auc: 0.91708 |  0:03:56s\n",
      "epoch 82 | loss: 0.27635 | val_0_auc: 0.91706 |  0:03:59s\n",
      "epoch 83 | loss: 0.27681 | val_0_auc: 0.91748 |  0:04:02s\n",
      "epoch 84 | loss: 0.2763  | val_0_auc: 0.91822 |  0:04:05s\n",
      "epoch 85 | loss: 0.27412 | val_0_auc: 0.91867 |  0:04:09s\n",
      "epoch 86 | loss: 0.27356 | val_0_auc: 0.91856 |  0:04:13s\n",
      "epoch 87 | loss: 0.27384 | val_0_auc: 0.91838 |  0:04:17s\n",
      "epoch 88 | loss: 0.27356 | val_0_auc: 0.92057 |  0:04:20s\n",
      "epoch 89 | loss: 0.27243 | val_0_auc: 0.92036 |  0:04:23s\n",
      "epoch 90 | loss: 0.26948 | val_0_auc: 0.91926 |  0:04:26s\n",
      "epoch 91 | loss: 0.27179 | val_0_auc: 0.91932 |  0:04:28s\n",
      "epoch 92 | loss: 0.27655 | val_0_auc: 0.91545 |  0:04:31s\n",
      "epoch 93 | loss: 0.27974 | val_0_auc: 0.91596 |  0:04:34s\n",
      "epoch 94 | loss: 0.28185 | val_0_auc: 0.91507 |  0:04:37s\n",
      "epoch 95 | loss: 0.27846 | val_0_auc: 0.91727 |  0:04:40s\n",
      "epoch 96 | loss: 0.27573 | val_0_auc: 0.91861 |  0:04:43s\n",
      "epoch 97 | loss: 0.2743  | val_0_auc: 0.91973 |  0:04:45s\n",
      "epoch 98 | loss: 0.27546 | val_0_auc: 0.91988 |  0:04:48s\n",
      "epoch 99 | loss: 0.27656 | val_0_auc: 0.9202  |  0:04:52s\n",
      "epoch 100| loss: 0.27362 | val_0_auc: 0.92045 |  0:04:55s\n",
      "epoch 101| loss: 0.27094 | val_0_auc: 0.92078 |  0:04:57s\n",
      "epoch 102| loss: 0.28063 | val_0_auc: 0.90803 |  0:05:01s\n",
      "epoch 103| loss: 0.3012  | val_0_auc: 0.89724 |  0:05:03s\n",
      "epoch 104| loss: 0.29907 | val_0_auc: 0.90345 |  0:05:06s\n",
      "epoch 105| loss: 0.2913  | val_0_auc: 0.90914 |  0:05:09s\n",
      "epoch 106| loss: 0.28503 | val_0_auc: 0.9129  |  0:05:12s\n",
      "epoch 107| loss: 0.27936 | val_0_auc: 0.91418 |  0:05:15s\n",
      "epoch 108| loss: 0.27728 | val_0_auc: 0.91734 |  0:05:17s\n",
      "epoch 109| loss: 0.27525 | val_0_auc: 0.91838 |  0:05:20s\n",
      "epoch 110| loss: 0.27338 | val_0_auc: 0.91944 |  0:05:23s\n",
      "epoch 111| loss: 0.27074 | val_0_auc: 0.91854 |  0:05:26s\n",
      "epoch 112| loss: 0.27235 | val_0_auc: 0.91942 |  0:05:29s\n",
      "epoch 113| loss: 0.27068 | val_0_auc: 0.92028 |  0:05:31s\n",
      "epoch 114| loss: 0.2705  | val_0_auc: 0.92088 |  0:05:34s\n",
      "epoch 115| loss: 0.27156 | val_0_auc: 0.91922 |  0:05:37s\n",
      "epoch 116| loss: 0.27266 | val_0_auc: 0.91952 |  0:05:40s\n",
      "epoch 117| loss: 0.27093 | val_0_auc: 0.9182  |  0:05:43s\n",
      "epoch 118| loss: 0.27971 | val_0_auc: 0.91203 |  0:05:46s\n",
      "epoch 119| loss: 0.27827 | val_0_auc: 0.91712 |  0:05:49s\n",
      "epoch 120| loss: 0.27274 | val_0_auc: 0.91817 |  0:05:52s\n",
      "epoch 121| loss: 0.2728  | val_0_auc: 0.91993 |  0:05:55s\n",
      "epoch 122| loss: 0.27032 | val_0_auc: 0.91617 |  0:05:57s\n",
      "epoch 123| loss: 0.27047 | val_0_auc: 0.91738 |  0:06:00s\n",
      "epoch 124| loss: 0.26861 | val_0_auc: 0.91987 |  0:06:03s\n",
      "epoch 125| loss: 0.26989 | val_0_auc: 0.91848 |  0:06:05s\n",
      "epoch 126| loss: 0.27066 | val_0_auc: 0.92012 |  0:06:08s\n",
      "epoch 127| loss: 0.2695  | val_0_auc: 0.91988 |  0:06:11s\n",
      "epoch 128| loss: 0.26786 | val_0_auc: 0.92076 |  0:06:14s\n",
      "epoch 129| loss: 0.26667 | val_0_auc: 0.92153 |  0:06:17s\n",
      "epoch 130| loss: 0.26575 | val_0_auc: 0.92158 |  0:06:19s\n",
      "epoch 131| loss: 0.26581 | val_0_auc: 0.92117 |  0:06:22s\n",
      "epoch 132| loss: 0.26577 | val_0_auc: 0.9205  |  0:06:25s\n",
      "epoch 133| loss: 0.26591 | val_0_auc: 0.92107 |  0:06:28s\n",
      "epoch 134| loss: 0.26397 | val_0_auc: 0.92069 |  0:06:31s\n",
      "epoch 135| loss: 0.2638  | val_0_auc: 0.91699 |  0:06:33s\n",
      "epoch 136| loss: 0.26544 | val_0_auc: 0.91746 |  0:06:36s\n",
      "epoch 137| loss: 0.26231 | val_0_auc: 0.91823 |  0:06:39s\n",
      "epoch 138| loss: 0.26097 | val_0_auc: 0.91979 |  0:06:42s\n",
      "epoch 139| loss: 0.25977 | val_0_auc: 0.92027 |  0:06:45s\n",
      "epoch 140| loss: 0.26131 | val_0_auc: 0.92051 |  0:06:49s\n",
      "epoch 141| loss: 0.25691 | val_0_auc: 0.92083 |  0:06:54s\n",
      "epoch 142| loss: 0.25825 | val_0_auc: 0.92103 |  0:07:00s\n",
      "epoch 143| loss: 0.25831 | val_0_auc: 0.92107 |  0:07:04s\n",
      "epoch 144| loss: 0.25544 | val_0_auc: 0.92167 |  0:07:09s\n",
      "epoch 145| loss: 0.25813 | val_0_auc: 0.92059 |  0:07:18s\n",
      "epoch 146| loss: 0.25558 | val_0_auc: 0.91995 |  0:07:23s\n",
      "epoch 147| loss: 0.25412 | val_0_auc: 0.91951 |  0:07:27s\n",
      "epoch 148| loss: 0.25632 | val_0_auc: 0.91813 |  0:07:30s\n",
      "epoch 149| loss: 0.26532 | val_0_auc: 0.91253 |  0:07:33s\n",
      "epoch 150| loss: 0.26645 | val_0_auc: 0.9074  |  0:07:37s\n",
      "epoch 151| loss: 0.26429 | val_0_auc: 0.90478 |  0:07:40s\n",
      "epoch 152| loss: 0.2657  | val_0_auc: 0.91387 |  0:07:43s\n",
      "epoch 153| loss: 0.26929 | val_0_auc: 0.91195 |  0:07:47s\n",
      "epoch 154| loss: 0.26891 | val_0_auc: 0.92119 |  0:07:51s\n",
      "epoch 155| loss: 0.26221 | val_0_auc: 0.92072 |  0:07:56s\n",
      "epoch 156| loss: 0.26118 | val_0_auc: 0.91944 |  0:08:00s\n",
      "epoch 157| loss: 0.25883 | val_0_auc: 0.91954 |  0:08:05s\n",
      "epoch 158| loss: 0.25418 | val_0_auc: 0.92003 |  0:08:09s\n",
      "epoch 159| loss: 0.25575 | val_0_auc: 0.9203  |  0:08:12s\n",
      "epoch 160| loss: 0.25398 | val_0_auc: 0.91805 |  0:08:15s\n",
      "epoch 161| loss: 0.25256 | val_0_auc: 0.91711 |  0:08:19s\n",
      "epoch 162| loss: 0.25808 | val_0_auc: 0.91735 |  0:08:23s\n",
      "epoch 163| loss: 0.25469 | val_0_auc: 0.91786 |  0:08:26s\n",
      "epoch 164| loss: 0.26673 | val_0_auc: 0.91366 |  0:08:29s\n",
      "\n",
      "Early stopping occurred at epoch 164 with best_epoch = 144 and best_val_0_auc = 0.92167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.63117 | val_0_auc: 0.49279 |  0:00:00s\n",
      "epoch 1  | loss: 0.50031 | val_0_auc: 0.63051 |  0:00:02s\n",
      "epoch 2  | loss: 0.4481  | val_0_auc: 0.72095 |  0:00:03s\n",
      "epoch 3  | loss: 0.4292  | val_0_auc: 0.75117 |  0:00:04s\n",
      "epoch 4  | loss: 0.41775 | val_0_auc: 0.79021 |  0:00:05s\n",
      "epoch 5  | loss: 0.4082  | val_0_auc: 0.83589 |  0:00:06s\n",
      "epoch 6  | loss: 0.3973  | val_0_auc: 0.83848 |  0:00:07s\n",
      "epoch 7  | loss: 0.38858 | val_0_auc: 0.85305 |  0:00:08s\n",
      "epoch 8  | loss: 0.37925 | val_0_auc: 0.85912 |  0:00:09s\n",
      "epoch 9  | loss: 0.37459 | val_0_auc: 0.86412 |  0:00:10s\n",
      "epoch 10 | loss: 0.3675  | val_0_auc: 0.86404 |  0:00:11s\n",
      "epoch 11 | loss: 0.36427 | val_0_auc: 0.86733 |  0:00:12s\n",
      "epoch 12 | loss: 0.36039 | val_0_auc: 0.8702  |  0:00:13s\n",
      "epoch 13 | loss: 0.35689 | val_0_auc: 0.86817 |  0:00:14s\n",
      "epoch 14 | loss: 0.35505 | val_0_auc: 0.87193 |  0:00:15s\n",
      "epoch 15 | loss: 0.3526  | val_0_auc: 0.87573 |  0:00:16s\n",
      "epoch 16 | loss: 0.35207 | val_0_auc: 0.87399 |  0:00:17s\n",
      "epoch 17 | loss: 0.34808 | val_0_auc: 0.87419 |  0:00:18s\n",
      "epoch 18 | loss: 0.34597 | val_0_auc: 0.87434 |  0:00:19s\n",
      "epoch 19 | loss: 0.34249 | val_0_auc: 0.86438 |  0:00:20s\n",
      "epoch 20 | loss: 0.34017 | val_0_auc: 0.86152 |  0:00:21s\n",
      "epoch 21 | loss: 0.33594 | val_0_auc: 0.88266 |  0:00:22s\n",
      "epoch 22 | loss: 0.33255 | val_0_auc: 0.89012 |  0:00:23s\n",
      "epoch 23 | loss: 0.33167 | val_0_auc: 0.88124 |  0:00:24s\n",
      "epoch 24 | loss: 0.33018 | val_0_auc: 0.89038 |  0:00:25s\n",
      "epoch 25 | loss: 0.32872 | val_0_auc: 0.89541 |  0:00:27s\n",
      "epoch 26 | loss: 0.32422 | val_0_auc: 0.89429 |  0:00:28s\n",
      "epoch 27 | loss: 0.32386 | val_0_auc: 0.89592 |  0:00:30s\n",
      "epoch 28 | loss: 0.32394 | val_0_auc: 0.89557 |  0:00:31s\n",
      "epoch 29 | loss: 0.32242 | val_0_auc: 0.89621 |  0:00:33s\n",
      "epoch 30 | loss: 0.32025 | val_0_auc: 0.8962  |  0:00:34s\n",
      "epoch 31 | loss: 0.31985 | val_0_auc: 0.89642 |  0:00:36s\n",
      "epoch 32 | loss: 0.31954 | val_0_auc: 0.89744 |  0:00:38s\n",
      "epoch 33 | loss: 0.3201  | val_0_auc: 0.8935  |  0:00:39s\n",
      "epoch 34 | loss: 0.31874 | val_0_auc: 0.89762 |  0:00:40s\n",
      "epoch 35 | loss: 0.32031 | val_0_auc: 0.89826 |  0:00:41s\n",
      "epoch 36 | loss: 0.31828 | val_0_auc: 0.89333 |  0:00:42s\n",
      "epoch 37 | loss: 0.31559 | val_0_auc: 0.90014 |  0:00:44s\n",
      "epoch 38 | loss: 0.3157  | val_0_auc: 0.89651 |  0:00:45s\n",
      "epoch 39 | loss: 0.31526 | val_0_auc: 0.90126 |  0:00:46s\n",
      "epoch 40 | loss: 0.31391 | val_0_auc: 0.89864 |  0:00:47s\n",
      "epoch 41 | loss: 0.31385 | val_0_auc: 0.89932 |  0:00:48s\n",
      "epoch 42 | loss: 0.31356 | val_0_auc: 0.89833 |  0:00:49s\n",
      "epoch 43 | loss: 0.3146  | val_0_auc: 0.89795 |  0:00:50s\n",
      "epoch 44 | loss: 0.31349 | val_0_auc: 0.89851 |  0:00:52s\n",
      "epoch 45 | loss: 0.31268 | val_0_auc: 0.89575 |  0:00:53s\n",
      "epoch 46 | loss: 0.31196 | val_0_auc: 0.90014 |  0:00:55s\n",
      "epoch 47 | loss: 0.31122 | val_0_auc: 0.90079 |  0:00:56s\n",
      "epoch 48 | loss: 0.30919 | val_0_auc: 0.90084 |  0:00:58s\n",
      "epoch 49 | loss: 0.30869 | val_0_auc: 0.90065 |  0:00:59s\n",
      "epoch 50 | loss: 0.31182 | val_0_auc: 0.8982  |  0:01:02s\n",
      "epoch 51 | loss: 0.30959 | val_0_auc: 0.89763 |  0:01:03s\n",
      "epoch 52 | loss: 0.30992 | val_0_auc: 0.89783 |  0:01:05s\n",
      "epoch 53 | loss: 0.3093  | val_0_auc: 0.89981 |  0:01:06s\n",
      "epoch 54 | loss: 0.31139 | val_0_auc: 0.8994  |  0:01:07s\n",
      "epoch 55 | loss: 0.30887 | val_0_auc: 0.90037 |  0:01:08s\n",
      "epoch 56 | loss: 0.30894 | val_0_auc: 0.89512 |  0:01:09s\n",
      "epoch 57 | loss: 0.30791 | val_0_auc: 0.89998 |  0:01:10s\n",
      "epoch 58 | loss: 0.30679 | val_0_auc: 0.89881 |  0:01:11s\n",
      "epoch 59 | loss: 0.307   | val_0_auc: 0.89836 |  0:01:12s\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.90126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.66797 | val_0_auc: 0.63971 |  0:00:01s\n",
      "epoch 1  | loss: 0.48327 | val_0_auc: 0.76635 |  0:00:02s\n",
      "epoch 2  | loss: 0.45312 | val_0_auc: 0.79535 |  0:00:03s\n",
      "epoch 3  | loss: 0.44069 | val_0_auc: 0.7916  |  0:00:04s\n",
      "epoch 4  | loss: 0.43146 | val_0_auc: 0.79438 |  0:00:05s\n",
      "epoch 5  | loss: 0.42629 | val_0_auc: 0.79564 |  0:00:06s\n",
      "epoch 6  | loss: 0.42068 | val_0_auc: 0.80848 |  0:00:07s\n",
      "epoch 7  | loss: 0.41655 | val_0_auc: 0.81651 |  0:00:08s\n",
      "epoch 8  | loss: 0.41393 | val_0_auc: 0.82355 |  0:00:09s\n",
      "epoch 9  | loss: 0.40681 | val_0_auc: 0.83453 |  0:00:10s\n",
      "epoch 10 | loss: 0.40272 | val_0_auc: 0.84224 |  0:00:11s\n",
      "epoch 11 | loss: 0.39999 | val_0_auc: 0.84448 |  0:00:12s\n",
      "epoch 12 | loss: 0.3939  | val_0_auc: 0.84915 |  0:00:14s\n",
      "epoch 13 | loss: 0.39272 | val_0_auc: 0.85664 |  0:00:15s\n",
      "epoch 14 | loss: 0.38812 | val_0_auc: 0.85875 |  0:00:16s\n",
      "epoch 15 | loss: 0.38473 | val_0_auc: 0.85667 |  0:00:18s\n",
      "epoch 16 | loss: 0.37943 | val_0_auc: 0.86311 |  0:00:19s\n",
      "epoch 17 | loss: 0.37742 | val_0_auc: 0.8657  |  0:00:20s\n",
      "epoch 18 | loss: 0.37301 | val_0_auc: 0.86531 |  0:00:21s\n",
      "epoch 19 | loss: 0.37567 | val_0_auc: 0.86429 |  0:00:22s\n",
      "epoch 20 | loss: 0.36877 | val_0_auc: 0.86667 |  0:00:23s\n",
      "epoch 21 | loss: 0.36572 | val_0_auc: 0.86812 |  0:00:25s\n",
      "epoch 22 | loss: 0.36501 | val_0_auc: 0.86749 |  0:00:26s\n",
      "epoch 23 | loss: 0.3608  | val_0_auc: 0.87111 |  0:00:27s\n",
      "epoch 24 | loss: 0.35855 | val_0_auc: 0.87041 |  0:00:29s\n",
      "epoch 25 | loss: 0.35638 | val_0_auc: 0.87233 |  0:00:30s\n",
      "epoch 26 | loss: 0.35328 | val_0_auc: 0.87518 |  0:00:31s\n",
      "epoch 27 | loss: 0.34898 | val_0_auc: 0.87634 |  0:00:32s\n",
      "epoch 28 | loss: 0.3468  | val_0_auc: 0.87753 |  0:00:33s\n",
      "epoch 29 | loss: 0.34281 | val_0_auc: 0.87791 |  0:00:34s\n",
      "epoch 30 | loss: 0.3404  | val_0_auc: 0.87958 |  0:00:35s\n",
      "epoch 31 | loss: 0.33808 | val_0_auc: 0.87947 |  0:00:36s\n",
      "epoch 32 | loss: 0.33511 | val_0_auc: 0.8803  |  0:00:37s\n",
      "epoch 33 | loss: 0.33143 | val_0_auc: 0.88191 |  0:00:38s\n",
      "epoch 34 | loss: 0.33161 | val_0_auc: 0.88381 |  0:00:40s\n",
      "epoch 35 | loss: 0.32745 | val_0_auc: 0.88503 |  0:00:41s\n",
      "epoch 36 | loss: 0.32672 | val_0_auc: 0.8851  |  0:00:42s\n",
      "epoch 37 | loss: 0.32607 | val_0_auc: 0.88495 |  0:00:43s\n",
      "epoch 38 | loss: 0.32625 | val_0_auc: 0.88578 |  0:00:44s\n",
      "epoch 39 | loss: 0.32337 | val_0_auc: 0.88532 |  0:00:45s\n",
      "epoch 40 | loss: 0.32276 | val_0_auc: 0.88867 |  0:00:46s\n",
      "epoch 41 | loss: 0.32314 | val_0_auc: 0.89044 |  0:00:47s\n",
      "epoch 42 | loss: 0.32242 | val_0_auc: 0.88857 |  0:00:48s\n",
      "epoch 43 | loss: 0.32363 | val_0_auc: 0.88946 |  0:00:49s\n",
      "epoch 44 | loss: 0.32187 | val_0_auc: 0.88969 |  0:00:50s\n",
      "epoch 45 | loss: 0.3209  | val_0_auc: 0.89058 |  0:00:51s\n",
      "epoch 46 | loss: 0.31952 | val_0_auc: 0.89006 |  0:00:53s\n",
      "epoch 47 | loss: 0.31857 | val_0_auc: 0.88998 |  0:00:54s\n",
      "epoch 48 | loss: 0.32024 | val_0_auc: 0.89403 |  0:00:55s\n",
      "epoch 49 | loss: 0.31674 | val_0_auc: 0.89349 |  0:00:57s\n",
      "epoch 50 | loss: 0.31537 | val_0_auc: 0.89281 |  0:00:59s\n",
      "epoch 51 | loss: 0.31458 | val_0_auc: 0.89103 |  0:01:00s\n",
      "epoch 52 | loss: 0.31526 | val_0_auc: 0.89302 |  0:01:02s\n",
      "epoch 53 | loss: 0.31372 | val_0_auc: 0.89372 |  0:01:03s\n",
      "epoch 54 | loss: 0.31524 | val_0_auc: 0.89114 |  0:01:04s\n",
      "epoch 55 | loss: 0.31784 | val_0_auc: 0.89162 |  0:01:05s\n",
      "epoch 56 | loss: 0.31784 | val_0_auc: 0.89668 |  0:01:06s\n",
      "epoch 57 | loss: 0.31344 | val_0_auc: 0.89754 |  0:01:07s\n",
      "epoch 58 | loss: 0.31756 | val_0_auc: 0.89734 |  0:01:08s\n",
      "epoch 59 | loss: 0.31361 | val_0_auc: 0.89569 |  0:01:09s\n",
      "epoch 60 | loss: 0.31179 | val_0_auc: 0.89483 |  0:01:10s\n",
      "epoch 61 | loss: 0.30717 | val_0_auc: 0.89856 |  0:01:11s\n",
      "epoch 62 | loss: 0.30643 | val_0_auc: 0.89991 |  0:01:12s\n",
      "epoch 63 | loss: 0.30846 | val_0_auc: 0.89657 |  0:01:13s\n",
      "epoch 64 | loss: 0.30692 | val_0_auc: 0.89881 |  0:01:14s\n",
      "epoch 65 | loss: 0.30745 | val_0_auc: 0.89826 |  0:01:15s\n",
      "epoch 66 | loss: 0.30607 | val_0_auc: 0.89714 |  0:01:16s\n",
      "epoch 67 | loss: 0.30991 | val_0_auc: 0.89559 |  0:01:17s\n",
      "epoch 68 | loss: 0.30922 | val_0_auc: 0.89677 |  0:01:19s\n",
      "epoch 69 | loss: 0.30983 | val_0_auc: 0.89651 |  0:01:20s\n",
      "epoch 70 | loss: 0.30746 | val_0_auc: 0.89727 |  0:01:21s\n",
      "epoch 71 | loss: 0.30884 | val_0_auc: 0.8973  |  0:01:22s\n",
      "epoch 72 | loss: 0.30657 | val_0_auc: 0.8995  |  0:01:23s\n",
      "epoch 73 | loss: 0.30379 | val_0_auc: 0.89957 |  0:01:24s\n",
      "epoch 74 | loss: 0.30474 | val_0_auc: 0.8984  |  0:01:25s\n",
      "epoch 75 | loss: 0.30354 | val_0_auc: 0.89841 |  0:01:26s\n",
      "epoch 76 | loss: 0.30371 | val_0_auc: 0.90015 |  0:01:27s\n",
      "epoch 77 | loss: 0.30479 | val_0_auc: 0.89793 |  0:01:29s\n",
      "epoch 78 | loss: 0.30146 | val_0_auc: 0.90053 |  0:01:31s\n",
      "epoch 79 | loss: 0.30332 | val_0_auc: 0.90096 |  0:01:32s\n",
      "epoch 80 | loss: 0.30061 | val_0_auc: 0.90225 |  0:01:33s\n",
      "epoch 81 | loss: 0.29872 | val_0_auc: 0.90004 |  0:01:34s\n",
      "epoch 82 | loss: 0.29901 | val_0_auc: 0.90399 |  0:01:35s\n",
      "epoch 83 | loss: 0.29885 | val_0_auc: 0.90397 |  0:01:36s\n",
      "epoch 84 | loss: 0.30203 | val_0_auc: 0.90288 |  0:01:38s\n",
      "epoch 85 | loss: 0.30728 | val_0_auc: 0.89711 |  0:01:39s\n",
      "epoch 86 | loss: 0.30427 | val_0_auc: 0.89966 |  0:01:40s\n",
      "epoch 87 | loss: 0.29878 | val_0_auc: 0.90157 |  0:01:41s\n",
      "epoch 88 | loss: 0.29866 | val_0_auc: 0.90217 |  0:01:42s\n",
      "epoch 89 | loss: 0.29686 | val_0_auc: 0.90277 |  0:01:44s\n",
      "epoch 90 | loss: 0.29787 | val_0_auc: 0.90082 |  0:01:45s\n",
      "epoch 91 | loss: 0.30018 | val_0_auc: 0.90211 |  0:01:46s\n",
      "epoch 92 | loss: 0.29832 | val_0_auc: 0.89882 |  0:01:47s\n",
      "epoch 93 | loss: 0.29797 | val_0_auc: 0.90119 |  0:01:48s\n",
      "epoch 94 | loss: 0.29458 | val_0_auc: 0.90327 |  0:01:50s\n",
      "epoch 95 | loss: 0.29393 | val_0_auc: 0.90339 |  0:01:51s\n",
      "epoch 96 | loss: 0.29276 | val_0_auc: 0.90663 |  0:01:53s\n",
      "epoch 97 | loss: 0.29306 | val_0_auc: 0.9073  |  0:01:54s\n",
      "epoch 98 | loss: 0.29127 | val_0_auc: 0.90635 |  0:01:55s\n",
      "epoch 99 | loss: 0.28988 | val_0_auc: 0.9074  |  0:01:56s\n",
      "epoch 100| loss: 0.28997 | val_0_auc: 0.90819 |  0:01:57s\n",
      "epoch 101| loss: 0.28964 | val_0_auc: 0.9087  |  0:01:59s\n",
      "epoch 102| loss: 0.29081 | val_0_auc: 0.90818 |  0:02:00s\n",
      "epoch 103| loss: 0.2894  | val_0_auc: 0.90951 |  0:02:01s\n",
      "epoch 104| loss: 0.28762 | val_0_auc: 0.90926 |  0:02:03s\n",
      "epoch 105| loss: 0.28834 | val_0_auc: 0.91065 |  0:02:04s\n",
      "epoch 106| loss: 0.28748 | val_0_auc: 0.91134 |  0:02:05s\n",
      "epoch 107| loss: 0.28823 | val_0_auc: 0.91036 |  0:02:06s\n",
      "epoch 108| loss: 0.28754 | val_0_auc: 0.91229 |  0:02:08s\n",
      "epoch 109| loss: 0.28744 | val_0_auc: 0.91282 |  0:02:09s\n",
      "epoch 110| loss: 0.28516 | val_0_auc: 0.91277 |  0:02:10s\n",
      "epoch 111| loss: 0.289   | val_0_auc: 0.90778 |  0:02:11s\n",
      "epoch 112| loss: 0.29384 | val_0_auc: 0.90901 |  0:02:12s\n",
      "epoch 113| loss: 0.28715 | val_0_auc: 0.90725 |  0:02:14s\n",
      "epoch 114| loss: 0.2968  | val_0_auc: 0.9091  |  0:02:15s\n",
      "epoch 115| loss: 0.29713 | val_0_auc: 0.91093 |  0:02:16s\n",
      "epoch 116| loss: 0.29353 | val_0_auc: 0.91079 |  0:02:17s\n",
      "epoch 117| loss: 0.29135 | val_0_auc: 0.91084 |  0:02:18s\n",
      "epoch 118| loss: 0.28971 | val_0_auc: 0.91271 |  0:02:20s\n",
      "epoch 119| loss: 0.28681 | val_0_auc: 0.91389 |  0:02:21s\n",
      "epoch 120| loss: 0.28726 | val_0_auc: 0.9147  |  0:02:22s\n",
      "epoch 121| loss: 0.28732 | val_0_auc: 0.91434 |  0:02:24s\n",
      "epoch 122| loss: 0.28907 | val_0_auc: 0.91387 |  0:02:25s\n",
      "epoch 123| loss: 0.29133 | val_0_auc: 0.91144 |  0:02:28s\n",
      "epoch 124| loss: 0.28807 | val_0_auc: 0.91478 |  0:02:31s\n",
      "epoch 125| loss: 0.2837  | val_0_auc: 0.91484 |  0:02:33s\n",
      "epoch 126| loss: 0.28214 | val_0_auc: 0.91525 |  0:02:34s\n",
      "epoch 127| loss: 0.28101 | val_0_auc: 0.91496 |  0:02:35s\n",
      "epoch 128| loss: 0.28372 | val_0_auc: 0.91524 |  0:02:36s\n",
      "epoch 129| loss: 0.28202 | val_0_auc: 0.91572 |  0:02:37s\n",
      "epoch 130| loss: 0.28057 | val_0_auc: 0.91651 |  0:02:38s\n",
      "epoch 131| loss: 0.27959 | val_0_auc: 0.91778 |  0:02:39s\n",
      "epoch 132| loss: 0.28112 | val_0_auc: 0.9165  |  0:02:41s\n",
      "epoch 133| loss: 0.28086 | val_0_auc: 0.91785 |  0:02:42s\n",
      "epoch 134| loss: 0.28128 | val_0_auc: 0.91648 |  0:02:43s\n",
      "epoch 135| loss: 0.27974 | val_0_auc: 0.91821 |  0:02:44s\n",
      "epoch 136| loss: 0.27912 | val_0_auc: 0.91792 |  0:02:45s\n",
      "epoch 137| loss: 0.27954 | val_0_auc: 0.91614 |  0:02:46s\n",
      "epoch 138| loss: 0.2818  | val_0_auc: 0.91523 |  0:02:47s\n",
      "epoch 139| loss: 0.27805 | val_0_auc: 0.91722 |  0:02:48s\n",
      "epoch 140| loss: 0.27992 | val_0_auc: 0.91691 |  0:02:49s\n",
      "epoch 141| loss: 0.28035 | val_0_auc: 0.91669 |  0:02:50s\n",
      "epoch 142| loss: 0.27843 | val_0_auc: 0.91728 |  0:02:51s\n",
      "epoch 143| loss: 0.27681 | val_0_auc: 0.91742 |  0:02:52s\n",
      "epoch 144| loss: 0.27757 | val_0_auc: 0.91681 |  0:02:53s\n",
      "epoch 145| loss: 0.27952 | val_0_auc: 0.91589 |  0:02:54s\n",
      "epoch 146| loss: 0.27782 | val_0_auc: 0.91678 |  0:02:55s\n",
      "epoch 147| loss: 0.27832 | val_0_auc: 0.91478 |  0:02:57s\n",
      "epoch 148| loss: 0.27962 | val_0_auc: 0.91634 |  0:02:58s\n",
      "epoch 149| loss: 0.2768  | val_0_auc: 0.91794 |  0:02:59s\n",
      "epoch 150| loss: 0.27627 | val_0_auc: 0.91782 |  0:03:00s\n",
      "epoch 151| loss: 0.27601 | val_0_auc: 0.91752 |  0:03:02s\n",
      "epoch 152| loss: 0.27705 | val_0_auc: 0.91814 |  0:03:03s\n",
      "epoch 153| loss: 0.27544 | val_0_auc: 0.91764 |  0:03:04s\n",
      "epoch 154| loss: 0.27546 | val_0_auc: 0.91771 |  0:03:05s\n",
      "epoch 155| loss: 0.2817  | val_0_auc: 0.91516 |  0:03:07s\n",
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 135 and best_val_0_auc = 0.91821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64533 | val_0_auc: 0.68581 |  0:00:01s\n",
      "epoch 1  | loss: 0.42853 | val_0_auc: 0.77452 |  0:00:02s\n",
      "epoch 2  | loss: 0.35902 | val_0_auc: 0.8149  |  0:00:03s\n",
      "epoch 3  | loss: 0.32873 | val_0_auc: 0.84747 |  0:00:05s\n",
      "epoch 4  | loss: 0.30772 | val_0_auc: 0.86807 |  0:00:06s\n",
      "epoch 5  | loss: 0.29604 | val_0_auc: 0.88514 |  0:00:07s\n",
      "epoch 6  | loss: 0.28733 | val_0_auc: 0.89166 |  0:00:08s\n",
      "epoch 7  | loss: 0.2787  | val_0_auc: 0.89687 |  0:00:09s\n",
      "epoch 8  | loss: 0.2748  | val_0_auc: 0.90134 |  0:00:11s\n",
      "epoch 9  | loss: 0.26987 | val_0_auc: 0.90359 |  0:00:12s\n",
      "epoch 10 | loss: 0.26447 | val_0_auc: 0.90368 |  0:00:13s\n",
      "epoch 11 | loss: 0.25976 | val_0_auc: 0.90638 |  0:00:14s\n",
      "epoch 12 | loss: 0.25611 | val_0_auc: 0.90583 |  0:00:15s\n",
      "epoch 13 | loss: 0.25461 | val_0_auc: 0.90606 |  0:00:16s\n",
      "epoch 14 | loss: 0.2485  | val_0_auc: 0.90686 |  0:00:18s\n",
      "epoch 15 | loss: 0.24581 | val_0_auc: 0.90717 |  0:00:19s\n",
      "epoch 16 | loss: 0.24539 | val_0_auc: 0.90387 |  0:00:20s\n",
      "epoch 17 | loss: 0.24665 | val_0_auc: 0.90425 |  0:00:21s\n",
      "epoch 18 | loss: 0.24231 | val_0_auc: 0.90434 |  0:00:22s\n",
      "epoch 19 | loss: 0.23884 | val_0_auc: 0.90469 |  0:00:24s\n",
      "epoch 20 | loss: 0.22965 | val_0_auc: 0.90406 |  0:00:25s\n",
      "epoch 21 | loss: 0.23095 | val_0_auc: 0.89999 |  0:00:26s\n",
      "epoch 22 | loss: 0.22707 | val_0_auc: 0.9019  |  0:00:27s\n",
      "epoch 23 | loss: 0.22786 | val_0_auc: 0.89652 |  0:00:28s\n",
      "epoch 24 | loss: 0.22529 | val_0_auc: 0.89802 |  0:00:30s\n",
      "epoch 25 | loss: 0.22122 | val_0_auc: 0.89781 |  0:00:31s\n",
      "epoch 26 | loss: 0.21836 | val_0_auc: 0.89096 |  0:00:32s\n",
      "epoch 27 | loss: 0.2148  | val_0_auc: 0.89378 |  0:00:33s\n",
      "epoch 28 | loss: 0.21432 | val_0_auc: 0.88657 |  0:00:35s\n",
      "epoch 29 | loss: 0.21137 | val_0_auc: 0.89021 |  0:00:36s\n",
      "epoch 30 | loss: 0.21006 | val_0_auc: 0.89013 |  0:00:37s\n",
      "epoch 31 | loss: 0.20843 | val_0_auc: 0.89323 |  0:00:38s\n",
      "epoch 32 | loss: 0.20512 | val_0_auc: 0.88794 |  0:00:39s\n",
      "epoch 33 | loss: 0.20109 | val_0_auc: 0.88601 |  0:00:41s\n",
      "epoch 34 | loss: 0.20007 | val_0_auc: 0.89035 |  0:00:42s\n",
      "epoch 35 | loss: 0.19863 | val_0_auc: 0.88509 |  0:00:43s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.90717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.56345 | val_0_auc: 0.75305 |  0:00:01s\n",
      "epoch 1  | loss: 0.43329 | val_0_auc: 0.82776 |  0:00:01s\n",
      "epoch 2  | loss: 0.38062 | val_0_auc: 0.85302 |  0:00:02s\n",
      "epoch 3  | loss: 0.35384 | val_0_auc: 0.87032 |  0:00:03s\n",
      "epoch 4  | loss: 0.33122 | val_0_auc: 0.87617 |  0:00:04s\n",
      "epoch 5  | loss: 0.31735 | val_0_auc: 0.89254 |  0:00:05s\n",
      "epoch 6  | loss: 0.3043  | val_0_auc: 0.90147 |  0:00:06s\n",
      "epoch 7  | loss: 0.29253 | val_0_auc: 0.90377 |  0:00:07s\n",
      "epoch 8  | loss: 0.28419 | val_0_auc: 0.90797 |  0:00:08s\n",
      "epoch 9  | loss: 0.28246 | val_0_auc: 0.91366 |  0:00:09s\n",
      "epoch 10 | loss: 0.27688 | val_0_auc: 0.91446 |  0:00:10s\n",
      "epoch 11 | loss: 0.27484 | val_0_auc: 0.91614 |  0:00:11s\n",
      "epoch 12 | loss: 0.27498 | val_0_auc: 0.91681 |  0:00:12s\n",
      "epoch 13 | loss: 0.27225 | val_0_auc: 0.9175  |  0:00:13s\n",
      "epoch 14 | loss: 0.26868 | val_0_auc: 0.91814 |  0:00:14s\n",
      "epoch 15 | loss: 0.26802 | val_0_auc: 0.91697 |  0:00:15s\n",
      "epoch 16 | loss: 0.26862 | val_0_auc: 0.91676 |  0:00:16s\n",
      "epoch 17 | loss: 0.26499 | val_0_auc: 0.91865 |  0:00:17s\n",
      "epoch 18 | loss: 0.26143 | val_0_auc: 0.91707 |  0:00:18s\n",
      "epoch 19 | loss: 0.26214 | val_0_auc: 0.91884 |  0:00:19s\n",
      "epoch 20 | loss: 0.25886 | val_0_auc: 0.91562 |  0:00:20s\n",
      "epoch 21 | loss: 0.25848 | val_0_auc: 0.91835 |  0:00:21s\n",
      "epoch 22 | loss: 0.25681 | val_0_auc: 0.91941 |  0:00:22s\n",
      "epoch 23 | loss: 0.25366 | val_0_auc: 0.91631 |  0:00:23s\n",
      "epoch 24 | loss: 0.25487 | val_0_auc: 0.91578 |  0:00:24s\n",
      "epoch 25 | loss: 0.25337 | val_0_auc: 0.91417 |  0:00:25s\n",
      "epoch 26 | loss: 0.25312 | val_0_auc: 0.91577 |  0:00:26s\n",
      "epoch 27 | loss: 0.25219 | val_0_auc: 0.91674 |  0:00:27s\n",
      "epoch 28 | loss: 0.24677 | val_0_auc: 0.91609 |  0:00:28s\n",
      "epoch 29 | loss: 0.24704 | val_0_auc: 0.91486 |  0:00:29s\n",
      "epoch 30 | loss: 0.24596 | val_0_auc: 0.91282 |  0:00:30s\n",
      "epoch 31 | loss: 0.2461  | val_0_auc: 0.91494 |  0:00:31s\n",
      "epoch 32 | loss: 0.2447  | val_0_auc: 0.91467 |  0:00:32s\n",
      "epoch 33 | loss: 0.24511 | val_0_auc: 0.91514 |  0:00:33s\n",
      "epoch 34 | loss: 0.24386 | val_0_auc: 0.91592 |  0:00:34s\n",
      "epoch 35 | loss: 0.24489 | val_0_auc: 0.91422 |  0:00:35s\n",
      "epoch 36 | loss: 0.24496 | val_0_auc: 0.91426 |  0:00:36s\n",
      "epoch 37 | loss: 0.24273 | val_0_auc: 0.91331 |  0:00:37s\n",
      "epoch 38 | loss: 0.24524 | val_0_auc: 0.91425 |  0:00:38s\n",
      "epoch 39 | loss: 0.24003 | val_0_auc: 0.91401 |  0:00:39s\n",
      "epoch 40 | loss: 0.24016 | val_0_auc: 0.912   |  0:00:40s\n",
      "epoch 41 | loss: 0.23965 | val_0_auc: 0.91132 |  0:00:41s\n",
      "epoch 42 | loss: 0.2368  | val_0_auc: 0.91091 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_auc = 0.91941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.5174  | val_0_auc: 0.52654 |  0:00:04s\n",
      "epoch 1  | loss: 1.33497 | val_0_auc: 0.50705 |  0:00:09s\n",
      "epoch 2  | loss: 1.22236 | val_0_auc: 0.50951 |  0:00:14s\n",
      "epoch 3  | loss: 1.13836 | val_0_auc: 0.51297 |  0:00:19s\n",
      "epoch 4  | loss: 1.09132 | val_0_auc: 0.53112 |  0:00:23s\n",
      "epoch 5  | loss: 1.06546 | val_0_auc: 0.51865 |  0:00:28s\n",
      "epoch 6  | loss: 0.99324 | val_0_auc: 0.51318 |  0:00:33s\n",
      "epoch 7  | loss: 1.0041  | val_0_auc: 0.51838 |  0:00:37s\n",
      "epoch 8  | loss: 0.97704 | val_0_auc: 0.51841 |  0:00:42s\n",
      "epoch 9  | loss: 0.96473 | val_0_auc: 0.51294 |  0:00:47s\n",
      "epoch 10 | loss: 0.93799 | val_0_auc: 0.51052 |  0:00:51s\n",
      "epoch 11 | loss: 0.93378 | val_0_auc: 0.52151 |  0:00:56s\n",
      "epoch 12 | loss: 0.91139 | val_0_auc: 0.49393 |  0:01:00s\n",
      "epoch 13 | loss: 0.88974 | val_0_auc: 0.49684 |  0:01:05s\n",
      "epoch 14 | loss: 0.87932 | val_0_auc: 0.50892 |  0:01:10s\n",
      "epoch 15 | loss: 0.84721 | val_0_auc: 0.49001 |  0:01:18s\n",
      "epoch 16 | loss: 0.85927 | val_0_auc: 0.50541 |  0:01:25s\n",
      "epoch 17 | loss: 0.84717 | val_0_auc: 0.50903 |  0:01:32s\n",
      "epoch 18 | loss: 0.84587 | val_0_auc: 0.51881 |  0:01:39s\n",
      "epoch 19 | loss: 0.83836 | val_0_auc: 0.52518 |  0:01:45s\n",
      "epoch 20 | loss: 0.83108 | val_0_auc: 0.50574 |  0:01:49s\n",
      "epoch 21 | loss: 0.82525 | val_0_auc: 0.50746 |  0:01:54s\n",
      "epoch 22 | loss: 0.79443 | val_0_auc: 0.51274 |  0:02:01s\n",
      "epoch 23 | loss: 0.80571 | val_0_auc: 0.52893 |  0:02:09s\n",
      "epoch 24 | loss: 0.79643 | val_0_auc: 0.5223  |  0:02:17s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_auc = 0.53112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84797 | val_0_auc: 0.63825 |  0:00:01s\n",
      "epoch 1  | loss: 0.78496 | val_0_auc: 0.6619  |  0:00:02s\n",
      "epoch 2  | loss: 0.73498 | val_0_auc: 0.66288 |  0:00:04s\n",
      "epoch 3  | loss: 0.69767 | val_0_auc: 0.67918 |  0:00:05s\n",
      "epoch 4  | loss: 0.67347 | val_0_auc: 0.70733 |  0:00:06s\n",
      "epoch 5  | loss: 0.65331 | val_0_auc: 0.73479 |  0:00:07s\n",
      "epoch 6  | loss: 0.63631 | val_0_auc: 0.75222 |  0:00:08s\n",
      "epoch 7  | loss: 0.62594 | val_0_auc: 0.76347 |  0:00:10s\n",
      "epoch 8  | loss: 0.61032 | val_0_auc: 0.77747 |  0:00:11s\n",
      "epoch 9  | loss: 0.60315 | val_0_auc: 0.78618 |  0:00:13s\n",
      "epoch 10 | loss: 0.59665 | val_0_auc: 0.7931  |  0:00:14s\n",
      "epoch 11 | loss: 0.58871 | val_0_auc: 0.79627 |  0:00:15s\n",
      "epoch 12 | loss: 0.58231 | val_0_auc: 0.80163 |  0:00:17s\n",
      "epoch 13 | loss: 0.57771 | val_0_auc: 0.80436 |  0:00:18s\n",
      "epoch 14 | loss: 0.57319 | val_0_auc: 0.8086  |  0:00:19s\n",
      "epoch 15 | loss: 0.56983 | val_0_auc: 0.81113 |  0:00:21s\n",
      "epoch 16 | loss: 0.56224 | val_0_auc: 0.81302 |  0:00:23s\n",
      "epoch 17 | loss: 0.55927 | val_0_auc: 0.81544 |  0:00:24s\n",
      "epoch 18 | loss: 0.55626 | val_0_auc: 0.81802 |  0:00:25s\n",
      "epoch 19 | loss: 0.55504 | val_0_auc: 0.81917 |  0:00:27s\n",
      "epoch 20 | loss: 0.54636 | val_0_auc: 0.82098 |  0:00:28s\n",
      "epoch 21 | loss: 0.54335 | val_0_auc: 0.82375 |  0:00:30s\n",
      "epoch 22 | loss: 0.54217 | val_0_auc: 0.82363 |  0:00:32s\n",
      "epoch 23 | loss: 0.54017 | val_0_auc: 0.82457 |  0:00:34s\n",
      "epoch 24 | loss: 0.53499 | val_0_auc: 0.82761 |  0:00:36s\n",
      "epoch 25 | loss: 0.53374 | val_0_auc: 0.82836 |  0:00:37s\n",
      "epoch 26 | loss: 0.53065 | val_0_auc: 0.82913 |  0:00:38s\n",
      "epoch 27 | loss: 0.5256  | val_0_auc: 0.83068 |  0:00:40s\n",
      "epoch 28 | loss: 0.5228  | val_0_auc: 0.83186 |  0:00:41s\n",
      "epoch 29 | loss: 0.52169 | val_0_auc: 0.83284 |  0:00:42s\n",
      "epoch 30 | loss: 0.51934 | val_0_auc: 0.8342  |  0:00:43s\n",
      "epoch 31 | loss: 0.51525 | val_0_auc: 0.83559 |  0:00:45s\n",
      "epoch 32 | loss: 0.51472 | val_0_auc: 0.83794 |  0:00:46s\n",
      "epoch 33 | loss: 0.51111 | val_0_auc: 0.83974 |  0:00:48s\n",
      "epoch 34 | loss: 0.50999 | val_0_auc: 0.84109 |  0:00:49s\n",
      "epoch 35 | loss: 0.50736 | val_0_auc: 0.84219 |  0:00:51s\n",
      "epoch 36 | loss: 0.50476 | val_0_auc: 0.84414 |  0:00:52s\n",
      "epoch 37 | loss: 0.50257 | val_0_auc: 0.84537 |  0:00:54s\n",
      "epoch 38 | loss: 0.50094 | val_0_auc: 0.84587 |  0:00:55s\n",
      "epoch 39 | loss: 0.49744 | val_0_auc: 0.84682 |  0:00:57s\n",
      "epoch 40 | loss: 0.49423 | val_0_auc: 0.84767 |  0:00:58s\n",
      "epoch 41 | loss: 0.49091 | val_0_auc: 0.84865 |  0:00:59s\n",
      "epoch 42 | loss: 0.48856 | val_0_auc: 0.84925 |  0:01:00s\n",
      "epoch 43 | loss: 0.48993 | val_0_auc: 0.85059 |  0:01:02s\n",
      "epoch 44 | loss: 0.4861  | val_0_auc: 0.85124 |  0:01:03s\n",
      "epoch 45 | loss: 0.48312 | val_0_auc: 0.85126 |  0:01:04s\n",
      "epoch 46 | loss: 0.48223 | val_0_auc: 0.85197 |  0:01:05s\n",
      "epoch 47 | loss: 0.48154 | val_0_auc: 0.85244 |  0:01:07s\n",
      "epoch 48 | loss: 0.47827 | val_0_auc: 0.85286 |  0:01:08s\n",
      "epoch 49 | loss: 0.4764  | val_0_auc: 0.85306 |  0:01:09s\n",
      "epoch 50 | loss: 0.47384 | val_0_auc: 0.85398 |  0:01:11s\n",
      "epoch 51 | loss: 0.47432 | val_0_auc: 0.85389 |  0:01:12s\n",
      "epoch 52 | loss: 0.47195 | val_0_auc: 0.85432 |  0:01:13s\n",
      "epoch 53 | loss: 0.46878 | val_0_auc: 0.855   |  0:01:14s\n",
      "epoch 54 | loss: 0.46791 | val_0_auc: 0.85524 |  0:01:16s\n",
      "epoch 55 | loss: 0.46767 | val_0_auc: 0.85535 |  0:01:17s\n",
      "epoch 56 | loss: 0.46671 | val_0_auc: 0.85625 |  0:01:18s\n",
      "epoch 57 | loss: 0.46425 | val_0_auc: 0.85649 |  0:01:19s\n",
      "epoch 58 | loss: 0.46143 | val_0_auc: 0.85659 |  0:01:21s\n",
      "epoch 59 | loss: 0.46062 | val_0_auc: 0.85727 |  0:01:22s\n",
      "epoch 60 | loss: 0.45959 | val_0_auc: 0.85725 |  0:01:23s\n",
      "epoch 61 | loss: 0.45665 | val_0_auc: 0.85698 |  0:01:25s\n",
      "epoch 62 | loss: 0.45633 | val_0_auc: 0.85724 |  0:01:27s\n",
      "epoch 63 | loss: 0.4532  | val_0_auc: 0.85768 |  0:01:29s\n",
      "epoch 64 | loss: 0.45274 | val_0_auc: 0.85775 |  0:01:30s\n",
      "epoch 65 | loss: 0.45283 | val_0_auc: 0.8581  |  0:01:31s\n",
      "epoch 66 | loss: 0.45083 | val_0_auc: 0.8583  |  0:01:32s\n",
      "epoch 67 | loss: 0.44962 | val_0_auc: 0.85801 |  0:01:33s\n",
      "epoch 68 | loss: 0.44837 | val_0_auc: 0.85844 |  0:01:34s\n",
      "epoch 69 | loss: 0.44648 | val_0_auc: 0.85878 |  0:01:35s\n",
      "epoch 70 | loss: 0.44639 | val_0_auc: 0.85867 |  0:01:37s\n",
      "epoch 71 | loss: 0.4436  | val_0_auc: 0.85911 |  0:01:38s\n",
      "epoch 72 | loss: 0.441   | val_0_auc: 0.8593  |  0:01:39s\n",
      "epoch 73 | loss: 0.44172 | val_0_auc: 0.85923 |  0:01:40s\n",
      "epoch 74 | loss: 0.44056 | val_0_auc: 0.85936 |  0:01:41s\n",
      "epoch 75 | loss: 0.43935 | val_0_auc: 0.85947 |  0:01:42s\n",
      "epoch 76 | loss: 0.43457 | val_0_auc: 0.85995 |  0:01:43s\n",
      "epoch 77 | loss: 0.43689 | val_0_auc: 0.86063 |  0:01:44s\n",
      "epoch 78 | loss: 0.43498 | val_0_auc: 0.86197 |  0:01:45s\n",
      "epoch 79 | loss: 0.43241 | val_0_auc: 0.86297 |  0:01:47s\n",
      "epoch 80 | loss: 0.43    | val_0_auc: 0.86318 |  0:01:48s\n",
      "epoch 81 | loss: 0.42956 | val_0_auc: 0.86421 |  0:01:49s\n",
      "epoch 82 | loss: 0.42842 | val_0_auc: 0.86453 |  0:01:50s\n",
      "epoch 83 | loss: 0.43046 | val_0_auc: 0.86491 |  0:01:51s\n",
      "epoch 84 | loss: 0.42471 | val_0_auc: 0.86438 |  0:01:52s\n",
      "epoch 85 | loss: 0.42425 | val_0_auc: 0.86535 |  0:01:53s\n",
      "epoch 86 | loss: 0.42372 | val_0_auc: 0.8661  |  0:01:54s\n",
      "epoch 87 | loss: 0.42465 | val_0_auc: 0.86554 |  0:01:55s\n",
      "epoch 88 | loss: 0.42132 | val_0_auc: 0.86597 |  0:01:56s\n",
      "epoch 89 | loss: 0.42239 | val_0_auc: 0.86678 |  0:01:57s\n",
      "epoch 90 | loss: 0.41944 | val_0_auc: 0.86658 |  0:01:58s\n",
      "epoch 91 | loss: 0.41775 | val_0_auc: 0.86727 |  0:02:00s\n",
      "epoch 92 | loss: 0.41719 | val_0_auc: 0.86726 |  0:02:01s\n",
      "epoch 93 | loss: 0.41643 | val_0_auc: 0.86708 |  0:02:02s\n",
      "epoch 94 | loss: 0.41694 | val_0_auc: 0.86693 |  0:02:03s\n",
      "epoch 95 | loss: 0.41502 | val_0_auc: 0.86741 |  0:02:04s\n",
      "epoch 96 | loss: 0.41262 | val_0_auc: 0.86765 |  0:02:05s\n",
      "epoch 97 | loss: 0.41239 | val_0_auc: 0.8681  |  0:02:06s\n",
      "epoch 98 | loss: 0.41158 | val_0_auc: 0.86777 |  0:02:07s\n",
      "epoch 99 | loss: 0.4104  | val_0_auc: 0.86857 |  0:02:09s\n",
      "epoch 100| loss: 0.40887 | val_0_auc: 0.86886 |  0:02:10s\n",
      "epoch 101| loss: 0.40888 | val_0_auc: 0.86913 |  0:02:11s\n",
      "epoch 102| loss: 0.4098  | val_0_auc: 0.86891 |  0:02:12s\n",
      "epoch 103| loss: 0.40641 | val_0_auc: 0.86922 |  0:02:13s\n",
      "epoch 104| loss: 0.40626 | val_0_auc: 0.869   |  0:02:14s\n",
      "epoch 105| loss: 0.40607 | val_0_auc: 0.86954 |  0:02:15s\n",
      "epoch 106| loss: 0.40276 | val_0_auc: 0.86943 |  0:02:16s\n",
      "epoch 107| loss: 0.40417 | val_0_auc: 0.87005 |  0:02:17s\n",
      "epoch 108| loss: 0.4024  | val_0_auc: 0.87021 |  0:02:19s\n",
      "epoch 109| loss: 0.40314 | val_0_auc: 0.8706  |  0:02:20s\n",
      "epoch 110| loss: 0.40147 | val_0_auc: 0.8698  |  0:02:22s\n",
      "epoch 111| loss: 0.40131 | val_0_auc: 0.87047 |  0:02:23s\n",
      "epoch 112| loss: 0.40166 | val_0_auc: 0.87055 |  0:02:24s\n",
      "epoch 113| loss: 0.40065 | val_0_auc: 0.87121 |  0:02:25s\n",
      "epoch 114| loss: 0.39892 | val_0_auc: 0.87178 |  0:02:26s\n",
      "epoch 115| loss: 0.39813 | val_0_auc: 0.87165 |  0:02:27s\n",
      "epoch 116| loss: 0.39488 | val_0_auc: 0.87102 |  0:02:28s\n",
      "epoch 117| loss: 0.39554 | val_0_auc: 0.87152 |  0:02:29s\n",
      "epoch 118| loss: 0.3968  | val_0_auc: 0.87144 |  0:02:30s\n",
      "epoch 119| loss: 0.39448 | val_0_auc: 0.87094 |  0:02:31s\n",
      "epoch 120| loss: 0.39403 | val_0_auc: 0.8721  |  0:02:32s\n",
      "epoch 121| loss: 0.39158 | val_0_auc: 0.87245 |  0:02:33s\n",
      "epoch 122| loss: 0.39225 | val_0_auc: 0.87293 |  0:02:35s\n",
      "epoch 123| loss: 0.39184 | val_0_auc: 0.87304 |  0:02:36s\n",
      "epoch 124| loss: 0.3895  | val_0_auc: 0.87343 |  0:02:37s\n",
      "epoch 125| loss: 0.38979 | val_0_auc: 0.87443 |  0:02:38s\n",
      "epoch 126| loss: 0.38617 | val_0_auc: 0.87425 |  0:02:39s\n",
      "epoch 127| loss: 0.38734 | val_0_auc: 0.87473 |  0:02:40s\n",
      "epoch 128| loss: 0.38672 | val_0_auc: 0.87442 |  0:02:41s\n",
      "epoch 129| loss: 0.38279 | val_0_auc: 0.87453 |  0:02:42s\n",
      "epoch 130| loss: 0.38478 | val_0_auc: 0.87468 |  0:02:43s\n",
      "epoch 131| loss: 0.38197 | val_0_auc: 0.87565 |  0:02:44s\n",
      "epoch 132| loss: 0.37897 | val_0_auc: 0.87543 |  0:02:46s\n",
      "epoch 133| loss: 0.38126 | val_0_auc: 0.8769  |  0:02:47s\n",
      "epoch 134| loss: 0.38012 | val_0_auc: 0.87715 |  0:02:48s\n",
      "epoch 135| loss: 0.37858 | val_0_auc: 0.8778  |  0:02:49s\n",
      "epoch 136| loss: 0.37623 | val_0_auc: 0.87911 |  0:02:51s\n",
      "epoch 137| loss: 0.37441 | val_0_auc: 0.8802  |  0:02:53s\n",
      "epoch 138| loss: 0.37328 | val_0_auc: 0.8804  |  0:02:55s\n",
      "epoch 139| loss: 0.37273 | val_0_auc: 0.87982 |  0:02:56s\n",
      "epoch 140| loss: 0.37278 | val_0_auc: 0.88074 |  0:02:58s\n",
      "epoch 141| loss: 0.37409 | val_0_auc: 0.88206 |  0:03:00s\n",
      "epoch 142| loss: 0.37094 | val_0_auc: 0.88272 |  0:03:01s\n",
      "epoch 143| loss: 0.36913 | val_0_auc: 0.88232 |  0:03:03s\n",
      "epoch 144| loss: 0.36815 | val_0_auc: 0.88415 |  0:03:05s\n",
      "epoch 145| loss: 0.36612 | val_0_auc: 0.88379 |  0:03:06s\n",
      "epoch 146| loss: 0.36654 | val_0_auc: 0.88327 |  0:03:08s\n",
      "epoch 147| loss: 0.36783 | val_0_auc: 0.88425 |  0:03:09s\n",
      "epoch 148| loss: 0.36492 | val_0_auc: 0.88451 |  0:03:10s\n",
      "epoch 149| loss: 0.36608 | val_0_auc: 0.88439 |  0:03:12s\n",
      "epoch 150| loss: 0.36435 | val_0_auc: 0.88457 |  0:03:13s\n",
      "epoch 151| loss: 0.36339 | val_0_auc: 0.88465 |  0:03:15s\n",
      "epoch 152| loss: 0.36175 | val_0_auc: 0.88408 |  0:03:16s\n",
      "epoch 153| loss: 0.36306 | val_0_auc: 0.885   |  0:03:18s\n",
      "epoch 154| loss: 0.36139 | val_0_auc: 0.88488 |  0:03:19s\n",
      "epoch 155| loss: 0.36043 | val_0_auc: 0.88509 |  0:03:20s\n",
      "epoch 156| loss: 0.35823 | val_0_auc: 0.88481 |  0:03:21s\n",
      "epoch 157| loss: 0.35995 | val_0_auc: 0.88649 |  0:03:22s\n",
      "epoch 158| loss: 0.36084 | val_0_auc: 0.88615 |  0:03:23s\n",
      "epoch 159| loss: 0.35786 | val_0_auc: 0.88615 |  0:03:24s\n",
      "epoch 160| loss: 0.35874 | val_0_auc: 0.88624 |  0:03:25s\n",
      "epoch 161| loss: 0.35631 | val_0_auc: 0.88621 |  0:03:26s\n",
      "epoch 162| loss: 0.35602 | val_0_auc: 0.88551 |  0:03:27s\n",
      "epoch 163| loss: 0.35648 | val_0_auc: 0.88595 |  0:03:29s\n",
      "epoch 164| loss: 0.35486 | val_0_auc: 0.88641 |  0:03:30s\n",
      "epoch 165| loss: 0.35656 | val_0_auc: 0.88717 |  0:03:31s\n",
      "epoch 166| loss: 0.35284 | val_0_auc: 0.88815 |  0:03:32s\n",
      "epoch 167| loss: 0.35554 | val_0_auc: 0.88817 |  0:03:33s\n",
      "epoch 168| loss: 0.35202 | val_0_auc: 0.88787 |  0:03:34s\n",
      "epoch 169| loss: 0.35352 | val_0_auc: 0.88818 |  0:03:35s\n",
      "epoch 170| loss: 0.35311 | val_0_auc: 0.88839 |  0:03:36s\n",
      "epoch 171| loss: 0.35051 | val_0_auc: 0.88827 |  0:03:37s\n",
      "epoch 172| loss: 0.35031 | val_0_auc: 0.88828 |  0:03:38s\n",
      "epoch 173| loss: 0.35215 | val_0_auc: 0.8893  |  0:03:39s\n",
      "epoch 174| loss: 0.35088 | val_0_auc: 0.89002 |  0:03:40s\n",
      "epoch 175| loss: 0.34933 | val_0_auc: 0.88907 |  0:03:41s\n",
      "epoch 176| loss: 0.34959 | val_0_auc: 0.88968 |  0:03:42s\n",
      "epoch 177| loss: 0.34653 | val_0_auc: 0.8891  |  0:03:44s\n",
      "epoch 178| loss: 0.34917 | val_0_auc: 0.88983 |  0:03:45s\n",
      "epoch 179| loss: 0.3472  | val_0_auc: 0.89065 |  0:03:46s\n",
      "epoch 180| loss: 0.34832 | val_0_auc: 0.89049 |  0:03:47s\n",
      "epoch 181| loss: 0.34654 | val_0_auc: 0.89118 |  0:03:49s\n",
      "epoch 182| loss: 0.3468  | val_0_auc: 0.8913  |  0:03:50s\n",
      "epoch 183| loss: 0.34694 | val_0_auc: 0.89127 |  0:03:52s\n",
      "epoch 184| loss: 0.34534 | val_0_auc: 0.8903  |  0:03:53s\n",
      "epoch 185| loss: 0.34558 | val_0_auc: 0.89061 |  0:03:55s\n",
      "epoch 186| loss: 0.34373 | val_0_auc: 0.89157 |  0:03:56s\n",
      "epoch 187| loss: 0.34451 | val_0_auc: 0.89115 |  0:03:57s\n",
      "epoch 188| loss: 0.34311 | val_0_auc: 0.89095 |  0:03:58s\n",
      "epoch 189| loss: 0.34478 | val_0_auc: 0.89112 |  0:04:00s\n",
      "epoch 190| loss: 0.34322 | val_0_auc: 0.89046 |  0:04:01s\n",
      "epoch 191| loss: 0.34309 | val_0_auc: 0.89074 |  0:04:02s\n",
      "epoch 192| loss: 0.34293 | val_0_auc: 0.89092 |  0:04:04s\n",
      "epoch 193| loss: 0.34246 | val_0_auc: 0.89128 |  0:04:05s\n",
      "epoch 194| loss: 0.34175 | val_0_auc: 0.89191 |  0:04:06s\n",
      "epoch 195| loss: 0.34185 | val_0_auc: 0.89098 |  0:04:08s\n",
      "epoch 196| loss: 0.33904 | val_0_auc: 0.89085 |  0:04:09s\n",
      "epoch 197| loss: 0.34136 | val_0_auc: 0.89122 |  0:04:10s\n",
      "epoch 198| loss: 0.33886 | val_0_auc: 0.89164 |  0:04:11s\n",
      "epoch 199| loss: 0.33955 | val_0_auc: 0.89214 |  0:04:12s\n",
      "epoch 200| loss: 0.34195 | val_0_auc: 0.89159 |  0:04:14s\n",
      "epoch 201| loss: 0.34112 | val_0_auc: 0.89164 |  0:04:15s\n",
      "epoch 202| loss: 0.3397  | val_0_auc: 0.89108 |  0:04:16s\n",
      "epoch 203| loss: 0.34094 | val_0_auc: 0.89154 |  0:04:17s\n",
      "epoch 204| loss: 0.34095 | val_0_auc: 0.89203 |  0:04:18s\n",
      "epoch 205| loss: 0.33714 | val_0_auc: 0.89199 |  0:04:19s\n",
      "epoch 206| loss: 0.34003 | val_0_auc: 0.89307 |  0:04:20s\n",
      "epoch 207| loss: 0.33977 | val_0_auc: 0.89257 |  0:04:21s\n",
      "epoch 208| loss: 0.33962 | val_0_auc: 0.89215 |  0:04:22s\n",
      "epoch 209| loss: 0.33789 | val_0_auc: 0.89284 |  0:04:24s\n",
      "epoch 210| loss: 0.33777 | val_0_auc: 0.89359 |  0:04:25s\n",
      "epoch 211| loss: 0.33758 | val_0_auc: 0.89378 |  0:04:26s\n",
      "epoch 212| loss: 0.33818 | val_0_auc: 0.89387 |  0:04:28s\n",
      "epoch 213| loss: 0.33795 | val_0_auc: 0.89365 |  0:04:29s\n",
      "epoch 214| loss: 0.33715 | val_0_auc: 0.89361 |  0:04:31s\n",
      "epoch 215| loss: 0.33899 | val_0_auc: 0.89359 |  0:04:32s\n",
      "epoch 216| loss: 0.33773 | val_0_auc: 0.89346 |  0:04:33s\n",
      "epoch 217| loss: 0.33595 | val_0_auc: 0.8934  |  0:04:35s\n",
      "epoch 218| loss: 0.33575 | val_0_auc: 0.89337 |  0:04:36s\n",
      "epoch 219| loss: 0.33659 | val_0_auc: 0.89389 |  0:04:38s\n",
      "epoch 220| loss: 0.33607 | val_0_auc: 0.89438 |  0:04:39s\n",
      "epoch 221| loss: 0.33732 | val_0_auc: 0.89441 |  0:04:40s\n",
      "epoch 222| loss: 0.33709 | val_0_auc: 0.8947  |  0:04:42s\n",
      "epoch 223| loss: 0.3358  | val_0_auc: 0.89428 |  0:04:43s\n",
      "epoch 224| loss: 0.33422 | val_0_auc: 0.89345 |  0:04:44s\n",
      "epoch 225| loss: 0.336   | val_0_auc: 0.89306 |  0:04:45s\n",
      "epoch 226| loss: 0.33496 | val_0_auc: 0.89464 |  0:04:47s\n",
      "epoch 227| loss: 0.33589 | val_0_auc: 0.89397 |  0:04:48s\n",
      "epoch 228| loss: 0.33439 | val_0_auc: 0.89412 |  0:04:49s\n",
      "epoch 229| loss: 0.33546 | val_0_auc: 0.89356 |  0:04:51s\n",
      "epoch 230| loss: 0.33426 | val_0_auc: 0.89374 |  0:04:52s\n",
      "epoch 231| loss: 0.33423 | val_0_auc: 0.89421 |  0:04:53s\n",
      "epoch 232| loss: 0.33304 | val_0_auc: 0.89556 |  0:04:54s\n",
      "epoch 233| loss: 0.33333 | val_0_auc: 0.89383 |  0:04:56s\n",
      "epoch 234| loss: 0.33316 | val_0_auc: 0.89462 |  0:04:57s\n",
      "epoch 235| loss: 0.33191 | val_0_auc: 0.89405 |  0:04:58s\n",
      "epoch 236| loss: 0.33223 | val_0_auc: 0.89475 |  0:04:59s\n",
      "epoch 237| loss: 0.33195 | val_0_auc: 0.89411 |  0:05:01s\n",
      "epoch 238| loss: 0.33409 | val_0_auc: 0.89366 |  0:05:02s\n",
      "epoch 239| loss: 0.33091 | val_0_auc: 0.89426 |  0:05:03s\n",
      "epoch 240| loss: 0.33165 | val_0_auc: 0.89516 |  0:05:05s\n",
      "epoch 241| loss: 0.33254 | val_0_auc: 0.89506 |  0:05:07s\n",
      "epoch 242| loss: 0.32958 | val_0_auc: 0.89547 |  0:05:08s\n",
      "epoch 243| loss: 0.33115 | val_0_auc: 0.89464 |  0:05:09s\n",
      "epoch 244| loss: 0.33138 | val_0_auc: 0.89528 |  0:05:10s\n",
      "epoch 245| loss: 0.33005 | val_0_auc: 0.89554 |  0:05:12s\n",
      "epoch 246| loss: 0.33151 | val_0_auc: 0.89552 |  0:05:13s\n",
      "epoch 247| loss: 0.33314 | val_0_auc: 0.89481 |  0:05:14s\n",
      "epoch 248| loss: 0.33258 | val_0_auc: 0.89456 |  0:05:15s\n",
      "epoch 249| loss: 0.33098 | val_0_auc: 0.89398 |  0:05:16s\n",
      "epoch 250| loss: 0.3296  | val_0_auc: 0.89485 |  0:05:17s\n",
      "epoch 251| loss: 0.3299  | val_0_auc: 0.89505 |  0:05:18s\n",
      "epoch 252| loss: 0.32924 | val_0_auc: 0.89563 |  0:05:19s\n",
      "epoch 253| loss: 0.32913 | val_0_auc: 0.89539 |  0:05:20s\n",
      "epoch 254| loss: 0.32744 | val_0_auc: 0.89577 |  0:05:21s\n",
      "epoch 255| loss: 0.33234 | val_0_auc: 0.89705 |  0:05:22s\n",
      "epoch 256| loss: 0.32919 | val_0_auc: 0.89684 |  0:05:23s\n",
      "epoch 257| loss: 0.32866 | val_0_auc: 0.89671 |  0:05:25s\n",
      "epoch 258| loss: 0.32777 | val_0_auc: 0.89643 |  0:05:26s\n",
      "epoch 259| loss: 0.32743 | val_0_auc: 0.89673 |  0:05:28s\n",
      "epoch 260| loss: 0.33072 | val_0_auc: 0.89626 |  0:05:29s\n",
      "epoch 261| loss: 0.32678 | val_0_auc: 0.89718 |  0:05:30s\n",
      "epoch 262| loss: 0.32793 | val_0_auc: 0.89628 |  0:05:31s\n",
      "epoch 263| loss: 0.32896 | val_0_auc: 0.89614 |  0:05:32s\n",
      "epoch 264| loss: 0.32824 | val_0_auc: 0.89632 |  0:05:33s\n",
      "epoch 265| loss: 0.3271  | val_0_auc: 0.89685 |  0:05:34s\n",
      "epoch 266| loss: 0.32752 | val_0_auc: 0.89676 |  0:05:35s\n",
      "epoch 267| loss: 0.32788 | val_0_auc: 0.89732 |  0:05:36s\n",
      "epoch 268| loss: 0.32577 | val_0_auc: 0.8973  |  0:05:37s\n",
      "epoch 269| loss: 0.32761 | val_0_auc: 0.89704 |  0:05:38s\n",
      "epoch 270| loss: 0.32662 | val_0_auc: 0.89762 |  0:05:39s\n",
      "epoch 271| loss: 0.3254  | val_0_auc: 0.89812 |  0:05:41s\n",
      "epoch 272| loss: 0.32613 | val_0_auc: 0.89748 |  0:05:42s\n",
      "epoch 273| loss: 0.32548 | val_0_auc: 0.89825 |  0:05:43s\n",
      "epoch 274| loss: 0.32655 | val_0_auc: 0.89805 |  0:05:44s\n",
      "epoch 275| loss: 0.32646 | val_0_auc: 0.8974  |  0:05:45s\n",
      "epoch 276| loss: 0.32737 | val_0_auc: 0.89698 |  0:05:46s\n",
      "epoch 277| loss: 0.32644 | val_0_auc: 0.89836 |  0:05:47s\n",
      "epoch 278| loss: 0.32349 | val_0_auc: 0.89921 |  0:05:48s\n",
      "epoch 279| loss: 0.3268  | val_0_auc: 0.8986  |  0:05:49s\n",
      "epoch 280| loss: 0.32445 | val_0_auc: 0.89978 |  0:05:50s\n",
      "epoch 281| loss: 0.32572 | val_0_auc: 0.89978 |  0:05:51s\n",
      "epoch 282| loss: 0.3256  | val_0_auc: 0.89936 |  0:05:52s\n",
      "epoch 283| loss: 0.32525 | val_0_auc: 0.89924 |  0:05:53s\n",
      "epoch 284| loss: 0.32687 | val_0_auc: 0.89939 |  0:05:54s\n",
      "epoch 285| loss: 0.32544 | val_0_auc: 0.89943 |  0:05:55s\n",
      "epoch 286| loss: 0.32557 | val_0_auc: 0.89876 |  0:05:56s\n",
      "epoch 287| loss: 0.32433 | val_0_auc: 0.89917 |  0:05:57s\n",
      "epoch 288| loss: 0.32432 | val_0_auc: 0.89892 |  0:05:59s\n",
      "epoch 289| loss: 0.327   | val_0_auc: 0.89829 |  0:06:00s\n",
      "epoch 290| loss: 0.32341 | val_0_auc: 0.89874 |  0:06:01s\n",
      "epoch 291| loss: 0.32509 | val_0_auc: 0.89817 |  0:06:02s\n",
      "epoch 292| loss: 0.32491 | val_0_auc: 0.89877 |  0:06:03s\n",
      "epoch 293| loss: 0.32525 | val_0_auc: 0.89933 |  0:06:04s\n",
      "epoch 294| loss: 0.32277 | val_0_auc: 0.89892 |  0:06:05s\n",
      "epoch 295| loss: 0.32442 | val_0_auc: 0.89902 |  0:06:06s\n",
      "epoch 296| loss: 0.32293 | val_0_auc: 0.89976 |  0:06:07s\n",
      "epoch 297| loss: 0.32249 | val_0_auc: 0.89857 |  0:06:08s\n",
      "epoch 298| loss: 0.32151 | val_0_auc: 0.89815 |  0:06:09s\n",
      "epoch 299| loss: 0.32181 | val_0_auc: 0.89918 |  0:06:10s\n",
      "epoch 300| loss: 0.32159 | val_0_auc: 0.89898 |  0:06:11s\n",
      "\n",
      "Early stopping occurred at epoch 300 with best_epoch = 280 and best_val_0_auc = 0.89978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78371 | val_0_auc: 0.52083 |  0:00:03s\n",
      "epoch 1  | loss: 0.77861 | val_0_auc: 0.53048 |  0:00:07s\n",
      "epoch 2  | loss: 0.74831 | val_0_auc: 0.53914 |  0:00:10s\n",
      "epoch 3  | loss: 0.73488 | val_0_auc: 0.55087 |  0:00:14s\n",
      "epoch 4  | loss: 0.72813 | val_0_auc: 0.54005 |  0:00:18s\n",
      "epoch 5  | loss: 0.70029 | val_0_auc: 0.54494 |  0:00:22s\n",
      "epoch 6  | loss: 0.69576 | val_0_auc: 0.54498 |  0:00:26s\n",
      "epoch 7  | loss: 0.68755 | val_0_auc: 0.54419 |  0:00:31s\n",
      "epoch 8  | loss: 0.68002 | val_0_auc: 0.54163 |  0:00:35s\n",
      "epoch 9  | loss: 0.66038 | val_0_auc: 0.54817 |  0:00:39s\n",
      "epoch 10 | loss: 0.65151 | val_0_auc: 0.55623 |  0:00:42s\n",
      "epoch 11 | loss: 0.63745 | val_0_auc: 0.56603 |  0:00:46s\n",
      "epoch 12 | loss: 0.63059 | val_0_auc: 0.55739 |  0:00:50s\n",
      "epoch 13 | loss: 0.62819 | val_0_auc: 0.56812 |  0:00:54s\n",
      "epoch 14 | loss: 0.61761 | val_0_auc: 0.57928 |  0:00:58s\n",
      "epoch 15 | loss: 0.60592 | val_0_auc: 0.58445 |  0:01:02s\n",
      "epoch 16 | loss: 0.59685 | val_0_auc: 0.58432 |  0:01:06s\n",
      "epoch 17 | loss: 0.59933 | val_0_auc: 0.56492 |  0:01:10s\n",
      "epoch 18 | loss: 0.59045 | val_0_auc: 0.57857 |  0:01:14s\n",
      "epoch 19 | loss: 0.59405 | val_0_auc: 0.57589 |  0:01:17s\n",
      "epoch 20 | loss: 0.58473 | val_0_auc: 0.56066 |  0:01:21s\n",
      "epoch 21 | loss: 0.58344 | val_0_auc: 0.55061 |  0:01:25s\n",
      "epoch 22 | loss: 0.57988 | val_0_auc: 0.58034 |  0:01:29s\n",
      "epoch 23 | loss: 0.57705 | val_0_auc: 0.59093 |  0:01:34s\n",
      "epoch 24 | loss: 0.57892 | val_0_auc: 0.58258 |  0:01:38s\n",
      "epoch 25 | loss: 0.57347 | val_0_auc: 0.57896 |  0:01:41s\n",
      "epoch 26 | loss: 0.57288 | val_0_auc: 0.58018 |  0:01:45s\n",
      "epoch 27 | loss: 0.56774 | val_0_auc: 0.59166 |  0:01:49s\n",
      "epoch 28 | loss: 0.56613 | val_0_auc: 0.60846 |  0:01:52s\n",
      "epoch 29 | loss: 0.56465 | val_0_auc: 0.60268 |  0:01:56s\n",
      "epoch 30 | loss: 0.56231 | val_0_auc: 0.61631 |  0:01:59s\n",
      "epoch 31 | loss: 0.55948 | val_0_auc: 0.59822 |  0:02:03s\n",
      "epoch 32 | loss: 0.55811 | val_0_auc: 0.60017 |  0:02:06s\n",
      "epoch 33 | loss: 0.55816 | val_0_auc: 0.60028 |  0:02:10s\n",
      "epoch 34 | loss: 0.55424 | val_0_auc: 0.61001 |  0:02:14s\n",
      "epoch 35 | loss: 0.55592 | val_0_auc: 0.60391 |  0:02:17s\n",
      "epoch 36 | loss: 0.54922 | val_0_auc: 0.63362 |  0:02:21s\n",
      "epoch 37 | loss: 0.54994 | val_0_auc: 0.63135 |  0:02:27s\n",
      "epoch 38 | loss: 0.55193 | val_0_auc: 0.62214 |  0:02:33s\n",
      "epoch 39 | loss: 0.54943 | val_0_auc: 0.64076 |  0:02:40s\n",
      "epoch 40 | loss: 0.54863 | val_0_auc: 0.62597 |  0:02:46s\n",
      "epoch 41 | loss: 0.54879 | val_0_auc: 0.61218 |  0:02:51s\n",
      "epoch 42 | loss: 0.54634 | val_0_auc: 0.63017 |  0:02:56s\n",
      "epoch 43 | loss: 0.54636 | val_0_auc: 0.63413 |  0:03:02s\n",
      "epoch 44 | loss: 0.54527 | val_0_auc: 0.62869 |  0:03:07s\n",
      "epoch 45 | loss: 0.54259 | val_0_auc: 0.64091 |  0:03:12s\n",
      "epoch 46 | loss: 0.5455  | val_0_auc: 0.64637 |  0:03:17s\n",
      "epoch 47 | loss: 0.54221 | val_0_auc: 0.6528  |  0:03:23s\n",
      "epoch 48 | loss: 0.54225 | val_0_auc: 0.65015 |  0:03:28s\n",
      "epoch 49 | loss: 0.54163 | val_0_auc: 0.65635 |  0:03:33s\n",
      "epoch 50 | loss: 0.533   | val_0_auc: 0.6446  |  0:03:37s\n",
      "epoch 51 | loss: 0.53521 | val_0_auc: 0.65174 |  0:03:41s\n",
      "epoch 52 | loss: 0.53231 | val_0_auc: 0.6711  |  0:03:46s\n",
      "epoch 53 | loss: 0.53352 | val_0_auc: 0.67338 |  0:03:50s\n",
      "epoch 54 | loss: 0.53267 | val_0_auc: 0.67219 |  0:03:55s\n",
      "epoch 55 | loss: 0.53029 | val_0_auc: 0.65741 |  0:04:00s\n",
      "epoch 56 | loss: 0.53071 | val_0_auc: 0.66806 |  0:04:04s\n",
      "epoch 57 | loss: 0.53032 | val_0_auc: 0.67526 |  0:04:08s\n",
      "epoch 58 | loss: 0.52891 | val_0_auc: 0.6665  |  0:04:15s\n",
      "epoch 59 | loss: 0.53056 | val_0_auc: 0.66818 |  0:04:21s\n",
      "epoch 60 | loss: 0.52762 | val_0_auc: 0.66756 |  0:04:27s\n",
      "epoch 61 | loss: 0.5316  | val_0_auc: 0.66631 |  0:04:33s\n",
      "epoch 62 | loss: 0.52798 | val_0_auc: 0.67086 |  0:04:38s\n",
      "epoch 63 | loss: 0.52698 | val_0_auc: 0.6757  |  0:04:43s\n",
      "epoch 64 | loss: 0.5232  | val_0_auc: 0.67868 |  0:04:50s\n",
      "epoch 65 | loss: 0.52367 | val_0_auc: 0.67936 |  0:04:55s\n",
      "epoch 66 | loss: 0.52347 | val_0_auc: 0.6787  |  0:05:01s\n",
      "epoch 67 | loss: 0.52291 | val_0_auc: 0.67876 |  0:05:05s\n",
      "epoch 68 | loss: 0.52294 | val_0_auc: 0.68798 |  0:05:10s\n",
      "epoch 69 | loss: 0.52207 | val_0_auc: 0.68643 |  0:05:14s\n",
      "epoch 70 | loss: 0.51981 | val_0_auc: 0.71434 |  0:05:18s\n",
      "epoch 71 | loss: 0.51281 | val_0_auc: 0.7209  |  0:05:23s\n",
      "epoch 72 | loss: 0.51095 | val_0_auc: 0.72607 |  0:05:27s\n",
      "epoch 73 | loss: 0.51213 | val_0_auc: 0.72511 |  0:05:32s\n",
      "epoch 74 | loss: 0.51049 | val_0_auc: 0.7322  |  0:05:36s\n",
      "epoch 75 | loss: 0.50754 | val_0_auc: 0.73968 |  0:05:40s\n",
      "epoch 76 | loss: 0.50692 | val_0_auc: 0.73261 |  0:05:44s\n",
      "epoch 77 | loss: 0.50979 | val_0_auc: 0.74443 |  0:05:49s\n",
      "epoch 78 | loss: 0.5079  | val_0_auc: 0.73151 |  0:05:55s\n",
      "epoch 79 | loss: 0.50504 | val_0_auc: 0.74214 |  0:05:59s\n",
      "epoch 80 | loss: 0.50667 | val_0_auc: 0.7476  |  0:06:03s\n",
      "epoch 81 | loss: 0.50655 | val_0_auc: 0.74191 |  0:06:08s\n",
      "epoch 82 | loss: 0.50648 | val_0_auc: 0.74165 |  0:06:13s\n",
      "epoch 83 | loss: 0.50627 | val_0_auc: 0.74094 |  0:06:18s\n",
      "epoch 84 | loss: 0.50172 | val_0_auc: 0.72526 |  0:06:23s\n",
      "epoch 85 | loss: 0.50293 | val_0_auc: 0.70916 |  0:06:28s\n",
      "epoch 86 | loss: 0.49838 | val_0_auc: 0.72234 |  0:06:33s\n",
      "epoch 87 | loss: 0.50009 | val_0_auc: 0.73649 |  0:06:38s\n",
      "epoch 88 | loss: 0.49949 | val_0_auc: 0.74409 |  0:06:44s\n",
      "epoch 89 | loss: 0.5006  | val_0_auc: 0.73555 |  0:06:48s\n",
      "epoch 90 | loss: 0.50049 | val_0_auc: 0.73065 |  0:06:52s\n",
      "epoch 91 | loss: 0.49935 | val_0_auc: 0.73578 |  0:06:56s\n",
      "epoch 92 | loss: 0.50009 | val_0_auc: 0.73443 |  0:07:01s\n",
      "epoch 93 | loss: 0.49593 | val_0_auc: 0.7254  |  0:07:05s\n",
      "epoch 94 | loss: 0.49396 | val_0_auc: 0.7455  |  0:07:09s\n",
      "epoch 95 | loss: 0.4943  | val_0_auc: 0.73695 |  0:07:13s\n",
      "epoch 96 | loss: 0.49261 | val_0_auc: 0.73014 |  0:07:18s\n",
      "epoch 97 | loss: 0.49196 | val_0_auc: 0.7282  |  0:07:22s\n",
      "epoch 98 | loss: 0.48975 | val_0_auc: 0.73497 |  0:07:26s\n",
      "epoch 99 | loss: 0.49594 | val_0_auc: 0.73774 |  0:07:30s\n",
      "epoch 100| loss: 0.48976 | val_0_auc: 0.73213 |  0:07:34s\n",
      "\n",
      "Early stopping occurred at epoch 100 with best_epoch = 80 and best_val_0_auc = 0.7476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.77042 | val_0_auc: 0.64267 |  0:00:02s\n",
      "epoch 1  | loss: 0.51784 | val_0_auc: 0.74515 |  0:00:04s\n",
      "epoch 2  | loss: 0.45492 | val_0_auc: 0.79736 |  0:00:07s\n",
      "epoch 3  | loss: 0.42441 | val_0_auc: 0.80884 |  0:00:09s\n",
      "epoch 4  | loss: 0.41981 | val_0_auc: 0.82172 |  0:00:12s\n",
      "epoch 5  | loss: 0.40741 | val_0_auc: 0.83163 |  0:00:14s\n",
      "epoch 6  | loss: 0.40117 | val_0_auc: 0.84003 |  0:00:17s\n",
      "epoch 7  | loss: 0.39059 | val_0_auc: 0.84443 |  0:00:19s\n",
      "epoch 8  | loss: 0.38394 | val_0_auc: 0.84998 |  0:00:22s\n",
      "epoch 9  | loss: 0.38167 | val_0_auc: 0.84199 |  0:00:24s\n",
      "epoch 10 | loss: 0.38407 | val_0_auc: 0.85042 |  0:00:27s\n",
      "epoch 11 | loss: 0.37986 | val_0_auc: 0.85519 |  0:00:29s\n",
      "epoch 12 | loss: 0.36964 | val_0_auc: 0.86236 |  0:00:31s\n",
      "epoch 13 | loss: 0.36541 | val_0_auc: 0.86259 |  0:00:34s\n",
      "epoch 14 | loss: 0.36251 | val_0_auc: 0.86323 |  0:00:36s\n",
      "epoch 15 | loss: 0.35646 | val_0_auc: 0.86648 |  0:00:39s\n",
      "epoch 16 | loss: 0.35519 | val_0_auc: 0.87193 |  0:00:42s\n",
      "epoch 17 | loss: 0.34467 | val_0_auc: 0.87657 |  0:00:45s\n",
      "epoch 18 | loss: 0.3409  | val_0_auc: 0.87925 |  0:00:47s\n",
      "epoch 19 | loss: 0.33959 | val_0_auc: 0.87648 |  0:00:50s\n",
      "epoch 20 | loss: 0.33526 | val_0_auc: 0.87898 |  0:00:52s\n",
      "epoch 21 | loss: 0.33441 | val_0_auc: 0.8822  |  0:00:54s\n",
      "epoch 22 | loss: 0.33016 | val_0_auc: 0.88213 |  0:00:57s\n",
      "epoch 23 | loss: 0.32625 | val_0_auc: 0.8862  |  0:00:59s\n",
      "epoch 24 | loss: 0.32549 | val_0_auc: 0.87912 |  0:01:01s\n",
      "epoch 25 | loss: 0.32125 | val_0_auc: 0.87901 |  0:01:04s\n",
      "epoch 26 | loss: 0.31992 | val_0_auc: 0.88301 |  0:01:07s\n",
      "epoch 27 | loss: 0.31529 | val_0_auc: 0.89101 |  0:01:09s\n",
      "epoch 28 | loss: 0.31116 | val_0_auc: 0.89115 |  0:01:12s\n",
      "epoch 29 | loss: 0.30704 | val_0_auc: 0.89486 |  0:01:14s\n",
      "epoch 30 | loss: 0.30552 | val_0_auc: 0.89373 |  0:01:17s\n",
      "epoch 31 | loss: 0.30675 | val_0_auc: 0.89396 |  0:01:19s\n",
      "epoch 32 | loss: 0.30956 | val_0_auc: 0.90059 |  0:01:21s\n",
      "epoch 33 | loss: 0.30387 | val_0_auc: 0.89904 |  0:01:24s\n",
      "epoch 34 | loss: 0.30558 | val_0_auc: 0.90076 |  0:01:26s\n",
      "epoch 35 | loss: 0.30284 | val_0_auc: 0.90101 |  0:01:29s\n",
      "epoch 36 | loss: 0.29925 | val_0_auc: 0.90252 |  0:01:31s\n",
      "epoch 37 | loss: 0.2969  | val_0_auc: 0.90068 |  0:01:34s\n",
      "epoch 38 | loss: 0.29611 | val_0_auc: 0.90305 |  0:01:37s\n",
      "epoch 39 | loss: 0.29413 | val_0_auc: 0.90357 |  0:01:39s\n",
      "epoch 40 | loss: 0.29393 | val_0_auc: 0.90566 |  0:01:42s\n",
      "epoch 41 | loss: 0.29168 | val_0_auc: 0.90564 |  0:01:44s\n",
      "epoch 42 | loss: 0.28819 | val_0_auc: 0.90744 |  0:01:47s\n",
      "epoch 43 | loss: 0.28584 | val_0_auc: 0.90529 |  0:01:49s\n",
      "epoch 44 | loss: 0.28516 | val_0_auc: 0.90498 |  0:01:51s\n",
      "epoch 45 | loss: 0.28375 | val_0_auc: 0.90869 |  0:01:54s\n",
      "epoch 46 | loss: 0.28149 | val_0_auc: 0.91275 |  0:01:56s\n",
      "epoch 47 | loss: 0.27814 | val_0_auc: 0.91309 |  0:01:58s\n",
      "epoch 48 | loss: 0.27945 | val_0_auc: 0.91341 |  0:02:01s\n",
      "epoch 49 | loss: 0.2752  | val_0_auc: 0.91452 |  0:02:03s\n",
      "epoch 50 | loss: 0.27766 | val_0_auc: 0.91423 |  0:02:06s\n",
      "epoch 51 | loss: 0.27607 | val_0_auc: 0.91406 |  0:02:08s\n",
      "epoch 52 | loss: 0.27611 | val_0_auc: 0.91571 |  0:02:11s\n",
      "epoch 53 | loss: 0.27308 | val_0_auc: 0.91549 |  0:02:13s\n",
      "epoch 54 | loss: 0.27138 | val_0_auc: 0.91482 |  0:02:16s\n",
      "epoch 55 | loss: 0.27155 | val_0_auc: 0.91563 |  0:02:18s\n",
      "epoch 56 | loss: 0.26849 | val_0_auc: 0.91462 |  0:02:20s\n",
      "epoch 57 | loss: 0.26923 | val_0_auc: 0.9155  |  0:02:23s\n",
      "epoch 58 | loss: 0.26785 | val_0_auc: 0.91508 |  0:02:25s\n",
      "epoch 59 | loss: 0.26661 | val_0_auc: 0.91706 |  0:02:28s\n",
      "epoch 60 | loss: 0.26558 | val_0_auc: 0.91627 |  0:02:30s\n",
      "epoch 61 | loss: 0.26417 | val_0_auc: 0.91565 |  0:02:32s\n",
      "epoch 62 | loss: 0.26463 | val_0_auc: 0.91556 |  0:02:35s\n",
      "epoch 63 | loss: 0.26337 | val_0_auc: 0.91424 |  0:02:37s\n",
      "epoch 64 | loss: 0.26372 | val_0_auc: 0.91528 |  0:02:40s\n",
      "epoch 65 | loss: 0.26202 | val_0_auc: 0.91488 |  0:02:42s\n",
      "epoch 66 | loss: 0.26227 | val_0_auc: 0.91536 |  0:02:45s\n",
      "epoch 67 | loss: 0.26127 | val_0_auc: 0.9149  |  0:02:47s\n",
      "epoch 68 | loss: 0.26139 | val_0_auc: 0.91661 |  0:02:50s\n",
      "epoch 69 | loss: 0.25873 | val_0_auc: 0.91583 |  0:02:52s\n",
      "epoch 70 | loss: 0.25877 | val_0_auc: 0.91384 |  0:02:54s\n",
      "epoch 71 | loss: 0.25838 | val_0_auc: 0.9139  |  0:02:57s\n",
      "epoch 72 | loss: 0.25814 | val_0_auc: 0.91623 |  0:02:59s\n",
      "epoch 73 | loss: 0.25778 | val_0_auc: 0.91522 |  0:03:02s\n",
      "epoch 74 | loss: 0.25776 | val_0_auc: 0.91427 |  0:03:05s\n",
      "epoch 75 | loss: 0.25729 | val_0_auc: 0.91625 |  0:03:07s\n",
      "epoch 76 | loss: 0.25529 | val_0_auc: 0.9145  |  0:03:09s\n",
      "epoch 77 | loss: 0.25579 | val_0_auc: 0.91498 |  0:03:12s\n",
      "epoch 78 | loss: 0.25567 | val_0_auc: 0.91333 |  0:03:14s\n",
      "epoch 79 | loss: 0.25289 | val_0_auc: 0.91461 |  0:03:17s\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_auc = 0.91706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.40783 | val_0_auc: 0.55995 |  0:00:05s\n",
      "epoch 1  | loss: 0.73596 | val_0_auc: 0.48105 |  0:00:11s\n",
      "epoch 2  | loss: 0.67481 | val_0_auc: 0.59337 |  0:00:17s\n",
      "epoch 3  | loss: 0.50404 | val_0_auc: 0.78797 |  0:00:23s\n",
      "epoch 4  | loss: 0.44059 | val_0_auc: 0.83386 |  0:00:33s\n",
      "epoch 5  | loss: 0.41285 | val_0_auc: 0.85708 |  0:00:39s\n",
      "epoch 6  | loss: 0.39172 | val_0_auc: 0.87095 |  0:00:45s\n",
      "epoch 7  | loss: 0.37518 | val_0_auc: 0.87373 |  0:00:51s\n",
      "epoch 8  | loss: 0.36455 | val_0_auc: 0.87889 |  0:00:57s\n",
      "epoch 9  | loss: 0.34774 | val_0_auc: 0.88913 |  0:01:03s\n",
      "epoch 10 | loss: 0.33633 | val_0_auc: 0.89728 |  0:01:09s\n",
      "epoch 11 | loss: 0.32951 | val_0_auc: 0.90257 |  0:01:15s\n",
      "epoch 12 | loss: 0.31946 | val_0_auc: 0.90682 |  0:01:21s\n",
      "epoch 13 | loss: 0.31529 | val_0_auc: 0.90978 |  0:01:26s\n",
      "epoch 14 | loss: 0.31015 | val_0_auc: 0.91049 |  0:01:32s\n",
      "epoch 15 | loss: 0.30577 | val_0_auc: 0.91272 |  0:01:38s\n",
      "epoch 16 | loss: 0.30042 | val_0_auc: 0.91396 |  0:01:43s\n",
      "epoch 17 | loss: 0.29658 | val_0_auc: 0.91543 |  0:01:49s\n",
      "epoch 18 | loss: 0.29604 | val_0_auc: 0.91578 |  0:01:56s\n",
      "epoch 19 | loss: 0.28996 | val_0_auc: 0.9163  |  0:02:10s\n",
      "epoch 20 | loss: 0.28657 | val_0_auc: 0.91652 |  0:02:17s\n",
      "epoch 21 | loss: 0.2903  | val_0_auc: 0.91752 |  0:02:25s\n",
      "epoch 22 | loss: 0.28703 | val_0_auc: 0.91881 |  0:02:34s\n",
      "epoch 23 | loss: 0.28236 | val_0_auc: 0.92017 |  0:02:42s\n",
      "epoch 24 | loss: 0.28152 | val_0_auc: 0.91876 |  0:02:50s\n",
      "epoch 25 | loss: 0.28158 | val_0_auc: 0.91954 |  0:02:57s\n",
      "epoch 26 | loss: 0.28    | val_0_auc: 0.92006 |  0:03:07s\n",
      "epoch 27 | loss: 0.27591 | val_0_auc: 0.91725 |  0:03:33s\n",
      "epoch 28 | loss: 0.27918 | val_0_auc: 0.91914 |  0:03:44s\n",
      "epoch 29 | loss: 0.27687 | val_0_auc: 0.91757 |  0:03:57s\n",
      "epoch 30 | loss: 0.2746  | val_0_auc: 0.91787 |  0:04:07s\n",
      "epoch 31 | loss: 0.27325 | val_0_auc: 0.91656 |  0:04:18s\n",
      "epoch 32 | loss: 0.27254 | val_0_auc: 0.91747 |  0:04:31s\n",
      "epoch 33 | loss: 0.27031 | val_0_auc: 0.91693 |  0:04:44s\n",
      "epoch 34 | loss: 0.26917 | val_0_auc: 0.91682 |  0:04:51s\n",
      "epoch 35 | loss: 0.26902 | val_0_auc: 0.91557 |  0:05:00s\n",
      "epoch 36 | loss: 0.26783 | val_0_auc: 0.91658 |  0:05:08s\n",
      "epoch 37 | loss: 0.26687 | val_0_auc: 0.91376 |  0:05:15s\n",
      "epoch 38 | loss: 0.26634 | val_0_auc: 0.91444 |  0:05:23s\n",
      "epoch 39 | loss: 0.26544 | val_0_auc: 0.91487 |  0:05:30s\n",
      "epoch 40 | loss: 0.26421 | val_0_auc: 0.91448 |  0:05:36s\n",
      "epoch 41 | loss: 0.26113 | val_0_auc: 0.91551 |  0:05:43s\n",
      "epoch 42 | loss: 0.264   | val_0_auc: 0.91373 |  0:05:51s\n",
      "epoch 43 | loss: 0.2597  | val_0_auc: 0.91388 |  0:05:59s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.92017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6847  | val_0_auc: 0.59667 |  0:00:04s\n",
      "epoch 1  | loss: 0.52385 | val_0_auc: 0.70255 |  0:00:10s\n",
      "epoch 2  | loss: 0.46815 | val_0_auc: 0.78391 |  0:00:14s\n",
      "epoch 3  | loss: 0.43346 | val_0_auc: 0.77075 |  0:00:19s\n",
      "epoch 4  | loss: 0.40673 | val_0_auc: 0.78607 |  0:00:24s\n",
      "epoch 5  | loss: 0.40058 | val_0_auc: 0.8443  |  0:00:28s\n",
      "epoch 6  | loss: 0.39228 | val_0_auc: 0.83253 |  0:00:32s\n",
      "epoch 7  | loss: 0.38853 | val_0_auc: 0.84475 |  0:00:42s\n",
      "epoch 8  | loss: 0.38438 | val_0_auc: 0.85687 |  0:00:48s\n",
      "epoch 9  | loss: 0.37552 | val_0_auc: 0.85537 |  0:00:54s\n",
      "epoch 10 | loss: 0.375   | val_0_auc: 0.85191 |  0:00:58s\n",
      "epoch 11 | loss: 0.37379 | val_0_auc: 0.85826 |  0:01:03s\n",
      "epoch 12 | loss: 0.37469 | val_0_auc: 0.85725 |  0:01:07s\n",
      "epoch 13 | loss: 0.36823 | val_0_auc: 0.85752 |  0:01:11s\n",
      "epoch 14 | loss: 0.36409 | val_0_auc: 0.85664 |  0:01:15s\n",
      "epoch 15 | loss: 0.38114 | val_0_auc: 0.86194 |  0:01:18s\n",
      "epoch 16 | loss: 0.37569 | val_0_auc: 0.85737 |  0:01:23s\n",
      "epoch 17 | loss: 0.38231 | val_0_auc: 0.84739 |  0:01:27s\n",
      "epoch 18 | loss: 0.37752 | val_0_auc: 0.85633 |  0:01:30s\n",
      "epoch 19 | loss: 0.37507 | val_0_auc: 0.85414 |  0:01:38s\n",
      "epoch 20 | loss: 0.38002 | val_0_auc: 0.85298 |  0:01:43s\n",
      "epoch 21 | loss: 0.37118 | val_0_auc: 0.85731 |  0:01:47s\n",
      "epoch 22 | loss: 0.36996 | val_0_auc: 0.86218 |  0:01:51s\n",
      "epoch 23 | loss: 0.36696 | val_0_auc: 0.85489 |  0:01:55s\n",
      "epoch 24 | loss: 0.36104 | val_0_auc: 0.85925 |  0:01:58s\n",
      "epoch 25 | loss: 0.35496 | val_0_auc: 0.86727 |  0:02:01s\n",
      "epoch 26 | loss: 0.35062 | val_0_auc: 0.87136 |  0:02:04s\n",
      "epoch 27 | loss: 0.34757 | val_0_auc: 0.87335 |  0:02:07s\n",
      "epoch 28 | loss: 0.34465 | val_0_auc: 0.87925 |  0:02:10s\n",
      "epoch 29 | loss: 0.34052 | val_0_auc: 0.87931 |  0:02:13s\n",
      "epoch 30 | loss: 0.33567 | val_0_auc: 0.88028 |  0:02:16s\n",
      "epoch 31 | loss: 0.33518 | val_0_auc: 0.88526 |  0:02:19s\n",
      "epoch 32 | loss: 0.33134 | val_0_auc: 0.88509 |  0:02:23s\n",
      "epoch 33 | loss: 0.32653 | val_0_auc: 0.88885 |  0:02:26s\n",
      "epoch 34 | loss: 0.33463 | val_0_auc: 0.8843  |  0:02:29s\n",
      "epoch 35 | loss: 0.33753 | val_0_auc: 0.88185 |  0:02:32s\n",
      "epoch 36 | loss: 0.34268 | val_0_auc: 0.87952 |  0:02:35s\n",
      "epoch 37 | loss: 0.34472 | val_0_auc: 0.87262 |  0:02:40s\n",
      "epoch 38 | loss: 0.33827 | val_0_auc: 0.8837  |  0:02:45s\n",
      "epoch 39 | loss: 0.33678 | val_0_auc: 0.88143 |  0:02:48s\n",
      "epoch 40 | loss: 0.33149 | val_0_auc: 0.88428 |  0:02:52s\n",
      "epoch 41 | loss: 0.3266  | val_0_auc: 0.88298 |  0:02:56s\n",
      "epoch 42 | loss: 0.32953 | val_0_auc: 0.88637 |  0:03:00s\n",
      "epoch 43 | loss: 0.33868 | val_0_auc: 0.87452 |  0:03:04s\n",
      "epoch 44 | loss: 0.33373 | val_0_auc: 0.88618 |  0:03:08s\n",
      "epoch 45 | loss: 0.32969 | val_0_auc: 0.89121 |  0:03:11s\n",
      "epoch 46 | loss: 0.32514 | val_0_auc: 0.88991 |  0:03:14s\n",
      "epoch 47 | loss: 0.31882 | val_0_auc: 0.89397 |  0:03:18s\n",
      "epoch 48 | loss: 0.31952 | val_0_auc: 0.89306 |  0:03:21s\n",
      "epoch 49 | loss: 0.3138  | val_0_auc: 0.89603 |  0:03:26s\n",
      "epoch 50 | loss: 0.312   | val_0_auc: 0.89454 |  0:03:30s\n",
      "epoch 51 | loss: 0.31898 | val_0_auc: 0.8915  |  0:03:34s\n",
      "epoch 52 | loss: 0.31345 | val_0_auc: 0.89705 |  0:03:37s\n",
      "epoch 53 | loss: 0.31123 | val_0_auc: 0.89704 |  0:03:40s\n",
      "epoch 54 | loss: 0.31102 | val_0_auc: 0.89779 |  0:03:43s\n",
      "epoch 55 | loss: 0.31207 | val_0_auc: 0.89888 |  0:03:46s\n",
      "epoch 56 | loss: 0.31181 | val_0_auc: 0.89742 |  0:03:49s\n",
      "epoch 57 | loss: 0.31964 | val_0_auc: 0.89521 |  0:03:52s\n",
      "epoch 58 | loss: 0.32541 | val_0_auc: 0.88802 |  0:03:55s\n",
      "epoch 59 | loss: 0.33117 | val_0_auc: 0.887   |  0:03:58s\n",
      "epoch 60 | loss: 0.32422 | val_0_auc: 0.89111 |  0:04:01s\n",
      "epoch 61 | loss: 0.31917 | val_0_auc: 0.8948  |  0:04:03s\n",
      "epoch 62 | loss: 0.31986 | val_0_auc: 0.89319 |  0:04:06s\n",
      "epoch 63 | loss: 0.32417 | val_0_auc: 0.88161 |  0:04:09s\n",
      "epoch 64 | loss: 0.32675 | val_0_auc: 0.89095 |  0:04:13s\n",
      "epoch 65 | loss: 0.31582 | val_0_auc: 0.89392 |  0:04:16s\n",
      "epoch 66 | loss: 0.31296 | val_0_auc: 0.89561 |  0:04:19s\n",
      "epoch 67 | loss: 0.31146 | val_0_auc: 0.89669 |  0:04:24s\n",
      "epoch 68 | loss: 0.30729 | val_0_auc: 0.89825 |  0:04:30s\n",
      "epoch 69 | loss: 0.30934 | val_0_auc: 0.89821 |  0:04:35s\n",
      "epoch 70 | loss: 0.30729 | val_0_auc: 0.90031 |  0:04:38s\n",
      "epoch 71 | loss: 0.30696 | val_0_auc: 0.89971 |  0:04:42s\n",
      "epoch 72 | loss: 0.30353 | val_0_auc: 0.89964 |  0:04:45s\n",
      "epoch 73 | loss: 0.30424 | val_0_auc: 0.90048 |  0:04:48s\n",
      "epoch 74 | loss: 0.30258 | val_0_auc: 0.90092 |  0:04:52s\n",
      "epoch 75 | loss: 0.30309 | val_0_auc: 0.89859 |  0:04:55s\n",
      "epoch 76 | loss: 0.30469 | val_0_auc: 0.89853 |  0:05:01s\n",
      "epoch 77 | loss: 0.30333 | val_0_auc: 0.89939 |  0:05:06s\n",
      "epoch 78 | loss: 0.30442 | val_0_auc: 0.90035 |  0:05:10s\n",
      "epoch 79 | loss: 0.30383 | val_0_auc: 0.90127 |  0:05:13s\n",
      "epoch 80 | loss: 0.30414 | val_0_auc: 0.90154 |  0:05:16s\n",
      "epoch 81 | loss: 0.30635 | val_0_auc: 0.89526 |  0:05:19s\n",
      "epoch 82 | loss: 0.30853 | val_0_auc: 0.89805 |  0:05:22s\n",
      "epoch 83 | loss: 0.30545 | val_0_auc: 0.89845 |  0:05:26s\n",
      "epoch 84 | loss: 0.30245 | val_0_auc: 0.90084 |  0:05:31s\n",
      "epoch 85 | loss: 0.30075 | val_0_auc: 0.90106 |  0:05:36s\n",
      "epoch 86 | loss: 0.29927 | val_0_auc: 0.90274 |  0:05:41s\n",
      "epoch 87 | loss: 0.29973 | val_0_auc: 0.90378 |  0:05:46s\n",
      "epoch 88 | loss: 0.29986 | val_0_auc: 0.90208 |  0:05:51s\n",
      "epoch 89 | loss: 0.30032 | val_0_auc: 0.90314 |  0:05:56s\n",
      "epoch 90 | loss: 0.30207 | val_0_auc: 0.90359 |  0:06:00s\n",
      "epoch 91 | loss: 0.29972 | val_0_auc: 0.904   |  0:06:06s\n",
      "epoch 92 | loss: 0.29714 | val_0_auc: 0.90348 |  0:06:10s\n",
      "epoch 93 | loss: 0.30054 | val_0_auc: 0.90261 |  0:06:15s\n",
      "epoch 94 | loss: 0.30475 | val_0_auc: 0.89622 |  0:06:20s\n",
      "epoch 95 | loss: 0.30224 | val_0_auc: 0.89402 |  0:06:27s\n",
      "epoch 96 | loss: 0.30225 | val_0_auc: 0.8958  |  0:06:38s\n",
      "epoch 97 | loss: 0.29835 | val_0_auc: 0.88366 |  0:06:46s\n",
      "epoch 98 | loss: 0.29741 | val_0_auc: 0.87299 |  0:06:57s\n",
      "epoch 99 | loss: 0.2955  | val_0_auc: 0.88658 |  0:07:04s\n",
      "epoch 100| loss: 0.29562 | val_0_auc: 0.8997  |  0:07:10s\n",
      "epoch 101| loss: 0.29424 | val_0_auc: 0.90053 |  0:07:17s\n",
      "epoch 102| loss: 0.29663 | val_0_auc: 0.90149 |  0:07:23s\n",
      "epoch 103| loss: 0.29546 | val_0_auc: 0.90081 |  0:07:33s\n",
      "epoch 104| loss: 0.29646 | val_0_auc: 0.90376 |  0:07:39s\n",
      "epoch 105| loss: 0.29452 | val_0_auc: 0.90409 |  0:07:46s\n",
      "epoch 106| loss: 0.29346 | val_0_auc: 0.9024  |  0:07:54s\n",
      "epoch 107| loss: 0.2956  | val_0_auc: 0.9026  |  0:08:00s\n",
      "epoch 108| loss: 0.29513 | val_0_auc: 0.90384 |  0:08:05s\n",
      "epoch 109| loss: 0.2937  | val_0_auc: 0.90456 |  0:08:10s\n",
      "epoch 110| loss: 0.29312 | val_0_auc: 0.90401 |  0:08:23s\n",
      "epoch 111| loss: 0.29353 | val_0_auc: 0.90458 |  0:08:45s\n",
      "epoch 112| loss: 0.29206 | val_0_auc: 0.90548 |  0:08:54s\n",
      "epoch 113| loss: 0.29065 | val_0_auc: 0.9045  |  0:08:59s\n",
      "epoch 114| loss: 0.289   | val_0_auc: 0.90542 |  0:09:04s\n",
      "epoch 115| loss: 0.28946 | val_0_auc: 0.90533 |  0:09:10s\n",
      "epoch 116| loss: 0.2889  | val_0_auc: 0.90707 |  0:09:14s\n",
      "epoch 117| loss: 0.28898 | val_0_auc: 0.90532 |  0:09:19s\n",
      "epoch 118| loss: 0.29378 | val_0_auc: 0.90681 |  0:09:24s\n",
      "epoch 119| loss: 0.29631 | val_0_auc: 0.90554 |  0:09:28s\n",
      "epoch 120| loss: 0.2941  | val_0_auc: 0.90581 |  0:09:33s\n",
      "epoch 121| loss: 0.29339 | val_0_auc: 0.90546 |  0:09:37s\n",
      "epoch 122| loss: 0.29231 | val_0_auc: 0.90615 |  0:09:42s\n",
      "epoch 123| loss: 0.29078 | val_0_auc: 0.90793 |  0:09:47s\n",
      "epoch 124| loss: 0.29123 | val_0_auc: 0.90602 |  0:09:52s\n",
      "epoch 125| loss: 0.29009 | val_0_auc: 0.9069  |  0:09:57s\n",
      "epoch 126| loss: 0.28991 | val_0_auc: 0.90711 |  0:10:01s\n",
      "epoch 127| loss: 0.28879 | val_0_auc: 0.90698 |  0:10:05s\n",
      "epoch 128| loss: 0.28839 | val_0_auc: 0.90584 |  0:10:09s\n",
      "epoch 129| loss: 0.28816 | val_0_auc: 0.90714 |  0:10:14s\n",
      "epoch 130| loss: 0.28617 | val_0_auc: 0.90752 |  0:10:18s\n",
      "epoch 131| loss: 0.28712 | val_0_auc: 0.9058  |  0:10:23s\n",
      "epoch 132| loss: 0.28579 | val_0_auc: 0.90769 |  0:10:27s\n",
      "epoch 133| loss: 0.28647 | val_0_auc: 0.90784 |  0:10:32s\n",
      "epoch 134| loss: 0.28713 | val_0_auc: 0.91016 |  0:10:36s\n",
      "epoch 135| loss: 0.28565 | val_0_auc: 0.91015 |  0:10:41s\n",
      "epoch 136| loss: 0.28647 | val_0_auc: 0.90697 |  0:10:45s\n",
      "epoch 137| loss: 0.28404 | val_0_auc: 0.90376 |  0:10:50s\n",
      "epoch 138| loss: 0.28288 | val_0_auc: 0.90465 |  0:10:54s\n",
      "epoch 139| loss: 0.28351 | val_0_auc: 0.90668 |  0:10:59s\n",
      "epoch 140| loss: 0.28342 | val_0_auc: 0.90832 |  0:11:03s\n",
      "epoch 141| loss: 0.28138 | val_0_auc: 0.91017 |  0:11:08s\n",
      "epoch 142| loss: 0.2841  | val_0_auc: 0.90944 |  0:11:13s\n",
      "epoch 143| loss: 0.28278 | val_0_auc: 0.91138 |  0:11:17s\n",
      "epoch 144| loss: 0.28173 | val_0_auc: 0.90987 |  0:11:22s\n",
      "epoch 145| loss: 0.28171 | val_0_auc: 0.91285 |  0:11:27s\n",
      "epoch 146| loss: 0.27983 | val_0_auc: 0.91158 |  0:11:31s\n",
      "epoch 147| loss: 0.27982 | val_0_auc: 0.91047 |  0:11:37s\n",
      "epoch 148| loss: 0.28277 | val_0_auc: 0.90885 |  0:11:41s\n",
      "epoch 149| loss: 0.28332 | val_0_auc: 0.90923 |  0:11:46s\n",
      "epoch 150| loss: 0.28133 | val_0_auc: 0.90906 |  0:11:50s\n",
      "epoch 151| loss: 0.2831  | val_0_auc: 0.91026 |  0:11:55s\n",
      "epoch 152| loss: 0.28232 | val_0_auc: 0.91008 |  0:12:00s\n",
      "epoch 153| loss: 0.28103 | val_0_auc: 0.91031 |  0:12:05s\n",
      "epoch 154| loss: 0.27965 | val_0_auc: 0.90993 |  0:12:10s\n",
      "epoch 155| loss: 0.28106 | val_0_auc: 0.91042 |  0:12:15s\n",
      "epoch 156| loss: 0.28104 | val_0_auc: 0.91022 |  0:12:19s\n",
      "epoch 157| loss: 0.28113 | val_0_auc: 0.91079 |  0:12:23s\n",
      "epoch 158| loss: 0.28006 | val_0_auc: 0.91054 |  0:12:27s\n",
      "epoch 159| loss: 0.27937 | val_0_auc: 0.91039 |  0:12:32s\n",
      "epoch 160| loss: 0.27934 | val_0_auc: 0.91105 |  0:12:37s\n",
      "epoch 161| loss: 0.27935 | val_0_auc: 0.91119 |  0:12:41s\n",
      "epoch 162| loss: 0.27891 | val_0_auc: 0.91229 |  0:12:48s\n",
      "epoch 163| loss: 0.27972 | val_0_auc: 0.91095 |  0:12:52s\n",
      "epoch 164| loss: 0.27828 | val_0_auc: 0.91081 |  0:12:57s\n",
      "epoch 165| loss: 0.27798 | val_0_auc: 0.91171 |  0:13:01s\n",
      "\n",
      "Early stopping occurred at epoch 165 with best_epoch = 145 and best_val_0_auc = 0.91285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.05    | val_0_auc: 0.46007 |  0:00:05s\n",
      "epoch 1  | loss: 1.01079 | val_0_auc: 0.46024 |  0:00:10s\n",
      "epoch 2  | loss: 0.95414 | val_0_auc: 0.46909 |  0:00:15s\n",
      "epoch 3  | loss: 0.89391 | val_0_auc: 0.48639 |  0:00:20s\n",
      "epoch 4  | loss: 0.867   | val_0_auc: 0.47712 |  0:00:24s\n",
      "epoch 5  | loss: 0.84347 | val_0_auc: 0.48756 |  0:00:29s\n",
      "epoch 6  | loss: 0.82241 | val_0_auc: 0.4868  |  0:00:34s\n",
      "epoch 7  | loss: 0.80059 | val_0_auc: 0.49684 |  0:00:39s\n",
      "epoch 8  | loss: 0.76308 | val_0_auc: 0.52146 |  0:00:44s\n",
      "epoch 9  | loss: 0.74847 | val_0_auc: 0.5203  |  0:00:49s\n",
      "epoch 10 | loss: 0.72158 | val_0_auc: 0.522   |  0:00:54s\n",
      "epoch 11 | loss: 0.70648 | val_0_auc: 0.54357 |  0:00:58s\n",
      "epoch 12 | loss: 0.69138 | val_0_auc: 0.54112 |  0:01:03s\n",
      "epoch 13 | loss: 0.68304 | val_0_auc: 0.55024 |  0:01:08s\n",
      "epoch 14 | loss: 0.68274 | val_0_auc: 0.56164 |  0:01:12s\n",
      "epoch 15 | loss: 0.6709  | val_0_auc: 0.55252 |  0:01:17s\n",
      "epoch 16 | loss: 0.65663 | val_0_auc: 0.58591 |  0:01:23s\n",
      "epoch 17 | loss: 0.64632 | val_0_auc: 0.60088 |  0:01:27s\n",
      "epoch 18 | loss: 0.64796 | val_0_auc: 0.60028 |  0:01:33s\n",
      "epoch 19 | loss: 0.63928 | val_0_auc: 0.59522 |  0:01:38s\n",
      "epoch 20 | loss: 0.63594 | val_0_auc: 0.6087  |  0:01:43s\n",
      "epoch 21 | loss: 0.63286 | val_0_auc: 0.62613 |  0:01:48s\n",
      "epoch 22 | loss: 0.6258  | val_0_auc: 0.64243 |  0:01:52s\n",
      "epoch 23 | loss: 0.61957 | val_0_auc: 0.66612 |  0:01:57s\n",
      "epoch 24 | loss: 0.61222 | val_0_auc: 0.67103 |  0:02:02s\n",
      "epoch 25 | loss: 0.60919 | val_0_auc: 0.66991 |  0:02:06s\n",
      "epoch 26 | loss: 0.60234 | val_0_auc: 0.67423 |  0:02:11s\n",
      "epoch 27 | loss: 0.59913 | val_0_auc: 0.6951  |  0:02:17s\n",
      "epoch 28 | loss: 0.59477 | val_0_auc: 0.69509 |  0:02:22s\n",
      "epoch 29 | loss: 0.59478 | val_0_auc: 0.71043 |  0:02:27s\n",
      "epoch 30 | loss: 0.58839 | val_0_auc: 0.69952 |  0:02:32s\n",
      "epoch 31 | loss: 0.58411 | val_0_auc: 0.71169 |  0:02:37s\n",
      "epoch 32 | loss: 0.58314 | val_0_auc: 0.70491 |  0:02:42s\n",
      "epoch 33 | loss: 0.57976 | val_0_auc: 0.7212  |  0:02:47s\n",
      "epoch 34 | loss: 0.57734 | val_0_auc: 0.71276 |  0:02:52s\n",
      "epoch 35 | loss: 0.57577 | val_0_auc: 0.72619 |  0:02:57s\n",
      "epoch 36 | loss: 0.56938 | val_0_auc: 0.74852 |  0:03:01s\n",
      "epoch 37 | loss: 0.56979 | val_0_auc: 0.71164 |  0:03:06s\n",
      "epoch 38 | loss: 0.56341 | val_0_auc: 0.72632 |  0:03:11s\n",
      "epoch 39 | loss: 0.5633  | val_0_auc: 0.72864 |  0:03:19s\n",
      "epoch 40 | loss: 0.55864 | val_0_auc: 0.72331 |  0:03:23s\n",
      "epoch 41 | loss: 0.5626  | val_0_auc: 0.73185 |  0:03:28s\n",
      "epoch 42 | loss: 0.55746 | val_0_auc: 0.72531 |  0:03:33s\n",
      "epoch 43 | loss: 0.55578 | val_0_auc: 0.73045 |  0:03:40s\n",
      "epoch 44 | loss: 0.55504 | val_0_auc: 0.73827 |  0:03:48s\n",
      "epoch 45 | loss: 0.55344 | val_0_auc: 0.73023 |  0:03:55s\n",
      "epoch 46 | loss: 0.55282 | val_0_auc: 0.73143 |  0:04:01s\n",
      "epoch 47 | loss: 0.55049 | val_0_auc: 0.73569 |  0:04:11s\n",
      "epoch 48 | loss: 0.55091 | val_0_auc: 0.72573 |  0:04:21s\n",
      "epoch 49 | loss: 0.55152 | val_0_auc: 0.73789 |  0:04:30s\n",
      "epoch 50 | loss: 0.5479  | val_0_auc: 0.73693 |  0:04:36s\n",
      "epoch 51 | loss: 0.55199 | val_0_auc: 0.7379  |  0:04:42s\n",
      "epoch 52 | loss: 0.54655 | val_0_auc: 0.73426 |  0:04:48s\n",
      "epoch 53 | loss: 0.54583 | val_0_auc: 0.74552 |  0:04:54s\n",
      "epoch 54 | loss: 0.548   | val_0_auc: 0.74724 |  0:05:00s\n",
      "epoch 55 | loss: 0.54202 | val_0_auc: 0.74812 |  0:05:04s\n",
      "epoch 56 | loss: 0.5452  | val_0_auc: 0.74033 |  0:05:09s\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_val_0_auc = 0.74852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72093 | val_0_auc: 0.52461 |  0:00:02s\n",
      "epoch 1  | loss: 0.67693 | val_0_auc: 0.53826 |  0:00:05s\n",
      "epoch 2  | loss: 0.64364 | val_0_auc: 0.55982 |  0:00:08s\n",
      "epoch 3  | loss: 0.61588 | val_0_auc: 0.58306 |  0:00:11s\n",
      "epoch 4  | loss: 0.59343 | val_0_auc: 0.60701 |  0:00:13s\n",
      "epoch 5  | loss: 0.57589 | val_0_auc: 0.63481 |  0:00:17s\n",
      "epoch 6  | loss: 0.55575 | val_0_auc: 0.67354 |  0:00:19s\n",
      "epoch 7  | loss: 0.54625 | val_0_auc: 0.70291 |  0:00:22s\n",
      "epoch 8  | loss: 0.53692 | val_0_auc: 0.72673 |  0:00:25s\n",
      "epoch 9  | loss: 0.52078 | val_0_auc: 0.73636 |  0:00:28s\n",
      "epoch 10 | loss: 0.50755 | val_0_auc: 0.74998 |  0:00:31s\n",
      "epoch 11 | loss: 0.50039 | val_0_auc: 0.76779 |  0:00:34s\n",
      "epoch 12 | loss: 0.49216 | val_0_auc: 0.77273 |  0:00:37s\n",
      "epoch 13 | loss: 0.47949 | val_0_auc: 0.78416 |  0:00:39s\n",
      "epoch 14 | loss: 0.47165 | val_0_auc: 0.78662 |  0:00:42s\n",
      "epoch 15 | loss: 0.46494 | val_0_auc: 0.79063 |  0:00:45s\n",
      "epoch 16 | loss: 0.45845 | val_0_auc: 0.8032  |  0:00:48s\n",
      "epoch 17 | loss: 0.45182 | val_0_auc: 0.80347 |  0:00:51s\n",
      "epoch 18 | loss: 0.44302 | val_0_auc: 0.81118 |  0:00:54s\n",
      "epoch 19 | loss: 0.4425  | val_0_auc: 0.81404 |  0:00:56s\n",
      "epoch 20 | loss: 0.43126 | val_0_auc: 0.81459 |  0:00:59s\n",
      "epoch 21 | loss: 0.43207 | val_0_auc: 0.81777 |  0:01:01s\n",
      "epoch 22 | loss: 0.43004 | val_0_auc: 0.82112 |  0:01:04s\n",
      "epoch 23 | loss: 0.42364 | val_0_auc: 0.82423 |  0:01:07s\n",
      "epoch 24 | loss: 0.41848 | val_0_auc: 0.82564 |  0:01:09s\n",
      "epoch 25 | loss: 0.41546 | val_0_auc: 0.82473 |  0:01:13s\n",
      "epoch 26 | loss: 0.41331 | val_0_auc: 0.83042 |  0:01:15s\n",
      "epoch 27 | loss: 0.40952 | val_0_auc: 0.83309 |  0:01:18s\n",
      "epoch 28 | loss: 0.40682 | val_0_auc: 0.83563 |  0:01:23s\n",
      "epoch 29 | loss: 0.40368 | val_0_auc: 0.83724 |  0:01:27s\n",
      "epoch 30 | loss: 0.40404 | val_0_auc: 0.839   |  0:01:30s\n",
      "epoch 31 | loss: 0.40143 | val_0_auc: 0.84241 |  0:01:33s\n",
      "epoch 32 | loss: 0.39788 | val_0_auc: 0.84407 |  0:01:35s\n",
      "epoch 33 | loss: 0.39614 | val_0_auc: 0.84325 |  0:01:37s\n",
      "epoch 34 | loss: 0.39521 | val_0_auc: 0.84736 |  0:01:39s\n",
      "epoch 35 | loss: 0.39235 | val_0_auc: 0.84971 |  0:01:41s\n",
      "epoch 36 | loss: 0.38953 | val_0_auc: 0.85052 |  0:01:44s\n",
      "epoch 37 | loss: 0.38559 | val_0_auc: 0.85172 |  0:01:47s\n",
      "epoch 38 | loss: 0.38238 | val_0_auc: 0.85411 |  0:01:50s\n",
      "epoch 39 | loss: 0.38451 | val_0_auc: 0.856   |  0:01:52s\n",
      "epoch 40 | loss: 0.37976 | val_0_auc: 0.85639 |  0:01:55s\n",
      "epoch 41 | loss: 0.37737 | val_0_auc: 0.8573  |  0:01:57s\n",
      "epoch 42 | loss: 0.37459 | val_0_auc: 0.85851 |  0:01:59s\n",
      "epoch 43 | loss: 0.37674 | val_0_auc: 0.85949 |  0:02:01s\n",
      "epoch 44 | loss: 0.37371 | val_0_auc: 0.86144 |  0:02:04s\n",
      "epoch 45 | loss: 0.37139 | val_0_auc: 0.86256 |  0:02:06s\n",
      "epoch 46 | loss: 0.36905 | val_0_auc: 0.86139 |  0:02:08s\n",
      "epoch 47 | loss: 0.36998 | val_0_auc: 0.86396 |  0:02:10s\n",
      "epoch 48 | loss: 0.36722 | val_0_auc: 0.86418 |  0:02:12s\n",
      "epoch 49 | loss: 0.36486 | val_0_auc: 0.86554 |  0:02:14s\n",
      "epoch 50 | loss: 0.36021 | val_0_auc: 0.86619 |  0:02:16s\n",
      "epoch 51 | loss: 0.36252 | val_0_auc: 0.86673 |  0:02:18s\n",
      "epoch 52 | loss: 0.36041 | val_0_auc: 0.86753 |  0:02:21s\n",
      "epoch 53 | loss: 0.35756 | val_0_auc: 0.86889 |  0:02:24s\n",
      "epoch 54 | loss: 0.35796 | val_0_auc: 0.8679  |  0:02:26s\n",
      "epoch 55 | loss: 0.3523  | val_0_auc: 0.86981 |  0:02:30s\n",
      "epoch 56 | loss: 0.35761 | val_0_auc: 0.87115 |  0:02:32s\n",
      "epoch 57 | loss: 0.3523  | val_0_auc: 0.87142 |  0:02:35s\n",
      "epoch 58 | loss: 0.35079 | val_0_auc: 0.87041 |  0:02:37s\n",
      "epoch 59 | loss: 0.3495  | val_0_auc: 0.87209 |  0:02:39s\n",
      "epoch 60 | loss: 0.35294 | val_0_auc: 0.87319 |  0:02:42s\n",
      "epoch 61 | loss: 0.34817 | val_0_auc: 0.8728  |  0:02:44s\n",
      "epoch 62 | loss: 0.34782 | val_0_auc: 0.8732  |  0:02:46s\n",
      "epoch 63 | loss: 0.34561 | val_0_auc: 0.87379 |  0:02:49s\n",
      "epoch 64 | loss: 0.34383 | val_0_auc: 0.87384 |  0:02:51s\n",
      "epoch 65 | loss: 0.3448  | val_0_auc: 0.87432 |  0:02:53s\n",
      "epoch 66 | loss: 0.34418 | val_0_auc: 0.87411 |  0:02:56s\n",
      "epoch 67 | loss: 0.34084 | val_0_auc: 0.87559 |  0:02:59s\n",
      "epoch 68 | loss: 0.3401  | val_0_auc: 0.87772 |  0:03:01s\n",
      "epoch 69 | loss: 0.33962 | val_0_auc: 0.87747 |  0:03:04s\n",
      "epoch 70 | loss: 0.33827 | val_0_auc: 0.87781 |  0:03:06s\n",
      "epoch 71 | loss: 0.33698 | val_0_auc: 0.87962 |  0:03:08s\n",
      "epoch 72 | loss: 0.33674 | val_0_auc: 0.88083 |  0:03:10s\n",
      "epoch 73 | loss: 0.33736 | val_0_auc: 0.88021 |  0:03:12s\n",
      "epoch 74 | loss: 0.33462 | val_0_auc: 0.87971 |  0:03:14s\n",
      "epoch 75 | loss: 0.33572 | val_0_auc: 0.88075 |  0:03:16s\n",
      "epoch 76 | loss: 0.33356 | val_0_auc: 0.88103 |  0:03:18s\n",
      "epoch 77 | loss: 0.3322  | val_0_auc: 0.88038 |  0:03:20s\n",
      "epoch 78 | loss: 0.33196 | val_0_auc: 0.88176 |  0:03:22s\n",
      "epoch 79 | loss: 0.32881 | val_0_auc: 0.88233 |  0:03:25s\n",
      "epoch 80 | loss: 0.32948 | val_0_auc: 0.88193 |  0:03:28s\n",
      "epoch 81 | loss: 0.3296  | val_0_auc: 0.88176 |  0:03:30s\n",
      "epoch 82 | loss: 0.32947 | val_0_auc: 0.88213 |  0:03:32s\n",
      "epoch 83 | loss: 0.32932 | val_0_auc: 0.8829  |  0:03:34s\n",
      "epoch 84 | loss: 0.32581 | val_0_auc: 0.88284 |  0:03:37s\n",
      "epoch 85 | loss: 0.32633 | val_0_auc: 0.88339 |  0:03:39s\n",
      "epoch 86 | loss: 0.32534 | val_0_auc: 0.88434 |  0:03:41s\n",
      "epoch 87 | loss: 0.32429 | val_0_auc: 0.88466 |  0:03:43s\n",
      "epoch 88 | loss: 0.32645 | val_0_auc: 0.88566 |  0:03:45s\n",
      "epoch 89 | loss: 0.32335 | val_0_auc: 0.88565 |  0:03:47s\n",
      "epoch 90 | loss: 0.32015 | val_0_auc: 0.88578 |  0:03:49s\n",
      "epoch 91 | loss: 0.32232 | val_0_auc: 0.88687 |  0:03:51s\n",
      "epoch 92 | loss: 0.32031 | val_0_auc: 0.88676 |  0:03:53s\n",
      "epoch 93 | loss: 0.31942 | val_0_auc: 0.88734 |  0:03:55s\n",
      "epoch 94 | loss: 0.31797 | val_0_auc: 0.88777 |  0:03:58s\n",
      "epoch 95 | loss: 0.31403 | val_0_auc: 0.88824 |  0:04:00s\n",
      "epoch 96 | loss: 0.31734 | val_0_auc: 0.88734 |  0:04:02s\n",
      "epoch 97 | loss: 0.31636 | val_0_auc: 0.8888  |  0:04:04s\n",
      "epoch 98 | loss: 0.31875 | val_0_auc: 0.88975 |  0:04:06s\n",
      "epoch 99 | loss: 0.31668 | val_0_auc: 0.88982 |  0:04:08s\n",
      "epoch 100| loss: 0.3143  | val_0_auc: 0.88964 |  0:04:10s\n",
      "epoch 101| loss: 0.31165 | val_0_auc: 0.89049 |  0:04:12s\n",
      "epoch 102| loss: 0.30979 | val_0_auc: 0.89039 |  0:04:14s\n",
      "epoch 103| loss: 0.31169 | val_0_auc: 0.89097 |  0:04:16s\n",
      "epoch 104| loss: 0.31075 | val_0_auc: 0.89039 |  0:04:18s\n",
      "epoch 105| loss: 0.31129 | val_0_auc: 0.8906  |  0:04:21s\n",
      "epoch 106| loss: 0.30819 | val_0_auc: 0.89027 |  0:04:23s\n",
      "epoch 107| loss: 0.30793 | val_0_auc: 0.8903  |  0:04:26s\n",
      "epoch 108| loss: 0.30739 | val_0_auc: 0.89105 |  0:04:28s\n",
      "epoch 109| loss: 0.30798 | val_0_auc: 0.89124 |  0:04:30s\n",
      "epoch 110| loss: 0.30886 | val_0_auc: 0.89115 |  0:04:33s\n",
      "epoch 111| loss: 0.30575 | val_0_auc: 0.89144 |  0:04:35s\n",
      "epoch 112| loss: 0.304   | val_0_auc: 0.89039 |  0:04:37s\n",
      "epoch 113| loss: 0.30614 | val_0_auc: 0.89096 |  0:04:39s\n",
      "epoch 114| loss: 0.30211 | val_0_auc: 0.89114 |  0:04:41s\n",
      "epoch 115| loss: 0.30382 | val_0_auc: 0.89158 |  0:04:43s\n",
      "epoch 116| loss: 0.3036  | val_0_auc: 0.89203 |  0:04:45s\n",
      "epoch 117| loss: 0.30259 | val_0_auc: 0.89144 |  0:04:47s\n",
      "epoch 118| loss: 0.3034  | val_0_auc: 0.89167 |  0:04:49s\n",
      "epoch 119| loss: 0.30128 | val_0_auc: 0.89173 |  0:04:51s\n",
      "epoch 120| loss: 0.30092 | val_0_auc: 0.89152 |  0:04:53s\n",
      "epoch 121| loss: 0.30023 | val_0_auc: 0.89235 |  0:04:55s\n",
      "epoch 122| loss: 0.29988 | val_0_auc: 0.89206 |  0:04:58s\n",
      "epoch 123| loss: 0.30051 | val_0_auc: 0.89242 |  0:04:59s\n",
      "epoch 124| loss: 0.30042 | val_0_auc: 0.89213 |  0:05:01s\n",
      "epoch 125| loss: 0.29785 | val_0_auc: 0.89221 |  0:05:04s\n",
      "epoch 126| loss: 0.29675 | val_0_auc: 0.89169 |  0:05:05s\n",
      "epoch 127| loss: 0.29546 | val_0_auc: 0.89172 |  0:05:08s\n",
      "epoch 128| loss: 0.29801 | val_0_auc: 0.89155 |  0:05:10s\n",
      "epoch 129| loss: 0.29348 | val_0_auc: 0.89265 |  0:05:12s\n",
      "epoch 130| loss: 0.29512 | val_0_auc: 0.89247 |  0:05:14s\n",
      "epoch 131| loss: 0.29483 | val_0_auc: 0.89262 |  0:05:16s\n",
      "epoch 132| loss: 0.29315 | val_0_auc: 0.89187 |  0:05:19s\n",
      "epoch 133| loss: 0.29199 | val_0_auc: 0.89178 |  0:05:21s\n",
      "epoch 134| loss: 0.29254 | val_0_auc: 0.89141 |  0:05:24s\n",
      "epoch 135| loss: 0.29192 | val_0_auc: 0.89211 |  0:05:26s\n",
      "epoch 136| loss: 0.29102 | val_0_auc: 0.89101 |  0:05:29s\n",
      "epoch 137| loss: 0.28783 | val_0_auc: 0.8913  |  0:05:31s\n",
      "epoch 138| loss: 0.28836 | val_0_auc: 0.89129 |  0:05:34s\n",
      "epoch 139| loss: 0.28867 | val_0_auc: 0.89159 |  0:05:36s\n",
      "epoch 140| loss: 0.2914  | val_0_auc: 0.89105 |  0:05:38s\n",
      "epoch 141| loss: 0.28947 | val_0_auc: 0.89224 |  0:05:41s\n",
      "epoch 142| loss: 0.2864  | val_0_auc: 0.89196 |  0:05:43s\n",
      "epoch 143| loss: 0.28719 | val_0_auc: 0.89156 |  0:05:46s\n",
      "epoch 144| loss: 0.28844 | val_0_auc: 0.89155 |  0:05:48s\n",
      "epoch 145| loss: 0.28306 | val_0_auc: 0.8934  |  0:05:51s\n",
      "epoch 146| loss: 0.28464 | val_0_auc: 0.89291 |  0:05:53s\n",
      "epoch 147| loss: 0.28433 | val_0_auc: 0.89204 |  0:05:56s\n",
      "epoch 148| loss: 0.2816  | val_0_auc: 0.89333 |  0:05:59s\n",
      "epoch 149| loss: 0.28388 | val_0_auc: 0.89211 |  0:06:01s\n",
      "epoch 150| loss: 0.2833  | val_0_auc: 0.89423 |  0:06:03s\n",
      "epoch 151| loss: 0.28197 | val_0_auc: 0.89201 |  0:06:05s\n",
      "epoch 152| loss: 0.28145 | val_0_auc: 0.89239 |  0:06:07s\n",
      "epoch 153| loss: 0.28175 | val_0_auc: 0.89158 |  0:06:09s\n",
      "epoch 154| loss: 0.28045 | val_0_auc: 0.89133 |  0:06:12s\n",
      "epoch 155| loss: 0.28053 | val_0_auc: 0.8912  |  0:06:13s\n",
      "epoch 156| loss: 0.27935 | val_0_auc: 0.89153 |  0:06:15s\n",
      "epoch 157| loss: 0.27942 | val_0_auc: 0.8907  |  0:06:17s\n",
      "epoch 158| loss: 0.27773 | val_0_auc: 0.89076 |  0:06:19s\n",
      "epoch 159| loss: 0.27683 | val_0_auc: 0.89192 |  0:06:21s\n",
      "epoch 160| loss: 0.27973 | val_0_auc: 0.89158 |  0:06:23s\n",
      "epoch 161| loss: 0.2768  | val_0_auc: 0.89124 |  0:06:25s\n",
      "epoch 162| loss: 0.27566 | val_0_auc: 0.89092 |  0:06:27s\n",
      "epoch 163| loss: 0.27707 | val_0_auc: 0.89084 |  0:06:29s\n",
      "epoch 164| loss: 0.27606 | val_0_auc: 0.89183 |  0:06:31s\n",
      "epoch 165| loss: 0.27445 | val_0_auc: 0.89202 |  0:06:33s\n",
      "epoch 166| loss: 0.27507 | val_0_auc: 0.89275 |  0:06:35s\n",
      "epoch 167| loss: 0.27588 | val_0_auc: 0.89365 |  0:06:37s\n",
      "epoch 168| loss: 0.27423 | val_0_auc: 0.89277 |  0:06:39s\n",
      "epoch 169| loss: 0.2711  | val_0_auc: 0.89177 |  0:06:41s\n",
      "epoch 170| loss: 0.27051 | val_0_auc: 0.89038 |  0:06:44s\n",
      "\n",
      "Early stopping occurred at epoch 170 with best_epoch = 150 and best_val_0_auc = 0.89423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.5752  | val_0_auc: 0.7533  |  0:00:00s\n",
      "epoch 1  | loss: 0.43599 | val_0_auc: 0.81528 |  0:00:01s\n",
      "epoch 2  | loss: 0.38554 | val_0_auc: 0.84539 |  0:00:02s\n",
      "epoch 3  | loss: 0.35341 | val_0_auc: 0.85716 |  0:00:03s\n",
      "epoch 4  | loss: 0.3356  | val_0_auc: 0.87009 |  0:00:05s\n",
      "epoch 5  | loss: 0.32799 | val_0_auc: 0.87753 |  0:00:05s\n",
      "epoch 6  | loss: 0.31842 | val_0_auc: 0.88676 |  0:00:06s\n",
      "epoch 7  | loss: 0.31088 | val_0_auc: 0.89136 |  0:00:07s\n",
      "epoch 8  | loss: 0.30489 | val_0_auc: 0.89174 |  0:00:08s\n",
      "epoch 9  | loss: 0.30075 | val_0_auc: 0.89611 |  0:00:09s\n",
      "epoch 10 | loss: 0.29759 | val_0_auc: 0.8936  |  0:00:10s\n",
      "epoch 11 | loss: 0.29554 | val_0_auc: 0.89649 |  0:00:11s\n",
      "epoch 12 | loss: 0.28921 | val_0_auc: 0.89847 |  0:00:12s\n",
      "epoch 13 | loss: 0.28634 | val_0_auc: 0.89645 |  0:00:13s\n",
      "epoch 14 | loss: 0.28352 | val_0_auc: 0.89707 |  0:00:14s\n",
      "epoch 15 | loss: 0.28063 | val_0_auc: 0.89859 |  0:00:15s\n",
      "epoch 16 | loss: 0.27874 | val_0_auc: 0.89746 |  0:00:16s\n",
      "epoch 17 | loss: 0.27949 | val_0_auc: 0.89929 |  0:00:17s\n",
      "epoch 18 | loss: 0.27431 | val_0_auc: 0.90278 |  0:00:18s\n",
      "epoch 19 | loss: 0.27221 | val_0_auc: 0.89808 |  0:00:19s\n",
      "epoch 20 | loss: 0.26873 | val_0_auc: 0.90147 |  0:00:20s\n",
      "epoch 21 | loss: 0.26819 | val_0_auc: 0.89744 |  0:00:21s\n",
      "epoch 22 | loss: 0.2624  | val_0_auc: 0.89716 |  0:00:22s\n",
      "epoch 23 | loss: 0.26025 | val_0_auc: 0.89804 |  0:00:23s\n",
      "epoch 24 | loss: 0.26006 | val_0_auc: 0.90149 |  0:00:24s\n",
      "epoch 25 | loss: 0.25649 | val_0_auc: 0.89509 |  0:00:25s\n",
      "epoch 26 | loss: 0.25678 | val_0_auc: 0.8972  |  0:00:27s\n",
      "epoch 27 | loss: 0.25204 | val_0_auc: 0.89848 |  0:00:28s\n",
      "epoch 28 | loss: 0.25186 | val_0_auc: 0.89992 |  0:00:29s\n",
      "epoch 29 | loss: 0.25103 | val_0_auc: 0.90119 |  0:00:30s\n",
      "epoch 30 | loss: 0.24593 | val_0_auc: 0.89884 |  0:00:31s\n",
      "epoch 31 | loss: 0.24427 | val_0_auc: 0.8994  |  0:00:31s\n",
      "epoch 32 | loss: 0.24121 | val_0_auc: 0.89515 |  0:00:32s\n",
      "epoch 33 | loss: 0.24375 | val_0_auc: 0.89612 |  0:00:33s\n",
      "epoch 34 | loss: 0.23632 | val_0_auc: 0.89194 |  0:00:34s\n",
      "epoch 35 | loss: 0.24173 | val_0_auc: 0.89605 |  0:00:35s\n",
      "epoch 36 | loss: 0.23996 | val_0_auc: 0.89421 |  0:00:36s\n",
      "epoch 37 | loss: 0.23557 | val_0_auc: 0.89481 |  0:00:37s\n",
      "epoch 38 | loss: 0.2349  | val_0_auc: 0.89339 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_auc = 0.90278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72887 | val_0_auc: 0.46098 |  0:00:02s\n",
      "epoch 1  | loss: 0.69163 | val_0_auc: 0.48007 |  0:00:04s\n",
      "epoch 2  | loss: 0.65053 | val_0_auc: 0.49532 |  0:00:07s\n",
      "epoch 3  | loss: 0.63611 | val_0_auc: 0.50776 |  0:00:09s\n",
      "epoch 4  | loss: 0.61287 | val_0_auc: 0.52765 |  0:00:12s\n",
      "epoch 5  | loss: 0.59863 | val_0_auc: 0.56273 |  0:00:14s\n",
      "epoch 6  | loss: 0.58977 | val_0_auc: 0.613   |  0:00:16s\n",
      "epoch 7  | loss: 0.56314 | val_0_auc: 0.6581  |  0:00:19s\n",
      "epoch 8  | loss: 0.55197 | val_0_auc: 0.70642 |  0:00:21s\n",
      "epoch 9  | loss: 0.54038 | val_0_auc: 0.71905 |  0:00:23s\n",
      "epoch 10 | loss: 0.52864 | val_0_auc: 0.73448 |  0:00:26s\n",
      "epoch 11 | loss: 0.52401 | val_0_auc: 0.73779 |  0:00:28s\n",
      "epoch 12 | loss: 0.51189 | val_0_auc: 0.74254 |  0:00:31s\n",
      "epoch 13 | loss: 0.50534 | val_0_auc: 0.75561 |  0:00:33s\n",
      "epoch 14 | loss: 0.49242 | val_0_auc: 0.75796 |  0:00:36s\n",
      "epoch 15 | loss: 0.48461 | val_0_auc: 0.77032 |  0:00:39s\n",
      "epoch 16 | loss: 0.48639 | val_0_auc: 0.77622 |  0:00:41s\n",
      "epoch 17 | loss: 0.47862 | val_0_auc: 0.77521 |  0:00:44s\n",
      "epoch 18 | loss: 0.47389 | val_0_auc: 0.78123 |  0:00:46s\n",
      "epoch 19 | loss: 0.47006 | val_0_auc: 0.78691 |  0:00:49s\n",
      "epoch 20 | loss: 0.46681 | val_0_auc: 0.78742 |  0:00:51s\n",
      "epoch 21 | loss: 0.4622  | val_0_auc: 0.79034 |  0:00:53s\n",
      "epoch 22 | loss: 0.45899 | val_0_auc: 0.79917 |  0:00:56s\n",
      "epoch 23 | loss: 0.4585  | val_0_auc: 0.79829 |  0:00:58s\n",
      "epoch 24 | loss: 0.44871 | val_0_auc: 0.80401 |  0:01:00s\n",
      "epoch 25 | loss: 0.45004 | val_0_auc: 0.80317 |  0:01:03s\n",
      "epoch 26 | loss: 0.44908 | val_0_auc: 0.8011  |  0:01:05s\n",
      "epoch 27 | loss: 0.44433 | val_0_auc: 0.80397 |  0:01:08s\n",
      "epoch 28 | loss: 0.44081 | val_0_auc: 0.80834 |  0:01:10s\n",
      "epoch 29 | loss: 0.44025 | val_0_auc: 0.80645 |  0:01:13s\n",
      "epoch 30 | loss: 0.43206 | val_0_auc: 0.80685 |  0:01:16s\n",
      "epoch 31 | loss: 0.43274 | val_0_auc: 0.80901 |  0:01:18s\n",
      "epoch 32 | loss: 0.42932 | val_0_auc: 0.81283 |  0:01:20s\n",
      "epoch 33 | loss: 0.42828 | val_0_auc: 0.81681 |  0:01:23s\n",
      "epoch 34 | loss: 0.42832 | val_0_auc: 0.81901 |  0:01:26s\n",
      "epoch 35 | loss: 0.42215 | val_0_auc: 0.82063 |  0:01:28s\n",
      "epoch 36 | loss: 0.4261  | val_0_auc: 0.82424 |  0:01:31s\n",
      "epoch 37 | loss: 0.42498 | val_0_auc: 0.82311 |  0:01:33s\n",
      "epoch 38 | loss: 0.42107 | val_0_auc: 0.82277 |  0:01:36s\n",
      "epoch 39 | loss: 0.41532 | val_0_auc: 0.82422 |  0:01:38s\n",
      "epoch 40 | loss: 0.41715 | val_0_auc: 0.82448 |  0:01:41s\n",
      "epoch 41 | loss: 0.41582 | val_0_auc: 0.82929 |  0:01:45s\n",
      "epoch 42 | loss: 0.41067 | val_0_auc: 0.82858 |  0:01:48s\n",
      "epoch 43 | loss: 0.41141 | val_0_auc: 0.82792 |  0:01:50s\n",
      "epoch 44 | loss: 0.40932 | val_0_auc: 0.83051 |  0:01:53s\n",
      "epoch 45 | loss: 0.41121 | val_0_auc: 0.8285  |  0:01:55s\n",
      "epoch 46 | loss: 0.4067  | val_0_auc: 0.82911 |  0:01:58s\n",
      "epoch 47 | loss: 0.40765 | val_0_auc: 0.83001 |  0:02:00s\n",
      "epoch 48 | loss: 0.40527 | val_0_auc: 0.83277 |  0:02:02s\n",
      "epoch 49 | loss: 0.40309 | val_0_auc: 0.83416 |  0:02:06s\n",
      "epoch 50 | loss: 0.40249 | val_0_auc: 0.83745 |  0:02:09s\n",
      "epoch 51 | loss: 0.40038 | val_0_auc: 0.83753 |  0:02:11s\n",
      "epoch 52 | loss: 0.39926 | val_0_auc: 0.83989 |  0:02:14s\n",
      "epoch 53 | loss: 0.40271 | val_0_auc: 0.83905 |  0:02:17s\n",
      "epoch 54 | loss: 0.39744 | val_0_auc: 0.8386  |  0:02:19s\n",
      "epoch 55 | loss: 0.39401 | val_0_auc: 0.83413 |  0:02:22s\n",
      "epoch 56 | loss: 0.39688 | val_0_auc: 0.83796 |  0:02:25s\n",
      "epoch 57 | loss: 0.39312 | val_0_auc: 0.8386  |  0:02:29s\n",
      "epoch 58 | loss: 0.39646 | val_0_auc: 0.84039 |  0:02:31s\n",
      "epoch 59 | loss: 0.39311 | val_0_auc: 0.84316 |  0:02:34s\n",
      "epoch 60 | loss: 0.3946  | val_0_auc: 0.84047 |  0:02:37s\n",
      "epoch 61 | loss: 0.39455 | val_0_auc: 0.84429 |  0:02:39s\n",
      "epoch 62 | loss: 0.38622 | val_0_auc: 0.84597 |  0:02:42s\n",
      "epoch 63 | loss: 0.38916 | val_0_auc: 0.84497 |  0:02:45s\n",
      "epoch 64 | loss: 0.38644 | val_0_auc: 0.84794 |  0:02:48s\n",
      "epoch 65 | loss: 0.38181 | val_0_auc: 0.85049 |  0:02:50s\n",
      "epoch 66 | loss: 0.38213 | val_0_auc: 0.85039 |  0:02:53s\n",
      "epoch 67 | loss: 0.38527 | val_0_auc: 0.85051 |  0:02:56s\n",
      "epoch 68 | loss: 0.38305 | val_0_auc: 0.85177 |  0:02:58s\n",
      "epoch 69 | loss: 0.3823  | val_0_auc: 0.85153 |  0:03:01s\n",
      "epoch 70 | loss: 0.38234 | val_0_auc: 0.85275 |  0:03:03s\n",
      "epoch 71 | loss: 0.38225 | val_0_auc: 0.85437 |  0:03:06s\n",
      "epoch 72 | loss: 0.37951 | val_0_auc: 0.85428 |  0:03:08s\n",
      "epoch 73 | loss: 0.37706 | val_0_auc: 0.85455 |  0:03:10s\n",
      "epoch 74 | loss: 0.37685 | val_0_auc: 0.8558  |  0:03:13s\n",
      "epoch 75 | loss: 0.37685 | val_0_auc: 0.85698 |  0:03:15s\n",
      "epoch 76 | loss: 0.37536 | val_0_auc: 0.85461 |  0:03:17s\n",
      "epoch 77 | loss: 0.37452 | val_0_auc: 0.85678 |  0:03:20s\n",
      "epoch 78 | loss: 0.37181 | val_0_auc: 0.8543  |  0:03:22s\n",
      "epoch 79 | loss: 0.37092 | val_0_auc: 0.85559 |  0:03:24s\n",
      "epoch 80 | loss: 0.37266 | val_0_auc: 0.85845 |  0:03:27s\n",
      "epoch 81 | loss: 0.36824 | val_0_auc: 0.85726 |  0:03:29s\n",
      "epoch 82 | loss: 0.36804 | val_0_auc: 0.85614 |  0:03:32s\n",
      "epoch 83 | loss: 0.36678 | val_0_auc: 0.85647 |  0:03:34s\n",
      "epoch 84 | loss: 0.36638 | val_0_auc: 0.85861 |  0:03:36s\n",
      "epoch 85 | loss: 0.36758 | val_0_auc: 0.86109 |  0:03:39s\n",
      "epoch 86 | loss: 0.36338 | val_0_auc: 0.86191 |  0:03:41s\n",
      "epoch 87 | loss: 0.36251 | val_0_auc: 0.86133 |  0:03:43s\n",
      "epoch 88 | loss: 0.36088 | val_0_auc: 0.86188 |  0:03:46s\n",
      "epoch 89 | loss: 0.35711 | val_0_auc: 0.86439 |  0:03:48s\n",
      "epoch 90 | loss: 0.35763 | val_0_auc: 0.86376 |  0:03:50s\n",
      "epoch 91 | loss: 0.35882 | val_0_auc: 0.86523 |  0:03:53s\n",
      "epoch 92 | loss: 0.35611 | val_0_auc: 0.86566 |  0:03:55s\n",
      "epoch 93 | loss: 0.35441 | val_0_auc: 0.86763 |  0:03:58s\n",
      "epoch 94 | loss: 0.35562 | val_0_auc: 0.86578 |  0:04:00s\n",
      "epoch 95 | loss: 0.35326 | val_0_auc: 0.86672 |  0:04:02s\n",
      "epoch 96 | loss: 0.35315 | val_0_auc: 0.86896 |  0:04:05s\n",
      "epoch 97 | loss: 0.35208 | val_0_auc: 0.86989 |  0:04:07s\n",
      "epoch 98 | loss: 0.34953 | val_0_auc: 0.86985 |  0:04:09s\n",
      "epoch 99 | loss: 0.34896 | val_0_auc: 0.86994 |  0:04:12s\n",
      "epoch 100| loss: 0.34732 | val_0_auc: 0.87179 |  0:04:14s\n",
      "epoch 101| loss: 0.34817 | val_0_auc: 0.87174 |  0:04:17s\n",
      "epoch 102| loss: 0.34731 | val_0_auc: 0.87396 |  0:04:19s\n",
      "epoch 103| loss: 0.34778 | val_0_auc: 0.87211 |  0:04:21s\n",
      "epoch 104| loss: 0.34569 | val_0_auc: 0.87454 |  0:04:24s\n",
      "epoch 105| loss: 0.34451 | val_0_auc: 0.87556 |  0:04:26s\n",
      "epoch 106| loss: 0.34534 | val_0_auc: 0.87549 |  0:04:28s\n",
      "epoch 107| loss: 0.34497 | val_0_auc: 0.87648 |  0:04:31s\n",
      "epoch 108| loss: 0.34261 | val_0_auc: 0.8764  |  0:04:33s\n",
      "epoch 109| loss: 0.34215 | val_0_auc: 0.87706 |  0:04:35s\n",
      "epoch 110| loss: 0.33981 | val_0_auc: 0.87749 |  0:04:38s\n",
      "epoch 111| loss: 0.3415  | val_0_auc: 0.87905 |  0:04:40s\n",
      "epoch 112| loss: 0.33857 | val_0_auc: 0.8798  |  0:04:42s\n",
      "epoch 113| loss: 0.33844 | val_0_auc: 0.87908 |  0:04:45s\n",
      "epoch 114| loss: 0.33663 | val_0_auc: 0.87987 |  0:04:47s\n",
      "epoch 115| loss: 0.33785 | val_0_auc: 0.88012 |  0:04:49s\n",
      "epoch 116| loss: 0.3358  | val_0_auc: 0.88001 |  0:04:52s\n",
      "epoch 117| loss: 0.33372 | val_0_auc: 0.87953 |  0:04:54s\n",
      "epoch 118| loss: 0.33596 | val_0_auc: 0.8791  |  0:04:56s\n",
      "epoch 119| loss: 0.33176 | val_0_auc: 0.8809  |  0:04:59s\n",
      "epoch 120| loss: 0.33044 | val_0_auc: 0.88202 |  0:05:01s\n",
      "epoch 121| loss: 0.33258 | val_0_auc: 0.88136 |  0:05:03s\n",
      "epoch 122| loss: 0.33158 | val_0_auc: 0.88252 |  0:05:06s\n",
      "epoch 123| loss: 0.33062 | val_0_auc: 0.88226 |  0:05:08s\n",
      "epoch 124| loss: 0.32903 | val_0_auc: 0.8836  |  0:05:10s\n",
      "epoch 125| loss: 0.32917 | val_0_auc: 0.88363 |  0:05:13s\n",
      "epoch 126| loss: 0.32888 | val_0_auc: 0.88434 |  0:05:15s\n",
      "epoch 127| loss: 0.32885 | val_0_auc: 0.88495 |  0:05:17s\n",
      "epoch 128| loss: 0.32493 | val_0_auc: 0.88401 |  0:05:20s\n",
      "epoch 129| loss: 0.32405 | val_0_auc: 0.88535 |  0:05:22s\n",
      "epoch 130| loss: 0.32475 | val_0_auc: 0.88527 |  0:05:24s\n",
      "epoch 131| loss: 0.32231 | val_0_auc: 0.8848  |  0:05:27s\n",
      "epoch 132| loss: 0.3224  | val_0_auc: 0.88596 |  0:05:29s\n",
      "epoch 133| loss: 0.3219  | val_0_auc: 0.88635 |  0:05:31s\n",
      "epoch 134| loss: 0.32272 | val_0_auc: 0.88621 |  0:05:34s\n",
      "epoch 135| loss: 0.32095 | val_0_auc: 0.88712 |  0:05:37s\n",
      "epoch 136| loss: 0.321   | val_0_auc: 0.88794 |  0:05:39s\n",
      "epoch 137| loss: 0.31977 | val_0_auc: 0.8876  |  0:05:42s\n",
      "epoch 138| loss: 0.32149 | val_0_auc: 0.88718 |  0:05:44s\n",
      "epoch 139| loss: 0.31647 | val_0_auc: 0.88881 |  0:05:47s\n",
      "epoch 140| loss: 0.32016 | val_0_auc: 0.88875 |  0:05:49s\n",
      "epoch 141| loss: 0.31769 | val_0_auc: 0.88903 |  0:05:51s\n",
      "epoch 142| loss: 0.31665 | val_0_auc: 0.88942 |  0:05:54s\n",
      "epoch 143| loss: 0.3146  | val_0_auc: 0.88839 |  0:05:56s\n",
      "epoch 144| loss: 0.31472 | val_0_auc: 0.88876 |  0:05:58s\n",
      "epoch 145| loss: 0.31467 | val_0_auc: 0.88975 |  0:06:00s\n",
      "epoch 146| loss: 0.31382 | val_0_auc: 0.89005 |  0:06:03s\n",
      "epoch 147| loss: 0.31278 | val_0_auc: 0.89017 |  0:06:05s\n",
      "epoch 148| loss: 0.31378 | val_0_auc: 0.89015 |  0:06:07s\n",
      "epoch 149| loss: 0.31346 | val_0_auc: 0.88929 |  0:06:10s\n",
      "epoch 150| loss: 0.31384 | val_0_auc: 0.89182 |  0:06:12s\n",
      "epoch 151| loss: 0.3124  | val_0_auc: 0.8919  |  0:06:14s\n",
      "epoch 152| loss: 0.3108  | val_0_auc: 0.89255 |  0:06:17s\n",
      "epoch 153| loss: 0.31192 | val_0_auc: 0.89263 |  0:06:19s\n",
      "epoch 154| loss: 0.31053 | val_0_auc: 0.89224 |  0:06:21s\n",
      "epoch 155| loss: 0.31327 | val_0_auc: 0.8925  |  0:06:24s\n",
      "epoch 156| loss: 0.31093 | val_0_auc: 0.89267 |  0:06:26s\n",
      "epoch 157| loss: 0.30841 | val_0_auc: 0.89248 |  0:06:29s\n",
      "epoch 158| loss: 0.30843 | val_0_auc: 0.89357 |  0:06:31s\n",
      "epoch 159| loss: 0.30966 | val_0_auc: 0.893   |  0:06:33s\n",
      "epoch 160| loss: 0.30645 | val_0_auc: 0.8944  |  0:06:36s\n",
      "epoch 161| loss: 0.30769 | val_0_auc: 0.89379 |  0:06:38s\n",
      "epoch 162| loss: 0.30626 | val_0_auc: 0.89253 |  0:06:40s\n",
      "epoch 163| loss: 0.30698 | val_0_auc: 0.89339 |  0:06:43s\n",
      "epoch 164| loss: 0.30592 | val_0_auc: 0.89409 |  0:06:45s\n",
      "epoch 165| loss: 0.30486 | val_0_auc: 0.89491 |  0:06:47s\n",
      "epoch 166| loss: 0.30596 | val_0_auc: 0.89441 |  0:06:49s\n",
      "epoch 167| loss: 0.30552 | val_0_auc: 0.89469 |  0:06:52s\n",
      "epoch 168| loss: 0.30636 | val_0_auc: 0.89504 |  0:06:54s\n",
      "epoch 169| loss: 0.30448 | val_0_auc: 0.89515 |  0:06:56s\n",
      "epoch 170| loss: 0.30267 | val_0_auc: 0.89567 |  0:06:59s\n",
      "epoch 171| loss: 0.30346 | val_0_auc: 0.89632 |  0:07:01s\n",
      "epoch 172| loss: 0.30242 | val_0_auc: 0.89667 |  0:07:04s\n",
      "epoch 173| loss: 0.30318 | val_0_auc: 0.89823 |  0:07:06s\n",
      "epoch 174| loss: 0.29987 | val_0_auc: 0.89686 |  0:07:08s\n",
      "epoch 175| loss: 0.30363 | val_0_auc: 0.8972  |  0:07:11s\n",
      "epoch 176| loss: 0.30223 | val_0_auc: 0.89733 |  0:07:13s\n",
      "epoch 177| loss: 0.30142 | val_0_auc: 0.89783 |  0:07:15s\n",
      "epoch 178| loss: 0.29961 | val_0_auc: 0.89746 |  0:07:18s\n",
      "epoch 179| loss: 0.30103 | val_0_auc: 0.89771 |  0:07:20s\n",
      "epoch 180| loss: 0.3015  | val_0_auc: 0.89829 |  0:07:22s\n",
      "epoch 181| loss: 0.30024 | val_0_auc: 0.89803 |  0:07:24s\n",
      "epoch 182| loss: 0.29957 | val_0_auc: 0.89861 |  0:07:27s\n",
      "epoch 183| loss: 0.29785 | val_0_auc: 0.89889 |  0:07:29s\n",
      "epoch 184| loss: 0.29803 | val_0_auc: 0.89912 |  0:07:31s\n",
      "epoch 185| loss: 0.2974  | val_0_auc: 0.89977 |  0:07:34s\n",
      "epoch 186| loss: 0.29925 | val_0_auc: 0.89985 |  0:07:36s\n",
      "epoch 187| loss: 0.29574 | val_0_auc: 0.90012 |  0:07:38s\n",
      "epoch 188| loss: 0.29513 | val_0_auc: 0.89978 |  0:07:41s\n",
      "epoch 189| loss: 0.29727 | val_0_auc: 0.90014 |  0:07:43s\n",
      "epoch 190| loss: 0.29582 | val_0_auc: 0.89978 |  0:07:45s\n",
      "epoch 191| loss: 0.2957  | val_0_auc: 0.90102 |  0:07:48s\n",
      "epoch 192| loss: 0.29532 | val_0_auc: 0.89951 |  0:07:50s\n",
      "epoch 193| loss: 0.29379 | val_0_auc: 0.89941 |  0:07:52s\n",
      "epoch 194| loss: 0.29515 | val_0_auc: 0.89974 |  0:07:55s\n",
      "epoch 195| loss: 0.29573 | val_0_auc: 0.90073 |  0:07:57s\n",
      "epoch 196| loss: 0.29567 | val_0_auc: 0.9013  |  0:08:00s\n",
      "epoch 197| loss: 0.29459 | val_0_auc: 0.90113 |  0:08:02s\n",
      "epoch 198| loss: 0.29498 | val_0_auc: 0.90169 |  0:08:04s\n",
      "epoch 199| loss: 0.29212 | val_0_auc: 0.9012  |  0:08:07s\n",
      "epoch 200| loss: 0.29203 | val_0_auc: 0.90128 |  0:08:09s\n",
      "epoch 201| loss: 0.29324 | val_0_auc: 0.90103 |  0:08:11s\n",
      "epoch 202| loss: 0.29019 | val_0_auc: 0.90075 |  0:08:14s\n",
      "epoch 203| loss: 0.29208 | val_0_auc: 0.90168 |  0:08:16s\n",
      "epoch 204| loss: 0.29403 | val_0_auc: 0.90268 |  0:08:18s\n",
      "epoch 205| loss: 0.29277 | val_0_auc: 0.90276 |  0:08:21s\n",
      "epoch 206| loss: 0.28977 | val_0_auc: 0.90305 |  0:08:23s\n",
      "epoch 207| loss: 0.29213 | val_0_auc: 0.90311 |  0:08:25s\n",
      "epoch 208| loss: 0.28888 | val_0_auc: 0.90378 |  0:08:28s\n",
      "epoch 209| loss: 0.28816 | val_0_auc: 0.90318 |  0:08:30s\n",
      "epoch 210| loss: 0.29001 | val_0_auc: 0.90371 |  0:08:33s\n",
      "epoch 211| loss: 0.29084 | val_0_auc: 0.90483 |  0:08:35s\n",
      "epoch 212| loss: 0.28861 | val_0_auc: 0.90545 |  0:08:37s\n",
      "epoch 213| loss: 0.28918 | val_0_auc: 0.90483 |  0:08:40s\n",
      "epoch 214| loss: 0.28992 | val_0_auc: 0.90467 |  0:08:42s\n",
      "epoch 215| loss: 0.28647 | val_0_auc: 0.90552 |  0:08:44s\n",
      "epoch 216| loss: 0.29074 | val_0_auc: 0.90544 |  0:08:47s\n",
      "epoch 217| loss: 0.28881 | val_0_auc: 0.90517 |  0:08:49s\n",
      "epoch 218| loss: 0.28497 | val_0_auc: 0.90504 |  0:08:51s\n",
      "epoch 219| loss: 0.28712 | val_0_auc: 0.90493 |  0:08:53s\n",
      "epoch 220| loss: 0.28698 | val_0_auc: 0.90484 |  0:08:56s\n",
      "epoch 221| loss: 0.28603 | val_0_auc: 0.90395 |  0:08:58s\n",
      "epoch 222| loss: 0.28807 | val_0_auc: 0.90456 |  0:09:01s\n",
      "epoch 223| loss: 0.2872  | val_0_auc: 0.90421 |  0:09:03s\n",
      "epoch 224| loss: 0.28451 | val_0_auc: 0.90504 |  0:09:05s\n",
      "epoch 225| loss: 0.2856  | val_0_auc: 0.90441 |  0:09:08s\n",
      "epoch 226| loss: 0.28919 | val_0_auc: 0.9046  |  0:09:10s\n",
      "epoch 227| loss: 0.28613 | val_0_auc: 0.90442 |  0:09:12s\n",
      "epoch 228| loss: 0.28558 | val_0_auc: 0.90393 |  0:09:15s\n",
      "epoch 229| loss: 0.28627 | val_0_auc: 0.90394 |  0:09:17s\n",
      "epoch 230| loss: 0.28368 | val_0_auc: 0.90387 |  0:09:19s\n",
      "epoch 231| loss: 0.28556 | val_0_auc: 0.90327 |  0:09:22s\n",
      "epoch 232| loss: 0.28555 | val_0_auc: 0.90314 |  0:09:24s\n",
      "epoch 233| loss: 0.28504 | val_0_auc: 0.90364 |  0:09:26s\n",
      "epoch 234| loss: 0.28659 | val_0_auc: 0.90386 |  0:09:29s\n",
      "epoch 235| loss: 0.28301 | val_0_auc: 0.90388 |  0:09:31s\n",
      "\n",
      "Early stopping occurred at epoch 235 with best_epoch = 215 and best_val_0_auc = 0.90552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.13855 | val_0_auc: 0.50946 |  0:00:05s\n",
      "epoch 1  | loss: 1.02632 | val_0_auc: 0.52263 |  0:00:10s\n",
      "epoch 2  | loss: 0.92013 | val_0_auc: 0.5351  |  0:00:15s\n",
      "epoch 3  | loss: 0.82606 | val_0_auc: 0.5573  |  0:00:20s\n",
      "epoch 4  | loss: 0.76209 | val_0_auc: 0.56167 |  0:00:25s\n",
      "epoch 5  | loss: 0.72822 | val_0_auc: 0.58539 |  0:00:30s\n",
      "epoch 6  | loss: 0.67748 | val_0_auc: 0.59619 |  0:00:35s\n",
      "epoch 7  | loss: 0.64293 | val_0_auc: 0.61701 |  0:00:40s\n",
      "epoch 8  | loss: 0.62332 | val_0_auc: 0.64155 |  0:00:45s\n",
      "epoch 9  | loss: 0.59182 | val_0_auc: 0.65341 |  0:00:50s\n",
      "epoch 10 | loss: 0.57533 | val_0_auc: 0.67309 |  0:00:54s\n",
      "epoch 11 | loss: 0.54747 | val_0_auc: 0.69793 |  0:00:59s\n",
      "epoch 12 | loss: 0.53574 | val_0_auc: 0.70349 |  0:01:04s\n",
      "epoch 13 | loss: 0.51625 | val_0_auc: 0.72643 |  0:01:09s\n",
      "epoch 14 | loss: 0.50852 | val_0_auc: 0.73677 |  0:01:14s\n",
      "epoch 15 | loss: 0.50385 | val_0_auc: 0.75012 |  0:01:19s\n",
      "epoch 16 | loss: 0.49319 | val_0_auc: 0.76133 |  0:01:23s\n",
      "epoch 17 | loss: 0.48622 | val_0_auc: 0.75629 |  0:01:28s\n",
      "epoch 18 | loss: 0.47584 | val_0_auc: 0.7695  |  0:01:33s\n",
      "epoch 19 | loss: 0.47973 | val_0_auc: 0.77948 |  0:01:38s\n",
      "epoch 20 | loss: 0.47552 | val_0_auc: 0.77871 |  0:01:43s\n",
      "epoch 21 | loss: 0.46419 | val_0_auc: 0.78407 |  0:01:48s\n",
      "epoch 22 | loss: 0.46049 | val_0_auc: 0.78625 |  0:01:53s\n",
      "epoch 23 | loss: 0.45874 | val_0_auc: 0.79256 |  0:01:57s\n",
      "epoch 24 | loss: 0.45006 | val_0_auc: 0.79088 |  0:02:02s\n",
      "epoch 25 | loss: 0.44532 | val_0_auc: 0.79451 |  0:02:07s\n",
      "epoch 26 | loss: 0.4505  | val_0_auc: 0.79804 |  0:02:13s\n",
      "epoch 27 | loss: 0.44267 | val_0_auc: 0.79547 |  0:02:18s\n",
      "epoch 28 | loss: 0.43429 | val_0_auc: 0.80053 |  0:02:23s\n",
      "epoch 29 | loss: 0.43613 | val_0_auc: 0.80657 |  0:02:28s\n",
      "epoch 30 | loss: 0.42901 | val_0_auc: 0.80096 |  0:02:33s\n",
      "epoch 31 | loss: 0.42976 | val_0_auc: 0.81448 |  0:02:38s\n",
      "epoch 32 | loss: 0.42374 | val_0_auc: 0.81229 |  0:02:43s\n",
      "epoch 33 | loss: 0.42162 | val_0_auc: 0.81267 |  0:02:47s\n",
      "epoch 34 | loss: 0.41622 | val_0_auc: 0.81127 |  0:02:52s\n",
      "epoch 35 | loss: 0.41826 | val_0_auc: 0.81688 |  0:02:57s\n",
      "epoch 36 | loss: 0.41299 | val_0_auc: 0.82198 |  0:03:02s\n",
      "epoch 37 | loss: 0.4145  | val_0_auc: 0.82333 |  0:03:07s\n",
      "epoch 38 | loss: 0.40812 | val_0_auc: 0.82243 |  0:03:12s\n",
      "epoch 39 | loss: 0.41402 | val_0_auc: 0.82166 |  0:03:16s\n",
      "epoch 40 | loss: 0.40594 | val_0_auc: 0.82559 |  0:03:21s\n",
      "epoch 41 | loss: 0.4089  | val_0_auc: 0.82088 |  0:03:26s\n",
      "epoch 42 | loss: 0.40619 | val_0_auc: 0.82664 |  0:03:31s\n",
      "epoch 43 | loss: 0.4095  | val_0_auc: 0.82571 |  0:03:36s\n",
      "epoch 44 | loss: 0.4067  | val_0_auc: 0.82864 |  0:03:41s\n",
      "epoch 45 | loss: 0.40188 | val_0_auc: 0.82699 |  0:03:46s\n",
      "epoch 46 | loss: 0.40417 | val_0_auc: 0.83056 |  0:03:51s\n",
      "epoch 47 | loss: 0.40228 | val_0_auc: 0.82439 |  0:03:56s\n",
      "epoch 48 | loss: 0.40189 | val_0_auc: 0.83574 |  0:04:00s\n",
      "epoch 49 | loss: 0.39981 | val_0_auc: 0.82873 |  0:04:05s\n",
      "epoch 50 | loss: 0.39958 | val_0_auc: 0.83431 |  0:04:10s\n",
      "epoch 51 | loss: 0.39924 | val_0_auc: 0.83415 |  0:04:15s\n",
      "epoch 52 | loss: 0.39821 | val_0_auc: 0.83458 |  0:04:20s\n",
      "epoch 53 | loss: 0.39714 | val_0_auc: 0.832   |  0:04:25s\n",
      "epoch 54 | loss: 0.4004  | val_0_auc: 0.83376 |  0:04:30s\n",
      "epoch 55 | loss: 0.39823 | val_0_auc: 0.83777 |  0:04:35s\n",
      "epoch 56 | loss: 0.39238 | val_0_auc: 0.83877 |  0:04:39s\n",
      "epoch 57 | loss: 0.39646 | val_0_auc: 0.83775 |  0:04:44s\n",
      "epoch 58 | loss: 0.3943  | val_0_auc: 0.83494 |  0:04:49s\n",
      "epoch 59 | loss: 0.39016 | val_0_auc: 0.84    |  0:04:54s\n",
      "epoch 60 | loss: 0.38853 | val_0_auc: 0.83737 |  0:04:59s\n",
      "epoch 61 | loss: 0.39033 | val_0_auc: 0.83912 |  0:05:03s\n",
      "epoch 62 | loss: 0.38972 | val_0_auc: 0.83557 |  0:05:08s\n",
      "epoch 63 | loss: 0.39506 | val_0_auc: 0.84188 |  0:05:13s\n",
      "epoch 64 | loss: 0.39013 | val_0_auc: 0.83598 |  0:05:18s\n",
      "epoch 65 | loss: 0.3903  | val_0_auc: 0.83778 |  0:05:23s\n",
      "epoch 66 | loss: 0.38996 | val_0_auc: 0.83555 |  0:05:28s\n",
      "epoch 67 | loss: 0.38996 | val_0_auc: 0.84166 |  0:05:32s\n",
      "epoch 68 | loss: 0.38771 | val_0_auc: 0.83986 |  0:05:37s\n",
      "epoch 69 | loss: 0.38588 | val_0_auc: 0.83545 |  0:05:42s\n",
      "epoch 70 | loss: 0.38933 | val_0_auc: 0.84159 |  0:05:47s\n",
      "epoch 71 | loss: 0.3878  | val_0_auc: 0.84043 |  0:05:52s\n",
      "epoch 72 | loss: 0.3837  | val_0_auc: 0.84095 |  0:05:57s\n",
      "epoch 73 | loss: 0.38515 | val_0_auc: 0.84032 |  0:06:01s\n",
      "epoch 74 | loss: 0.38681 | val_0_auc: 0.83618 |  0:06:06s\n",
      "epoch 75 | loss: 0.38955 | val_0_auc: 0.84268 |  0:06:11s\n",
      "epoch 76 | loss: 0.38631 | val_0_auc: 0.84068 |  0:06:16s\n",
      "epoch 77 | loss: 0.38691 | val_0_auc: 0.8403  |  0:06:21s\n",
      "epoch 78 | loss: 0.38407 | val_0_auc: 0.84266 |  0:06:26s\n",
      "epoch 79 | loss: 0.38375 | val_0_auc: 0.84111 |  0:06:31s\n",
      "epoch 80 | loss: 0.38442 | val_0_auc: 0.84197 |  0:06:36s\n",
      "epoch 81 | loss: 0.38679 | val_0_auc: 0.83991 |  0:06:41s\n",
      "epoch 82 | loss: 0.38171 | val_0_auc: 0.84349 |  0:06:46s\n",
      "epoch 83 | loss: 0.38095 | val_0_auc: 0.84366 |  0:06:51s\n",
      "epoch 84 | loss: 0.38185 | val_0_auc: 0.84742 |  0:06:55s\n",
      "epoch 85 | loss: 0.38205 | val_0_auc: 0.84034 |  0:07:00s\n",
      "epoch 86 | loss: 0.38219 | val_0_auc: 0.84419 |  0:07:05s\n",
      "epoch 87 | loss: 0.38344 | val_0_auc: 0.83908 |  0:07:10s\n",
      "epoch 88 | loss: 0.38092 | val_0_auc: 0.84111 |  0:07:15s\n",
      "epoch 89 | loss: 0.38009 | val_0_auc: 0.84052 |  0:07:20s\n",
      "epoch 90 | loss: 0.37914 | val_0_auc: 0.84317 |  0:07:25s\n",
      "epoch 91 | loss: 0.38167 | val_0_auc: 0.84326 |  0:07:29s\n",
      "epoch 92 | loss: 0.3794  | val_0_auc: 0.8423  |  0:07:34s\n",
      "epoch 93 | loss: 0.38034 | val_0_auc: 0.84601 |  0:07:39s\n",
      "epoch 94 | loss: 0.38022 | val_0_auc: 0.84465 |  0:07:44s\n",
      "epoch 95 | loss: 0.37913 | val_0_auc: 0.8406  |  0:07:49s\n",
      "epoch 96 | loss: 0.38079 | val_0_auc: 0.84048 |  0:07:54s\n",
      "epoch 97 | loss: 0.38244 | val_0_auc: 0.84132 |  0:07:59s\n",
      "epoch 98 | loss: 0.37975 | val_0_auc: 0.8408  |  0:08:04s\n",
      "epoch 99 | loss: 0.38007 | val_0_auc: 0.83794 |  0:08:09s\n",
      "epoch 100| loss: 0.37821 | val_0_auc: 0.84171 |  0:08:14s\n",
      "epoch 101| loss: 0.38083 | val_0_auc: 0.84368 |  0:08:18s\n",
      "epoch 102| loss: 0.37729 | val_0_auc: 0.84583 |  0:08:23s\n",
      "epoch 103| loss: 0.37887 | val_0_auc: 0.84383 |  0:08:28s\n",
      "epoch 104| loss: 0.37698 | val_0_auc: 0.8422  |  0:08:33s\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 84 and best_val_0_auc = 0.84742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.83165 | val_0_auc: 0.46143 |  0:00:01s\n",
      "epoch 1  | loss: 0.76242 | val_0_auc: 0.47267 |  0:00:03s\n",
      "epoch 2  | loss: 0.68859 | val_0_auc: 0.50475 |  0:00:05s\n",
      "epoch 3  | loss: 0.64185 | val_0_auc: 0.53511 |  0:00:07s\n",
      "epoch 4  | loss: 0.60908 | val_0_auc: 0.58294 |  0:00:08s\n",
      "epoch 5  | loss: 0.57707 | val_0_auc: 0.61823 |  0:00:10s\n",
      "epoch 6  | loss: 0.56482 | val_0_auc: 0.65414 |  0:00:12s\n",
      "epoch 7  | loss: 0.546   | val_0_auc: 0.66935 |  0:00:14s\n",
      "epoch 8  | loss: 0.53774 | val_0_auc: 0.69448 |  0:00:16s\n",
      "epoch 9  | loss: 0.51629 | val_0_auc: 0.70802 |  0:00:18s\n",
      "epoch 10 | loss: 0.50874 | val_0_auc: 0.72057 |  0:00:19s\n",
      "epoch 11 | loss: 0.50369 | val_0_auc: 0.7402  |  0:00:21s\n",
      "epoch 12 | loss: 0.49415 | val_0_auc: 0.74874 |  0:00:23s\n",
      "epoch 13 | loss: 0.48831 | val_0_auc: 0.75981 |  0:00:25s\n",
      "epoch 14 | loss: 0.48592 | val_0_auc: 0.76645 |  0:00:27s\n",
      "epoch 15 | loss: 0.4809  | val_0_auc: 0.77495 |  0:00:28s\n",
      "epoch 16 | loss: 0.47669 | val_0_auc: 0.78193 |  0:00:30s\n",
      "epoch 17 | loss: 0.46987 | val_0_auc: 0.78578 |  0:00:32s\n",
      "epoch 18 | loss: 0.46876 | val_0_auc: 0.79299 |  0:00:34s\n",
      "epoch 19 | loss: 0.46278 | val_0_auc: 0.79474 |  0:00:36s\n",
      "epoch 20 | loss: 0.45901 | val_0_auc: 0.7996  |  0:00:37s\n",
      "epoch 21 | loss: 0.45984 | val_0_auc: 0.79884 |  0:00:39s\n",
      "epoch 22 | loss: 0.45789 | val_0_auc: 0.79956 |  0:00:41s\n",
      "epoch 23 | loss: 0.44995 | val_0_auc: 0.80357 |  0:00:43s\n",
      "epoch 24 | loss: 0.44683 | val_0_auc: 0.80388 |  0:00:44s\n",
      "epoch 25 | loss: 0.44935 | val_0_auc: 0.80068 |  0:00:46s\n",
      "epoch 26 | loss: 0.44408 | val_0_auc: 0.80449 |  0:00:48s\n",
      "epoch 27 | loss: 0.44516 | val_0_auc: 0.80085 |  0:00:50s\n",
      "epoch 28 | loss: 0.4393  | val_0_auc: 0.80612 |  0:00:51s\n",
      "epoch 29 | loss: 0.44072 | val_0_auc: 0.80758 |  0:00:53s\n",
      "epoch 30 | loss: 0.43685 | val_0_auc: 0.81127 |  0:00:55s\n",
      "epoch 31 | loss: 0.43377 | val_0_auc: 0.81024 |  0:00:57s\n",
      "epoch 32 | loss: 0.43561 | val_0_auc: 0.80998 |  0:00:59s\n",
      "epoch 33 | loss: 0.43827 | val_0_auc: 0.82032 |  0:01:00s\n",
      "epoch 34 | loss: 0.43276 | val_0_auc: 0.81958 |  0:01:02s\n",
      "epoch 35 | loss: 0.43066 | val_0_auc: 0.81976 |  0:01:04s\n",
      "epoch 36 | loss: 0.42913 | val_0_auc: 0.81713 |  0:01:06s\n",
      "epoch 37 | loss: 0.42783 | val_0_auc: 0.82073 |  0:01:08s\n",
      "epoch 38 | loss: 0.42369 | val_0_auc: 0.82077 |  0:01:09s\n",
      "epoch 39 | loss: 0.42523 | val_0_auc: 0.82408 |  0:01:11s\n",
      "epoch 40 | loss: 0.42615 | val_0_auc: 0.82536 |  0:01:13s\n",
      "epoch 41 | loss: 0.42431 | val_0_auc: 0.82668 |  0:01:15s\n",
      "epoch 42 | loss: 0.42179 | val_0_auc: 0.82686 |  0:01:17s\n",
      "epoch 43 | loss: 0.42098 | val_0_auc: 0.82734 |  0:01:18s\n",
      "epoch 44 | loss: 0.42202 | val_0_auc: 0.82955 |  0:01:20s\n",
      "epoch 45 | loss: 0.42042 | val_0_auc: 0.82899 |  0:01:22s\n",
      "epoch 46 | loss: 0.42037 | val_0_auc: 0.83161 |  0:01:24s\n",
      "epoch 47 | loss: 0.41886 | val_0_auc: 0.83386 |  0:01:25s\n",
      "epoch 48 | loss: 0.41734 | val_0_auc: 0.8355  |  0:01:27s\n",
      "epoch 49 | loss: 0.41491 | val_0_auc: 0.83432 |  0:01:29s\n",
      "epoch 50 | loss: 0.41383 | val_0_auc: 0.83523 |  0:01:31s\n",
      "epoch 51 | loss: 0.41345 | val_0_auc: 0.83674 |  0:01:33s\n",
      "epoch 52 | loss: 0.4155  | val_0_auc: 0.83528 |  0:01:34s\n",
      "epoch 53 | loss: 0.41039 | val_0_auc: 0.83818 |  0:01:36s\n",
      "epoch 54 | loss: 0.41417 | val_0_auc: 0.83376 |  0:01:38s\n",
      "epoch 55 | loss: 0.41232 | val_0_auc: 0.83899 |  0:01:40s\n",
      "epoch 56 | loss: 0.41442 | val_0_auc: 0.83933 |  0:01:41s\n",
      "epoch 57 | loss: 0.4116  | val_0_auc: 0.83828 |  0:01:43s\n",
      "epoch 58 | loss: 0.40971 | val_0_auc: 0.83994 |  0:01:45s\n",
      "epoch 59 | loss: 0.40926 | val_0_auc: 0.83949 |  0:01:47s\n",
      "epoch 60 | loss: 0.40894 | val_0_auc: 0.83916 |  0:01:49s\n",
      "epoch 61 | loss: 0.40657 | val_0_auc: 0.83846 |  0:01:50s\n",
      "epoch 62 | loss: 0.40699 | val_0_auc: 0.83816 |  0:01:52s\n",
      "epoch 63 | loss: 0.40875 | val_0_auc: 0.84057 |  0:01:54s\n",
      "epoch 64 | loss: 0.40553 | val_0_auc: 0.8407  |  0:01:56s\n",
      "epoch 65 | loss: 0.4094  | val_0_auc: 0.8412  |  0:01:57s\n",
      "epoch 66 | loss: 0.40368 | val_0_auc: 0.83887 |  0:01:59s\n",
      "epoch 67 | loss: 0.40272 | val_0_auc: 0.84388 |  0:02:01s\n",
      "epoch 68 | loss: 0.40514 | val_0_auc: 0.83964 |  0:02:03s\n",
      "epoch 69 | loss: 0.40232 | val_0_auc: 0.84163 |  0:02:05s\n",
      "epoch 70 | loss: 0.40124 | val_0_auc: 0.84335 |  0:02:06s\n",
      "epoch 71 | loss: 0.39988 | val_0_auc: 0.84101 |  0:02:08s\n",
      "epoch 72 | loss: 0.4026  | val_0_auc: 0.8424  |  0:02:10s\n",
      "epoch 73 | loss: 0.40179 | val_0_auc: 0.84416 |  0:02:12s\n",
      "epoch 74 | loss: 0.40015 | val_0_auc: 0.84271 |  0:02:14s\n",
      "epoch 75 | loss: 0.39936 | val_0_auc: 0.84516 |  0:02:15s\n",
      "epoch 76 | loss: 0.40021 | val_0_auc: 0.84737 |  0:02:17s\n",
      "epoch 77 | loss: 0.40163 | val_0_auc: 0.84513 |  0:02:19s\n",
      "epoch 78 | loss: 0.40284 | val_0_auc: 0.84607 |  0:02:21s\n",
      "epoch 79 | loss: 0.40145 | val_0_auc: 0.84725 |  0:02:22s\n",
      "epoch 80 | loss: 0.39838 | val_0_auc: 0.84652 |  0:02:24s\n",
      "epoch 81 | loss: 0.3992  | val_0_auc: 0.84683 |  0:02:26s\n",
      "epoch 82 | loss: 0.39978 | val_0_auc: 0.84633 |  0:02:28s\n",
      "epoch 83 | loss: 0.3992  | val_0_auc: 0.84347 |  0:02:30s\n",
      "epoch 84 | loss: 0.39852 | val_0_auc: 0.84386 |  0:02:31s\n",
      "epoch 85 | loss: 0.39529 | val_0_auc: 0.84795 |  0:02:33s\n",
      "epoch 86 | loss: 0.39679 | val_0_auc: 0.84446 |  0:02:35s\n",
      "epoch 87 | loss: 0.39645 | val_0_auc: 0.84675 |  0:02:37s\n",
      "epoch 88 | loss: 0.39715 | val_0_auc: 0.84444 |  0:02:38s\n",
      "epoch 89 | loss: 0.39605 | val_0_auc: 0.8446  |  0:02:40s\n",
      "epoch 90 | loss: 0.39478 | val_0_auc: 0.8466  |  0:02:42s\n",
      "epoch 91 | loss: 0.39588 | val_0_auc: 0.84599 |  0:02:44s\n",
      "epoch 92 | loss: 0.39303 | val_0_auc: 0.84483 |  0:02:45s\n",
      "epoch 93 | loss: 0.39428 | val_0_auc: 0.84612 |  0:02:47s\n",
      "epoch 94 | loss: 0.39491 | val_0_auc: 0.84457 |  0:02:49s\n",
      "epoch 95 | loss: 0.39464 | val_0_auc: 0.84588 |  0:02:51s\n",
      "epoch 96 | loss: 0.39422 | val_0_auc: 0.84743 |  0:02:53s\n",
      "epoch 97 | loss: 0.39339 | val_0_auc: 0.84572 |  0:02:54s\n",
      "epoch 98 | loss: 0.39242 | val_0_auc: 0.84803 |  0:02:56s\n",
      "epoch 99 | loss: 0.39336 | val_0_auc: 0.84713 |  0:02:58s\n",
      "epoch 100| loss: 0.39039 | val_0_auc: 0.84863 |  0:03:00s\n",
      "epoch 101| loss: 0.39277 | val_0_auc: 0.84823 |  0:03:02s\n",
      "epoch 102| loss: 0.39369 | val_0_auc: 0.84756 |  0:03:03s\n",
      "epoch 103| loss: 0.39091 | val_0_auc: 0.84792 |  0:03:05s\n",
      "epoch 104| loss: 0.39185 | val_0_auc: 0.85018 |  0:03:07s\n",
      "epoch 105| loss: 0.39006 | val_0_auc: 0.85074 |  0:03:09s\n",
      "epoch 106| loss: 0.38879 | val_0_auc: 0.85181 |  0:03:10s\n",
      "epoch 107| loss: 0.38891 | val_0_auc: 0.85183 |  0:03:12s\n",
      "epoch 108| loss: 0.39131 | val_0_auc: 0.84969 |  0:03:14s\n",
      "epoch 109| loss: 0.386   | val_0_auc: 0.84797 |  0:03:16s\n",
      "epoch 110| loss: 0.38589 | val_0_auc: 0.85477 |  0:03:18s\n",
      "epoch 111| loss: 0.38782 | val_0_auc: 0.84898 |  0:03:19s\n",
      "epoch 112| loss: 0.38749 | val_0_auc: 0.84842 |  0:03:21s\n",
      "epoch 113| loss: 0.38841 | val_0_auc: 0.84769 |  0:03:23s\n",
      "epoch 114| loss: 0.38769 | val_0_auc: 0.84663 |  0:03:25s\n",
      "epoch 115| loss: 0.38565 | val_0_auc: 0.84762 |  0:03:27s\n",
      "epoch 116| loss: 0.38608 | val_0_auc: 0.8478  |  0:03:28s\n",
      "epoch 117| loss: 0.38582 | val_0_auc: 0.84954 |  0:03:30s\n",
      "epoch 118| loss: 0.38441 | val_0_auc: 0.84814 |  0:03:33s\n",
      "epoch 119| loss: 0.38289 | val_0_auc: 0.84667 |  0:03:35s\n",
      "epoch 120| loss: 0.38403 | val_0_auc: 0.84921 |  0:03:36s\n",
      "epoch 121| loss: 0.3862  | val_0_auc: 0.84925 |  0:03:38s\n",
      "epoch 122| loss: 0.38373 | val_0_auc: 0.84631 |  0:03:40s\n",
      "epoch 123| loss: 0.38332 | val_0_auc: 0.84639 |  0:03:42s\n",
      "epoch 124| loss: 0.38215 | val_0_auc: 0.84918 |  0:03:44s\n",
      "epoch 125| loss: 0.38178 | val_0_auc: 0.8473  |  0:03:45s\n",
      "epoch 126| loss: 0.38349 | val_0_auc: 0.84861 |  0:03:47s\n",
      "epoch 127| loss: 0.3837  | val_0_auc: 0.84784 |  0:03:49s\n",
      "epoch 128| loss: 0.38286 | val_0_auc: 0.85012 |  0:03:51s\n",
      "epoch 129| loss: 0.38025 | val_0_auc: 0.85013 |  0:03:52s\n",
      "epoch 130| loss: 0.38017 | val_0_auc: 0.8501  |  0:03:55s\n",
      "\n",
      "Early stopping occurred at epoch 130 with best_epoch = 110 and best_val_0_auc = 0.85477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.80547 | val_0_auc: 0.55453 |  0:00:04s\n",
      "epoch 1  | loss: 0.62827 | val_0_auc: 0.51066 |  0:00:08s\n",
      "epoch 2  | loss: 0.58117 | val_0_auc: 0.51855 |  0:00:12s\n",
      "epoch 3  | loss: 0.55844 | val_0_auc: 0.63525 |  0:00:16s\n",
      "epoch 4  | loss: 0.54373 | val_0_auc: 0.66865 |  0:00:20s\n",
      "epoch 5  | loss: 0.52924 | val_0_auc: 0.68763 |  0:00:24s\n",
      "epoch 6  | loss: 0.50652 | val_0_auc: 0.72245 |  0:00:28s\n",
      "epoch 7  | loss: 0.4873  | val_0_auc: 0.73542 |  0:00:32s\n",
      "epoch 8  | loss: 0.48391 | val_0_auc: 0.75442 |  0:00:36s\n",
      "epoch 9  | loss: 0.46296 | val_0_auc: 0.78432 |  0:00:40s\n",
      "epoch 10 | loss: 0.44717 | val_0_auc: 0.78834 |  0:00:44s\n",
      "epoch 11 | loss: 0.45017 | val_0_auc: 0.78357 |  0:00:48s\n",
      "epoch 12 | loss: 0.43225 | val_0_auc: 0.80509 |  0:00:52s\n",
      "epoch 13 | loss: 0.42027 | val_0_auc: 0.82044 |  0:00:56s\n",
      "epoch 14 | loss: 0.40271 | val_0_auc: 0.84311 |  0:01:00s\n",
      "epoch 15 | loss: 0.38189 | val_0_auc: 0.85619 |  0:01:04s\n",
      "epoch 16 | loss: 0.37608 | val_0_auc: 0.86742 |  0:01:08s\n",
      "epoch 17 | loss: 0.3643  | val_0_auc: 0.87266 |  0:01:12s\n",
      "epoch 18 | loss: 0.35356 | val_0_auc: 0.88135 |  0:01:16s\n",
      "epoch 19 | loss: 0.34317 | val_0_auc: 0.88308 |  0:01:20s\n",
      "epoch 20 | loss: 0.34514 | val_0_auc: 0.85602 |  0:01:24s\n",
      "epoch 21 | loss: 0.34688 | val_0_auc: 0.88302 |  0:01:28s\n",
      "epoch 22 | loss: 0.33557 | val_0_auc: 0.89152 |  0:01:32s\n",
      "epoch 23 | loss: 0.32208 | val_0_auc: 0.89331 |  0:01:36s\n",
      "epoch 24 | loss: 0.31861 | val_0_auc: 0.90062 |  0:01:40s\n",
      "epoch 25 | loss: 0.31397 | val_0_auc: 0.90305 |  0:01:44s\n",
      "epoch 26 | loss: 0.30736 | val_0_auc: 0.90489 |  0:01:48s\n",
      "epoch 27 | loss: 0.30612 | val_0_auc: 0.90577 |  0:01:52s\n",
      "epoch 28 | loss: 0.30286 | val_0_auc: 0.90512 |  0:01:56s\n",
      "epoch 29 | loss: 0.30185 | val_0_auc: 0.90916 |  0:02:00s\n",
      "epoch 30 | loss: 0.29892 | val_0_auc: 0.90608 |  0:02:04s\n",
      "epoch 31 | loss: 0.2946  | val_0_auc: 0.9129  |  0:02:08s\n",
      "epoch 32 | loss: 0.2951  | val_0_auc: 0.89621 |  0:02:12s\n",
      "epoch 33 | loss: 0.30539 | val_0_auc: 0.90693 |  0:02:16s\n",
      "epoch 34 | loss: 0.30032 | val_0_auc: 0.91159 |  0:02:20s\n",
      "epoch 35 | loss: 0.29934 | val_0_auc: 0.91416 |  0:02:24s\n",
      "epoch 36 | loss: 0.29735 | val_0_auc: 0.90993 |  0:02:28s\n",
      "epoch 37 | loss: 0.29853 | val_0_auc: 0.90628 |  0:02:32s\n",
      "epoch 38 | loss: 0.30032 | val_0_auc: 0.91329 |  0:02:36s\n",
      "epoch 39 | loss: 0.29362 | val_0_auc: 0.91595 |  0:02:40s\n",
      "epoch 40 | loss: 0.29399 | val_0_auc: 0.915   |  0:02:44s\n",
      "epoch 41 | loss: 0.28881 | val_0_auc: 0.91681 |  0:02:48s\n",
      "epoch 42 | loss: 0.28651 | val_0_auc: 0.91704 |  0:02:52s\n",
      "epoch 43 | loss: 0.28312 | val_0_auc: 0.9171  |  0:02:56s\n",
      "epoch 44 | loss: 0.28588 | val_0_auc: 0.91632 |  0:03:00s\n",
      "epoch 45 | loss: 0.28312 | val_0_auc: 0.91748 |  0:03:04s\n",
      "epoch 46 | loss: 0.28707 | val_0_auc: 0.91717 |  0:03:08s\n",
      "epoch 47 | loss: 0.28139 | val_0_auc: 0.91823 |  0:03:12s\n",
      "epoch 48 | loss: 0.27901 | val_0_auc: 0.91737 |  0:03:16s\n",
      "epoch 49 | loss: 0.27894 | val_0_auc: 0.91867 |  0:03:20s\n",
      "epoch 50 | loss: 0.27542 | val_0_auc: 0.91849 |  0:03:24s\n",
      "epoch 51 | loss: 0.27524 | val_0_auc: 0.91932 |  0:03:28s\n",
      "epoch 52 | loss: 0.27392 | val_0_auc: 0.91948 |  0:03:32s\n",
      "epoch 53 | loss: 0.27309 | val_0_auc: 0.91884 |  0:03:36s\n",
      "epoch 54 | loss: 0.27388 | val_0_auc: 0.91911 |  0:03:40s\n",
      "epoch 55 | loss: 0.27712 | val_0_auc: 0.9195  |  0:03:44s\n",
      "epoch 56 | loss: 0.27478 | val_0_auc: 0.9186  |  0:03:48s\n",
      "epoch 57 | loss: 0.27242 | val_0_auc: 0.91881 |  0:03:52s\n",
      "epoch 58 | loss: 0.2754  | val_0_auc: 0.9157  |  0:03:56s\n",
      "epoch 59 | loss: 0.27229 | val_0_auc: 0.91707 |  0:04:00s\n",
      "epoch 60 | loss: 0.29194 | val_0_auc: 0.90571 |  0:04:04s\n",
      "epoch 61 | loss: 0.2886  | val_0_auc: 0.91431 |  0:04:08s\n",
      "epoch 62 | loss: 0.27839 | val_0_auc: 0.91609 |  0:04:12s\n",
      "epoch 63 | loss: 0.27569 | val_0_auc: 0.91875 |  0:04:16s\n",
      "epoch 64 | loss: 0.2703  | val_0_auc: 0.91977 |  0:04:20s\n",
      "epoch 65 | loss: 0.268   | val_0_auc: 0.91959 |  0:04:24s\n",
      "epoch 66 | loss: 0.26848 | val_0_auc: 0.91762 |  0:04:28s\n",
      "epoch 67 | loss: 0.26933 | val_0_auc: 0.91718 |  0:04:31s\n",
      "epoch 68 | loss: 0.29925 | val_0_auc: 0.90123 |  0:04:35s\n",
      "epoch 69 | loss: 0.30199 | val_0_auc: 0.90278 |  0:04:39s\n",
      "epoch 70 | loss: 0.2952  | val_0_auc: 0.90923 |  0:04:43s\n",
      "epoch 71 | loss: 0.28604 | val_0_auc: 0.91363 |  0:04:47s\n",
      "epoch 72 | loss: 0.28251 | val_0_auc: 0.91623 |  0:04:51s\n",
      "epoch 73 | loss: 0.27846 | val_0_auc: 0.91491 |  0:04:55s\n",
      "epoch 74 | loss: 0.27416 | val_0_auc: 0.91581 |  0:04:59s\n",
      "epoch 75 | loss: 0.27288 | val_0_auc: 0.91669 |  0:05:03s\n",
      "epoch 76 | loss: 0.27202 | val_0_auc: 0.91662 |  0:05:07s\n",
      "epoch 77 | loss: 0.27125 | val_0_auc: 0.91686 |  0:05:11s\n",
      "epoch 78 | loss: 0.27073 | val_0_auc: 0.91908 |  0:05:15s\n",
      "epoch 79 | loss: 0.26683 | val_0_auc: 0.91864 |  0:05:19s\n",
      "epoch 80 | loss: 0.26935 | val_0_auc: 0.91626 |  0:05:23s\n",
      "epoch 81 | loss: 0.26827 | val_0_auc: 0.91776 |  0:05:27s\n",
      "epoch 82 | loss: 0.26456 | val_0_auc: 0.91728 |  0:05:31s\n",
      "epoch 83 | loss: 0.26207 | val_0_auc: 0.9186  |  0:05:35s\n",
      "epoch 84 | loss: 0.2636  | val_0_auc: 0.91832 |  0:05:38s\n",
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 64 and best_val_0_auc = 0.91977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.45496 | val_0_auc: 0.51467 |  0:00:01s\n",
      "epoch 1  | loss: 0.74111 | val_0_auc: 0.52812 |  0:00:02s\n",
      "epoch 2  | loss: 0.59193 | val_0_auc: 0.5841  |  0:00:04s\n",
      "epoch 3  | loss: 0.53752 | val_0_auc: 0.67527 |  0:00:05s\n",
      "epoch 4  | loss: 0.50228 | val_0_auc: 0.71781 |  0:00:07s\n",
      "epoch 5  | loss: 0.48524 | val_0_auc: 0.72834 |  0:00:08s\n",
      "epoch 6  | loss: 0.46351 | val_0_auc: 0.7506  |  0:00:10s\n",
      "epoch 7  | loss: 0.44465 | val_0_auc: 0.75764 |  0:00:11s\n",
      "epoch 8  | loss: 0.43016 | val_0_auc: 0.78084 |  0:00:13s\n",
      "epoch 9  | loss: 0.41975 | val_0_auc: 0.7884  |  0:00:14s\n",
      "epoch 10 | loss: 0.41921 | val_0_auc: 0.80975 |  0:00:15s\n",
      "epoch 11 | loss: 0.41824 | val_0_auc: 0.77856 |  0:00:17s\n",
      "epoch 12 | loss: 0.41677 | val_0_auc: 0.78202 |  0:00:18s\n",
      "epoch 13 | loss: 0.41411 | val_0_auc: 0.79023 |  0:00:20s\n",
      "epoch 14 | loss: 0.41165 | val_0_auc: 0.79627 |  0:00:21s\n",
      "epoch 15 | loss: 0.4069  | val_0_auc: 0.8285  |  0:00:23s\n",
      "epoch 16 | loss: 0.40049 | val_0_auc: 0.83718 |  0:00:24s\n",
      "epoch 17 | loss: 0.39454 | val_0_auc: 0.84555 |  0:00:25s\n",
      "epoch 18 | loss: 0.38937 | val_0_auc: 0.84009 |  0:00:27s\n",
      "epoch 19 | loss: 0.38785 | val_0_auc: 0.84289 |  0:00:28s\n",
      "epoch 20 | loss: 0.38263 | val_0_auc: 0.84082 |  0:00:30s\n",
      "epoch 21 | loss: 0.37991 | val_0_auc: 0.84598 |  0:00:31s\n",
      "epoch 22 | loss: 0.37709 | val_0_auc: 0.84684 |  0:00:33s\n",
      "epoch 23 | loss: 0.37535 | val_0_auc: 0.84193 |  0:00:34s\n",
      "epoch 24 | loss: 0.37687 | val_0_auc: 0.84423 |  0:00:36s\n",
      "epoch 25 | loss: 0.3722  | val_0_auc: 0.8515  |  0:00:37s\n",
      "epoch 26 | loss: 0.37026 | val_0_auc: 0.85313 |  0:00:38s\n",
      "epoch 27 | loss: 0.36648 | val_0_auc: 0.85877 |  0:00:40s\n",
      "epoch 28 | loss: 0.36408 | val_0_auc: 0.85738 |  0:00:41s\n",
      "epoch 29 | loss: 0.36508 | val_0_auc: 0.85943 |  0:00:43s\n",
      "epoch 30 | loss: 0.36178 | val_0_auc: 0.86344 |  0:00:44s\n",
      "epoch 31 | loss: 0.35698 | val_0_auc: 0.86861 |  0:00:45s\n",
      "epoch 32 | loss: 0.35805 | val_0_auc: 0.86926 |  0:00:47s\n",
      "epoch 33 | loss: 0.35514 | val_0_auc: 0.86257 |  0:00:48s\n",
      "epoch 34 | loss: 0.3565  | val_0_auc: 0.86449 |  0:00:50s\n",
      "epoch 35 | loss: 0.35457 | val_0_auc: 0.86778 |  0:00:51s\n",
      "epoch 36 | loss: 0.35177 | val_0_auc: 0.86994 |  0:00:53s\n",
      "epoch 37 | loss: 0.34941 | val_0_auc: 0.86914 |  0:00:54s\n",
      "epoch 38 | loss: 0.35255 | val_0_auc: 0.87041 |  0:00:55s\n",
      "epoch 39 | loss: 0.34904 | val_0_auc: 0.87294 |  0:00:57s\n",
      "epoch 40 | loss: 0.34426 | val_0_auc: 0.87133 |  0:00:58s\n",
      "epoch 41 | loss: 0.34488 | val_0_auc: 0.8743  |  0:01:00s\n",
      "epoch 42 | loss: 0.34245 | val_0_auc: 0.87525 |  0:01:01s\n",
      "epoch 43 | loss: 0.33778 | val_0_auc: 0.87997 |  0:01:02s\n",
      "epoch 44 | loss: 0.33796 | val_0_auc: 0.88146 |  0:01:04s\n",
      "epoch 45 | loss: 0.33338 | val_0_auc: 0.87873 |  0:01:05s\n",
      "epoch 46 | loss: 0.3348  | val_0_auc: 0.87775 |  0:01:07s\n",
      "epoch 47 | loss: 0.33298 | val_0_auc: 0.88048 |  0:01:08s\n",
      "epoch 48 | loss: 0.33155 | val_0_auc: 0.87974 |  0:01:09s\n",
      "epoch 49 | loss: 0.33131 | val_0_auc: 0.88197 |  0:01:11s\n",
      "epoch 50 | loss: 0.32862 | val_0_auc: 0.88307 |  0:01:12s\n",
      "epoch 51 | loss: 0.32699 | val_0_auc: 0.88532 |  0:01:14s\n",
      "epoch 52 | loss: 0.32539 | val_0_auc: 0.88586 |  0:01:15s\n",
      "epoch 53 | loss: 0.32757 | val_0_auc: 0.88337 |  0:01:17s\n",
      "epoch 54 | loss: 0.32334 | val_0_auc: 0.88469 |  0:01:18s\n",
      "epoch 55 | loss: 0.31964 | val_0_auc: 0.88755 |  0:01:20s\n",
      "epoch 56 | loss: 0.32034 | val_0_auc: 0.88625 |  0:01:21s\n",
      "epoch 57 | loss: 0.31995 | val_0_auc: 0.88582 |  0:01:22s\n",
      "epoch 58 | loss: 0.31822 | val_0_auc: 0.88573 |  0:01:24s\n",
      "epoch 59 | loss: 0.31732 | val_0_auc: 0.88813 |  0:01:25s\n",
      "epoch 60 | loss: 0.31599 | val_0_auc: 0.88825 |  0:01:27s\n",
      "epoch 61 | loss: 0.31672 | val_0_auc: 0.89027 |  0:01:28s\n",
      "epoch 62 | loss: 0.31533 | val_0_auc: 0.89011 |  0:01:30s\n",
      "epoch 63 | loss: 0.31252 | val_0_auc: 0.89256 |  0:01:31s\n",
      "epoch 64 | loss: 0.30971 | val_0_auc: 0.8922  |  0:01:32s\n",
      "epoch 65 | loss: 0.30991 | val_0_auc: 0.8931  |  0:01:34s\n",
      "epoch 66 | loss: 0.30905 | val_0_auc: 0.89189 |  0:01:35s\n",
      "epoch 67 | loss: 0.30953 | val_0_auc: 0.89299 |  0:01:37s\n",
      "epoch 68 | loss: 0.30848 | val_0_auc: 0.89362 |  0:01:38s\n",
      "epoch 69 | loss: 0.3045  | val_0_auc: 0.89543 |  0:01:39s\n",
      "epoch 70 | loss: 0.30556 | val_0_auc: 0.89394 |  0:01:41s\n",
      "epoch 71 | loss: 0.30596 | val_0_auc: 0.89566 |  0:01:42s\n",
      "epoch 72 | loss: 0.30446 | val_0_auc: 0.89617 |  0:01:44s\n",
      "epoch 73 | loss: 0.30353 | val_0_auc: 0.89858 |  0:01:45s\n",
      "epoch 74 | loss: 0.30391 | val_0_auc: 0.89705 |  0:01:47s\n",
      "epoch 75 | loss: 0.30279 | val_0_auc: 0.89761 |  0:01:48s\n",
      "epoch 76 | loss: 0.30358 | val_0_auc: 0.89959 |  0:01:50s\n",
      "epoch 77 | loss: 0.30021 | val_0_auc: 0.89916 |  0:01:51s\n",
      "epoch 78 | loss: 0.29983 | val_0_auc: 0.89945 |  0:01:52s\n",
      "epoch 79 | loss: 0.29925 | val_0_auc: 0.89929 |  0:01:54s\n",
      "epoch 80 | loss: 0.29577 | val_0_auc: 0.90054 |  0:01:55s\n",
      "epoch 81 | loss: 0.29636 | val_0_auc: 0.89997 |  0:01:57s\n",
      "epoch 82 | loss: 0.29967 | val_0_auc: 0.90016 |  0:01:58s\n",
      "epoch 83 | loss: 0.29849 | val_0_auc: 0.90133 |  0:02:00s\n",
      "epoch 84 | loss: 0.2952  | val_0_auc: 0.90324 |  0:02:01s\n",
      "epoch 85 | loss: 0.29398 | val_0_auc: 0.90362 |  0:02:02s\n",
      "epoch 86 | loss: 0.29424 | val_0_auc: 0.90247 |  0:02:04s\n",
      "epoch 87 | loss: 0.29097 | val_0_auc: 0.90371 |  0:02:05s\n",
      "epoch 88 | loss: 0.29131 | val_0_auc: 0.90344 |  0:02:07s\n",
      "epoch 89 | loss: 0.29154 | val_0_auc: 0.90439 |  0:02:08s\n",
      "epoch 90 | loss: 0.2915  | val_0_auc: 0.90242 |  0:02:10s\n",
      "epoch 91 | loss: 0.29089 | val_0_auc: 0.90573 |  0:02:11s\n",
      "epoch 92 | loss: 0.28936 | val_0_auc: 0.90388 |  0:02:13s\n",
      "epoch 93 | loss: 0.28923 | val_0_auc: 0.90463 |  0:02:14s\n",
      "epoch 94 | loss: 0.28771 | val_0_auc: 0.90444 |  0:02:16s\n",
      "epoch 95 | loss: 0.28796 | val_0_auc: 0.90323 |  0:02:17s\n",
      "epoch 96 | loss: 0.29112 | val_0_auc: 0.90299 |  0:02:18s\n",
      "epoch 97 | loss: 0.2895  | val_0_auc: 0.90512 |  0:02:20s\n",
      "epoch 98 | loss: 0.2876  | val_0_auc: 0.90477 |  0:02:21s\n",
      "epoch 99 | loss: 0.29015 | val_0_auc: 0.90351 |  0:02:23s\n",
      "epoch 100| loss: 0.29012 | val_0_auc: 0.90323 |  0:02:24s\n",
      "epoch 101| loss: 0.28721 | val_0_auc: 0.90551 |  0:02:26s\n",
      "epoch 102| loss: 0.28745 | val_0_auc: 0.9043  |  0:02:27s\n",
      "epoch 103| loss: 0.28421 | val_0_auc: 0.90625 |  0:02:28s\n",
      "epoch 104| loss: 0.28769 | val_0_auc: 0.90686 |  0:02:30s\n",
      "epoch 105| loss: 0.28442 | val_0_auc: 0.90546 |  0:02:31s\n",
      "epoch 106| loss: 0.28502 | val_0_auc: 0.90584 |  0:02:33s\n",
      "epoch 107| loss: 0.28561 | val_0_auc: 0.90567 |  0:02:34s\n",
      "epoch 108| loss: 0.28529 | val_0_auc: 0.90802 |  0:02:35s\n",
      "epoch 109| loss: 0.28381 | val_0_auc: 0.90862 |  0:02:37s\n",
      "epoch 110| loss: 0.28234 | val_0_auc: 0.90798 |  0:02:38s\n",
      "epoch 111| loss: 0.2821  | val_0_auc: 0.90944 |  0:02:40s\n",
      "epoch 112| loss: 0.28514 | val_0_auc: 0.90788 |  0:02:41s\n",
      "epoch 113| loss: 0.28401 | val_0_auc: 0.90686 |  0:02:43s\n",
      "epoch 114| loss: 0.28405 | val_0_auc: 0.90789 |  0:02:44s\n",
      "epoch 115| loss: 0.28276 | val_0_auc: 0.90935 |  0:02:45s\n",
      "epoch 116| loss: 0.28184 | val_0_auc: 0.90874 |  0:02:47s\n",
      "epoch 117| loss: 0.28155 | val_0_auc: 0.90909 |  0:02:48s\n",
      "epoch 118| loss: 0.28375 | val_0_auc: 0.90716 |  0:02:49s\n",
      "epoch 119| loss: 0.28155 | val_0_auc: 0.90647 |  0:02:51s\n",
      "epoch 120| loss: 0.2841  | val_0_auc: 0.90675 |  0:02:52s\n",
      "epoch 121| loss: 0.28162 | val_0_auc: 0.90718 |  0:02:54s\n",
      "epoch 122| loss: 0.28168 | val_0_auc: 0.90743 |  0:02:55s\n",
      "epoch 123| loss: 0.27988 | val_0_auc: 0.90781 |  0:02:56s\n",
      "epoch 124| loss: 0.27881 | val_0_auc: 0.90742 |  0:02:58s\n",
      "epoch 125| loss: 0.27947 | val_0_auc: 0.90696 |  0:02:59s\n",
      "epoch 126| loss: 0.276   | val_0_auc: 0.90993 |  0:03:01s\n",
      "epoch 127| loss: 0.27993 | val_0_auc: 0.90807 |  0:03:02s\n",
      "epoch 128| loss: 0.28187 | val_0_auc: 0.90656 |  0:03:04s\n",
      "epoch 129| loss: 0.28165 | val_0_auc: 0.90676 |  0:03:05s\n",
      "epoch 130| loss: 0.28129 | val_0_auc: 0.90858 |  0:03:06s\n",
      "epoch 131| loss: 0.27922 | val_0_auc: 0.90852 |  0:03:08s\n",
      "epoch 132| loss: 0.27675 | val_0_auc: 0.9101  |  0:03:09s\n",
      "epoch 133| loss: 0.27593 | val_0_auc: 0.90991 |  0:03:11s\n",
      "epoch 134| loss: 0.27585 | val_0_auc: 0.90913 |  0:03:12s\n",
      "epoch 135| loss: 0.27486 | val_0_auc: 0.91121 |  0:03:13s\n",
      "epoch 136| loss: 0.2787  | val_0_auc: 0.91082 |  0:03:15s\n",
      "epoch 137| loss: 0.27478 | val_0_auc: 0.90992 |  0:03:16s\n",
      "epoch 138| loss: 0.27798 | val_0_auc: 0.90881 |  0:03:18s\n",
      "epoch 139| loss: 0.27649 | val_0_auc: 0.90891 |  0:03:19s\n",
      "epoch 140| loss: 0.27546 | val_0_auc: 0.90875 |  0:03:21s\n",
      "epoch 141| loss: 0.275   | val_0_auc: 0.90904 |  0:03:22s\n",
      "epoch 142| loss: 0.27386 | val_0_auc: 0.91008 |  0:03:23s\n",
      "epoch 143| loss: 0.27731 | val_0_auc: 0.91098 |  0:03:25s\n",
      "epoch 144| loss: 0.27517 | val_0_auc: 0.91064 |  0:03:26s\n",
      "epoch 145| loss: 0.27351 | val_0_auc: 0.90992 |  0:03:28s\n",
      "epoch 146| loss: 0.275   | val_0_auc: 0.91036 |  0:03:29s\n",
      "epoch 147| loss: 0.27515 | val_0_auc: 0.91133 |  0:03:30s\n",
      "epoch 148| loss: 0.27281 | val_0_auc: 0.9118  |  0:03:32s\n",
      "epoch 149| loss: 0.27123 | val_0_auc: 0.91191 |  0:03:33s\n",
      "epoch 150| loss: 0.27175 | val_0_auc: 0.9122  |  0:03:35s\n",
      "epoch 151| loss: 0.27358 | val_0_auc: 0.91194 |  0:03:36s\n",
      "epoch 152| loss: 0.27255 | val_0_auc: 0.91159 |  0:03:38s\n",
      "epoch 153| loss: 0.27138 | val_0_auc: 0.91226 |  0:03:39s\n",
      "epoch 154| loss: 0.27149 | val_0_auc: 0.91099 |  0:03:40s\n",
      "epoch 155| loss: 0.27384 | val_0_auc: 0.90994 |  0:03:42s\n",
      "epoch 156| loss: 0.27354 | val_0_auc: 0.91116 |  0:03:43s\n",
      "epoch 157| loss: 0.27428 | val_0_auc: 0.91185 |  0:03:45s\n",
      "epoch 158| loss: 0.27084 | val_0_auc: 0.91252 |  0:03:46s\n",
      "epoch 159| loss: 0.27261 | val_0_auc: 0.91154 |  0:03:47s\n",
      "epoch 160| loss: 0.27207 | val_0_auc: 0.91105 |  0:03:49s\n",
      "epoch 161| loss: 0.27539 | val_0_auc: 0.90924 |  0:03:50s\n",
      "epoch 162| loss: 0.27395 | val_0_auc: 0.91163 |  0:03:52s\n",
      "epoch 163| loss: 0.27023 | val_0_auc: 0.91175 |  0:03:54s\n",
      "epoch 164| loss: 0.27051 | val_0_auc: 0.91159 |  0:03:56s\n",
      "epoch 165| loss: 0.27364 | val_0_auc: 0.91083 |  0:03:58s\n",
      "epoch 166| loss: 0.27335 | val_0_auc: 0.91121 |  0:03:59s\n",
      "epoch 167| loss: 0.27226 | val_0_auc: 0.91142 |  0:04:00s\n",
      "epoch 168| loss: 0.27192 | val_0_auc: 0.91162 |  0:04:02s\n",
      "epoch 169| loss: 0.27218 | val_0_auc: 0.91099 |  0:04:03s\n",
      "epoch 170| loss: 0.27127 | val_0_auc: 0.91062 |  0:04:05s\n",
      "epoch 171| loss: 0.27182 | val_0_auc: 0.91088 |  0:04:06s\n",
      "epoch 172| loss: 0.27083 | val_0_auc: 0.91098 |  0:04:08s\n",
      "epoch 173| loss: 0.2715  | val_0_auc: 0.91039 |  0:04:09s\n",
      "epoch 174| loss: 0.2713  | val_0_auc: 0.90937 |  0:04:10s\n",
      "epoch 175| loss: 0.27633 | val_0_auc: 0.90964 |  0:04:12s\n",
      "epoch 176| loss: 0.27552 | val_0_auc: 0.90958 |  0:04:13s\n",
      "epoch 177| loss: 0.27679 | val_0_auc: 0.91042 |  0:04:15s\n",
      "epoch 178| loss: 0.27542 | val_0_auc: 0.91146 |  0:04:16s\n",
      "\n",
      "Early stopping occurred at epoch 178 with best_epoch = 158 and best_val_0_auc = 0.91252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.91723 | val_0_auc: 0.51304 |  0:00:03s\n",
      "epoch 1  | loss: 0.77949 | val_0_auc: 0.55473 |  0:00:07s\n",
      "epoch 2  | loss: 0.70859 | val_0_auc: 0.59446 |  0:00:11s\n",
      "epoch 3  | loss: 0.65244 | val_0_auc: 0.63672 |  0:00:14s\n",
      "epoch 4  | loss: 0.60872 | val_0_auc: 0.70057 |  0:00:18s\n",
      "epoch 5  | loss: 0.58329 | val_0_auc: 0.7202  |  0:00:22s\n",
      "epoch 6  | loss: 0.54749 | val_0_auc: 0.75332 |  0:00:25s\n",
      "epoch 7  | loss: 0.52515 | val_0_auc: 0.78227 |  0:00:29s\n",
      "epoch 8  | loss: 0.50039 | val_0_auc: 0.79361 |  0:00:33s\n",
      "epoch 9  | loss: 0.49225 | val_0_auc: 0.80312 |  0:00:36s\n",
      "epoch 10 | loss: 0.47812 | val_0_auc: 0.80893 |  0:00:40s\n",
      "epoch 11 | loss: 0.47142 | val_0_auc: 0.81984 |  0:00:44s\n",
      "epoch 12 | loss: 0.46589 | val_0_auc: 0.81988 |  0:00:47s\n",
      "epoch 13 | loss: 0.45172 | val_0_auc: 0.82437 |  0:00:51s\n",
      "epoch 14 | loss: 0.44099 | val_0_auc: 0.82945 |  0:00:55s\n",
      "epoch 15 | loss: 0.43611 | val_0_auc: 0.82632 |  0:00:58s\n",
      "epoch 16 | loss: 0.43237 | val_0_auc: 0.83701 |  0:01:02s\n",
      "epoch 17 | loss: 0.42487 | val_0_auc: 0.84059 |  0:01:06s\n",
      "epoch 18 | loss: 0.41839 | val_0_auc: 0.84284 |  0:01:10s\n",
      "epoch 19 | loss: 0.40939 | val_0_auc: 0.85179 |  0:01:14s\n",
      "epoch 20 | loss: 0.40664 | val_0_auc: 0.84941 |  0:01:17s\n",
      "epoch 21 | loss: 0.40706 | val_0_auc: 0.84544 |  0:01:21s\n",
      "epoch 22 | loss: 0.40252 | val_0_auc: 0.85206 |  0:01:25s\n",
      "epoch 23 | loss: 0.39829 | val_0_auc: 0.85051 |  0:01:29s\n",
      "epoch 24 | loss: 0.3968  | val_0_auc: 0.85983 |  0:01:33s\n",
      "epoch 25 | loss: 0.39178 | val_0_auc: 0.8553  |  0:01:37s\n",
      "epoch 26 | loss: 0.39104 | val_0_auc: 0.86045 |  0:01:41s\n",
      "epoch 27 | loss: 0.38588 | val_0_auc: 0.86097 |  0:01:45s\n",
      "epoch 28 | loss: 0.38394 | val_0_auc: 0.86121 |  0:01:49s\n",
      "epoch 29 | loss: 0.38005 | val_0_auc: 0.85832 |  0:01:53s\n",
      "epoch 30 | loss: 0.38029 | val_0_auc: 0.86439 |  0:01:56s\n",
      "epoch 31 | loss: 0.37397 | val_0_auc: 0.86277 |  0:02:00s\n",
      "epoch 32 | loss: 0.37762 | val_0_auc: 0.86171 |  0:02:04s\n",
      "epoch 33 | loss: 0.37381 | val_0_auc: 0.86016 |  0:02:08s\n",
      "epoch 34 | loss: 0.37354 | val_0_auc: 0.86651 |  0:02:11s\n",
      "epoch 35 | loss: 0.37138 | val_0_auc: 0.86395 |  0:02:15s\n",
      "epoch 36 | loss: 0.36742 | val_0_auc: 0.86494 |  0:02:19s\n",
      "epoch 37 | loss: 0.36509 | val_0_auc: 0.86927 |  0:02:23s\n",
      "epoch 38 | loss: 0.36555 | val_0_auc: 0.87144 |  0:02:27s\n",
      "epoch 39 | loss: 0.3644  | val_0_auc: 0.8718  |  0:02:30s\n",
      "epoch 40 | loss: 0.36254 | val_0_auc: 0.87327 |  0:02:34s\n",
      "epoch 41 | loss: 0.36132 | val_0_auc: 0.87554 |  0:02:38s\n",
      "epoch 42 | loss: 0.35959 | val_0_auc: 0.87734 |  0:02:41s\n",
      "epoch 43 | loss: 0.35812 | val_0_auc: 0.87695 |  0:02:45s\n",
      "epoch 44 | loss: 0.35672 | val_0_auc: 0.87575 |  0:02:49s\n",
      "epoch 45 | loss: 0.35565 | val_0_auc: 0.87803 |  0:02:52s\n",
      "epoch 46 | loss: 0.35314 | val_0_auc: 0.88014 |  0:02:56s\n",
      "epoch 47 | loss: 0.35134 | val_0_auc: 0.87755 |  0:03:00s\n",
      "epoch 48 | loss: 0.35011 | val_0_auc: 0.88309 |  0:03:03s\n",
      "epoch 49 | loss: 0.35097 | val_0_auc: 0.87952 |  0:03:07s\n",
      "epoch 50 | loss: 0.34968 | val_0_auc: 0.88181 |  0:03:10s\n",
      "epoch 51 | loss: 0.34824 | val_0_auc: 0.88104 |  0:03:14s\n",
      "epoch 52 | loss: 0.34585 | val_0_auc: 0.8811  |  0:03:18s\n",
      "epoch 53 | loss: 0.34732 | val_0_auc: 0.88377 |  0:03:22s\n",
      "epoch 54 | loss: 0.34435 | val_0_auc: 0.8823  |  0:03:25s\n",
      "epoch 55 | loss: 0.34492 | val_0_auc: 0.88029 |  0:03:29s\n",
      "epoch 56 | loss: 0.34602 | val_0_auc: 0.88401 |  0:03:33s\n",
      "epoch 57 | loss: 0.34278 | val_0_auc: 0.8853  |  0:03:36s\n",
      "epoch 58 | loss: 0.34236 | val_0_auc: 0.884   |  0:03:40s\n",
      "epoch 59 | loss: 0.34179 | val_0_auc: 0.88454 |  0:03:43s\n",
      "epoch 60 | loss: 0.34307 | val_0_auc: 0.88462 |  0:03:47s\n",
      "epoch 61 | loss: 0.34037 | val_0_auc: 0.88659 |  0:03:51s\n",
      "epoch 62 | loss: 0.33722 | val_0_auc: 0.88373 |  0:03:55s\n",
      "epoch 63 | loss: 0.33846 | val_0_auc: 0.88437 |  0:03:58s\n",
      "epoch 64 | loss: 0.33956 | val_0_auc: 0.88523 |  0:04:02s\n",
      "epoch 65 | loss: 0.33565 | val_0_auc: 0.88551 |  0:04:06s\n",
      "epoch 66 | loss: 0.33693 | val_0_auc: 0.88466 |  0:04:09s\n",
      "epoch 67 | loss: 0.33399 | val_0_auc: 0.88576 |  0:04:13s\n",
      "epoch 68 | loss: 0.33345 | val_0_auc: 0.8859  |  0:04:17s\n",
      "epoch 69 | loss: 0.33505 | val_0_auc: 0.88708 |  0:04:21s\n",
      "epoch 70 | loss: 0.33103 | val_0_auc: 0.88588 |  0:04:24s\n",
      "epoch 71 | loss: 0.33133 | val_0_auc: 0.8859  |  0:04:28s\n",
      "epoch 72 | loss: 0.33163 | val_0_auc: 0.88462 |  0:04:31s\n",
      "epoch 73 | loss: 0.32976 | val_0_auc: 0.88561 |  0:04:35s\n",
      "epoch 74 | loss: 0.33164 | val_0_auc: 0.88708 |  0:04:39s\n",
      "epoch 75 | loss: 0.32565 | val_0_auc: 0.88781 |  0:04:42s\n",
      "epoch 76 | loss: 0.32709 | val_0_auc: 0.88698 |  0:04:46s\n",
      "epoch 77 | loss: 0.32772 | val_0_auc: 0.88771 |  0:04:50s\n",
      "epoch 78 | loss: 0.32283 | val_0_auc: 0.89017 |  0:04:53s\n",
      "epoch 79 | loss: 0.32112 | val_0_auc: 0.89034 |  0:04:57s\n",
      "epoch 80 | loss: 0.32344 | val_0_auc: 0.889   |  0:05:01s\n",
      "epoch 81 | loss: 0.31868 | val_0_auc: 0.8906  |  0:05:04s\n",
      "epoch 82 | loss: 0.32195 | val_0_auc: 0.89032 |  0:05:08s\n",
      "epoch 83 | loss: 0.32009 | val_0_auc: 0.89042 |  0:05:12s\n",
      "epoch 84 | loss: 0.31954 | val_0_auc: 0.89093 |  0:05:15s\n",
      "epoch 85 | loss: 0.318   | val_0_auc: 0.89296 |  0:05:19s\n",
      "epoch 86 | loss: 0.31716 | val_0_auc: 0.89158 |  0:05:23s\n",
      "epoch 87 | loss: 0.31684 | val_0_auc: 0.89191 |  0:05:26s\n",
      "epoch 88 | loss: 0.31407 | val_0_auc: 0.88944 |  0:05:30s\n",
      "epoch 89 | loss: 0.31416 | val_0_auc: 0.89323 |  0:05:34s\n",
      "epoch 90 | loss: 0.3126  | val_0_auc: 0.89564 |  0:05:37s\n",
      "epoch 91 | loss: 0.30935 | val_0_auc: 0.89483 |  0:05:41s\n",
      "epoch 92 | loss: 0.30858 | val_0_auc: 0.89695 |  0:05:45s\n",
      "epoch 93 | loss: 0.30957 | val_0_auc: 0.89644 |  0:05:48s\n",
      "epoch 94 | loss: 0.30867 | val_0_auc: 0.89557 |  0:05:52s\n",
      "epoch 95 | loss: 0.30844 | val_0_auc: 0.89755 |  0:05:56s\n",
      "epoch 96 | loss: 0.30648 | val_0_auc: 0.89578 |  0:05:59s\n",
      "epoch 97 | loss: 0.30583 | val_0_auc: 0.89769 |  0:06:03s\n",
      "epoch 98 | loss: 0.30281 | val_0_auc: 0.89822 |  0:06:07s\n",
      "epoch 99 | loss: 0.29968 | val_0_auc: 0.89644 |  0:06:10s\n",
      "epoch 100| loss: 0.30132 | val_0_auc: 0.89755 |  0:06:14s\n",
      "epoch 101| loss: 0.30199 | val_0_auc: 0.89816 |  0:06:17s\n",
      "epoch 102| loss: 0.29978 | val_0_auc: 0.89825 |  0:06:21s\n",
      "epoch 103| loss: 0.29759 | val_0_auc: 0.89821 |  0:06:25s\n",
      "epoch 104| loss: 0.2969  | val_0_auc: 0.90174 |  0:06:28s\n",
      "epoch 105| loss: 0.29557 | val_0_auc: 0.8986  |  0:06:32s\n",
      "epoch 106| loss: 0.29318 | val_0_auc: 0.9009  |  0:06:36s\n",
      "epoch 107| loss: 0.2942  | val_0_auc: 0.90102 |  0:06:39s\n",
      "epoch 108| loss: 0.29093 | val_0_auc: 0.90185 |  0:06:43s\n",
      "epoch 109| loss: 0.29144 | val_0_auc: 0.90077 |  0:06:47s\n",
      "epoch 110| loss: 0.29159 | val_0_auc: 0.90431 |  0:06:51s\n",
      "epoch 111| loss: 0.29113 | val_0_auc: 0.90141 |  0:06:54s\n",
      "epoch 112| loss: 0.28663 | val_0_auc: 0.90279 |  0:06:58s\n",
      "epoch 113| loss: 0.28594 | val_0_auc: 0.90247 |  0:07:02s\n",
      "epoch 114| loss: 0.28652 | val_0_auc: 0.90369 |  0:07:05s\n",
      "epoch 115| loss: 0.28794 | val_0_auc: 0.90594 |  0:07:09s\n",
      "epoch 116| loss: 0.28368 | val_0_auc: 0.90376 |  0:07:13s\n",
      "epoch 117| loss: 0.28507 | val_0_auc: 0.90476 |  0:07:17s\n",
      "epoch 118| loss: 0.28044 | val_0_auc: 0.90525 |  0:07:21s\n",
      "epoch 119| loss: 0.28027 | val_0_auc: 0.90578 |  0:07:25s\n",
      "epoch 120| loss: 0.27763 | val_0_auc: 0.90538 |  0:07:29s\n",
      "epoch 121| loss: 0.27794 | val_0_auc: 0.90434 |  0:07:33s\n",
      "epoch 122| loss: 0.27636 | val_0_auc: 0.90437 |  0:07:36s\n",
      "epoch 123| loss: 0.27716 | val_0_auc: 0.90465 |  0:07:40s\n",
      "epoch 124| loss: 0.27902 | val_0_auc: 0.90505 |  0:07:44s\n",
      "epoch 125| loss: 0.2734  | val_0_auc: 0.90567 |  0:07:48s\n",
      "epoch 126| loss: 0.277   | val_0_auc: 0.90459 |  0:07:51s\n",
      "epoch 127| loss: 0.27384 | val_0_auc: 0.90421 |  0:07:55s\n",
      "epoch 128| loss: 0.27265 | val_0_auc: 0.90642 |  0:07:58s\n",
      "epoch 129| loss: 0.27265 | val_0_auc: 0.90543 |  0:08:02s\n",
      "epoch 130| loss: 0.27158 | val_0_auc: 0.90563 |  0:08:06s\n",
      "epoch 131| loss: 0.26931 | val_0_auc: 0.90497 |  0:08:09s\n",
      "epoch 132| loss: 0.26959 | val_0_auc: 0.90425 |  0:08:13s\n",
      "epoch 133| loss: 0.26762 | val_0_auc: 0.9036  |  0:08:16s\n",
      "epoch 134| loss: 0.26699 | val_0_auc: 0.90343 |  0:08:20s\n",
      "epoch 135| loss: 0.26731 | val_0_auc: 0.90545 |  0:08:24s\n",
      "epoch 136| loss: 0.26447 | val_0_auc: 0.90648 |  0:08:27s\n",
      "epoch 137| loss: 0.26381 | val_0_auc: 0.90539 |  0:08:31s\n",
      "epoch 138| loss: 0.2646  | val_0_auc: 0.90422 |  0:08:35s\n",
      "epoch 139| loss: 0.25978 | val_0_auc: 0.90433 |  0:08:38s\n",
      "epoch 140| loss: 0.26072 | val_0_auc: 0.90648 |  0:08:42s\n",
      "epoch 141| loss: 0.25843 | val_0_auc: 0.90543 |  0:08:45s\n",
      "epoch 142| loss: 0.25779 | val_0_auc: 0.90378 |  0:08:49s\n",
      "epoch 143| loss: 0.26059 | val_0_auc: 0.90318 |  0:08:54s\n",
      "epoch 144| loss: 0.25584 | val_0_auc: 0.90419 |  0:08:57s\n",
      "epoch 145| loss: 0.25536 | val_0_auc: 0.90191 |  0:09:01s\n",
      "epoch 146| loss: 0.25362 | val_0_auc: 0.90218 |  0:09:05s\n",
      "epoch 147| loss: 0.25471 | val_0_auc: 0.9028  |  0:09:08s\n",
      "epoch 148| loss: 0.25233 | val_0_auc: 0.90345 |  0:09:12s\n",
      "epoch 149| loss: 0.2513  | val_0_auc: 0.90349 |  0:09:16s\n",
      "epoch 150| loss: 0.25189 | val_0_auc: 0.90253 |  0:09:19s\n",
      "epoch 151| loss: 0.24946 | val_0_auc: 0.90128 |  0:09:23s\n",
      "epoch 152| loss: 0.24926 | val_0_auc: 0.90135 |  0:09:27s\n",
      "epoch 153| loss: 0.24594 | val_0_auc: 0.90217 |  0:09:30s\n",
      "epoch 154| loss: 0.24549 | val_0_auc: 0.90101 |  0:09:34s\n",
      "epoch 155| loss: 0.24452 | val_0_auc: 0.90007 |  0:09:38s\n",
      "epoch 156| loss: 0.24343 | val_0_auc: 0.90164 |  0:09:42s\n",
      "epoch 157| loss: 0.24193 | val_0_auc: 0.90047 |  0:09:46s\n",
      "epoch 158| loss: 0.23897 | val_0_auc: 0.90079 |  0:09:49s\n",
      "epoch 159| loss: 0.2401  | val_0_auc: 0.89926 |  0:09:53s\n",
      "epoch 160| loss: 0.2401  | val_0_auc: 0.90045 |  0:09:56s\n",
      "\n",
      "Early stopping occurred at epoch 160 with best_epoch = 140 and best_val_0_auc = 0.90648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.85349 | val_0_auc: 0.56597 |  0:00:04s\n",
      "epoch 1  | loss: 0.57964 | val_0_auc: 0.66693 |  0:00:09s\n",
      "epoch 2  | loss: 0.52245 | val_0_auc: 0.70254 |  0:00:12s\n",
      "epoch 3  | loss: 0.50006 | val_0_auc: 0.72105 |  0:00:16s\n",
      "epoch 4  | loss: 0.49456 | val_0_auc: 0.74995 |  0:00:19s\n",
      "epoch 5  | loss: 0.478   | val_0_auc: 0.79291 |  0:00:23s\n",
      "epoch 6  | loss: 0.44458 | val_0_auc: 0.81317 |  0:00:27s\n",
      "epoch 7  | loss: 0.41481 | val_0_auc: 0.83186 |  0:00:30s\n",
      "epoch 8  | loss: 0.41093 | val_0_auc: 0.83415 |  0:00:34s\n",
      "epoch 9  | loss: 0.3953  | val_0_auc: 0.84737 |  0:00:38s\n",
      "epoch 10 | loss: 0.38573 | val_0_auc: 0.85481 |  0:00:41s\n",
      "epoch 11 | loss: 0.37757 | val_0_auc: 0.85926 |  0:00:44s\n",
      "epoch 12 | loss: 0.36738 | val_0_auc: 0.87337 |  0:00:48s\n",
      "epoch 13 | loss: 0.36045 | val_0_auc: 0.87053 |  0:00:52s\n",
      "epoch 14 | loss: 0.35271 | val_0_auc: 0.87689 |  0:00:55s\n",
      "epoch 15 | loss: 0.3426  | val_0_auc: 0.87174 |  0:00:58s\n",
      "epoch 16 | loss: 0.33942 | val_0_auc: 0.8742  |  0:01:02s\n",
      "epoch 17 | loss: 0.33622 | val_0_auc: 0.87921 |  0:01:05s\n",
      "epoch 18 | loss: 0.3406  | val_0_auc: 0.87511 |  0:01:09s\n",
      "epoch 19 | loss: 0.33844 | val_0_auc: 0.88237 |  0:01:12s\n",
      "epoch 20 | loss: 0.33062 | val_0_auc: 0.88697 |  0:01:16s\n",
      "epoch 21 | loss: 0.32321 | val_0_auc: 0.89214 |  0:01:19s\n",
      "epoch 22 | loss: 0.32205 | val_0_auc: 0.89605 |  0:01:23s\n",
      "epoch 23 | loss: 0.31972 | val_0_auc: 0.89627 |  0:01:26s\n",
      "epoch 24 | loss: 0.31838 | val_0_auc: 0.89608 |  0:01:30s\n",
      "epoch 25 | loss: 0.32053 | val_0_auc: 0.89131 |  0:01:33s\n",
      "epoch 26 | loss: 0.32084 | val_0_auc: 0.89424 |  0:01:37s\n",
      "epoch 27 | loss: 0.3211  | val_0_auc: 0.89321 |  0:01:40s\n",
      "epoch 28 | loss: 0.31621 | val_0_auc: 0.89114 |  0:01:44s\n",
      "epoch 29 | loss: 0.31896 | val_0_auc: 0.89504 |  0:01:47s\n",
      "epoch 30 | loss: 0.31381 | val_0_auc: 0.89667 |  0:01:51s\n",
      "epoch 31 | loss: 0.31202 | val_0_auc: 0.89854 |  0:01:54s\n",
      "epoch 32 | loss: 0.31175 | val_0_auc: 0.89898 |  0:01:58s\n",
      "epoch 33 | loss: 0.30664 | val_0_auc: 0.89951 |  0:02:01s\n",
      "epoch 34 | loss: 0.31228 | val_0_auc: 0.89725 |  0:02:05s\n",
      "epoch 35 | loss: 0.30911 | val_0_auc: 0.89772 |  0:02:08s\n",
      "epoch 36 | loss: 0.31625 | val_0_auc: 0.89145 |  0:02:12s\n",
      "epoch 37 | loss: 0.32225 | val_0_auc: 0.89359 |  0:02:15s\n",
      "epoch 38 | loss: 0.31845 | val_0_auc: 0.89652 |  0:02:19s\n",
      "epoch 39 | loss: 0.31046 | val_0_auc: 0.89531 |  0:02:22s\n",
      "epoch 40 | loss: 0.30663 | val_0_auc: 0.89719 |  0:02:25s\n",
      "epoch 41 | loss: 0.3053  | val_0_auc: 0.89346 |  0:02:29s\n",
      "epoch 42 | loss: 0.3111  | val_0_auc: 0.89259 |  0:02:32s\n",
      "epoch 43 | loss: 0.31289 | val_0_auc: 0.894   |  0:02:36s\n",
      "epoch 44 | loss: 0.30968 | val_0_auc: 0.89406 |  0:02:39s\n",
      "epoch 45 | loss: 0.30828 | val_0_auc: 0.89583 |  0:02:43s\n",
      "epoch 46 | loss: 0.30348 | val_0_auc: 0.89638 |  0:02:46s\n",
      "epoch 47 | loss: 0.30304 | val_0_auc: 0.89962 |  0:02:50s\n",
      "epoch 48 | loss: 0.29913 | val_0_auc: 0.90167 |  0:02:53s\n",
      "epoch 49 | loss: 0.29943 | val_0_auc: 0.90327 |  0:02:57s\n",
      "epoch 50 | loss: 0.30107 | val_0_auc: 0.89746 |  0:03:00s\n",
      "epoch 51 | loss: 0.31375 | val_0_auc: 0.89073 |  0:03:04s\n",
      "epoch 52 | loss: 0.3163  | val_0_auc: 0.88792 |  0:03:07s\n",
      "epoch 53 | loss: 0.31854 | val_0_auc: 0.89046 |  0:03:11s\n",
      "epoch 54 | loss: 0.32294 | val_0_auc: 0.88297 |  0:03:14s\n",
      "epoch 55 | loss: 0.32027 | val_0_auc: 0.88882 |  0:03:17s\n",
      "epoch 56 | loss: 0.31408 | val_0_auc: 0.88602 |  0:03:21s\n",
      "epoch 57 | loss: 0.32259 | val_0_auc: 0.88516 |  0:03:24s\n",
      "epoch 58 | loss: 0.31974 | val_0_auc: 0.88447 |  0:03:28s\n",
      "epoch 59 | loss: 0.31939 | val_0_auc: 0.88654 |  0:03:31s\n",
      "epoch 60 | loss: 0.3183  | val_0_auc: 0.89074 |  0:03:35s\n",
      "epoch 61 | loss: 0.31644 | val_0_auc: 0.89639 |  0:03:38s\n",
      "epoch 62 | loss: 0.30992 | val_0_auc: 0.9021  |  0:03:42s\n",
      "epoch 63 | loss: 0.30679 | val_0_auc: 0.90147 |  0:03:45s\n",
      "epoch 64 | loss: 0.30498 | val_0_auc: 0.90354 |  0:03:49s\n",
      "epoch 65 | loss: 0.30432 | val_0_auc: 0.90307 |  0:03:52s\n",
      "epoch 66 | loss: 0.30318 | val_0_auc: 0.90352 |  0:03:55s\n",
      "epoch 67 | loss: 0.30166 | val_0_auc: 0.90465 |  0:03:59s\n",
      "epoch 68 | loss: 0.29902 | val_0_auc: 0.90907 |  0:04:02s\n",
      "epoch 69 | loss: 0.29753 | val_0_auc: 0.91022 |  0:04:06s\n",
      "epoch 70 | loss: 0.29576 | val_0_auc: 0.91033 |  0:04:09s\n",
      "epoch 71 | loss: 0.29755 | val_0_auc: 0.91042 |  0:04:13s\n",
      "epoch 72 | loss: 0.2971  | val_0_auc: 0.90772 |  0:04:16s\n",
      "epoch 73 | loss: 0.30326 | val_0_auc: 0.89908 |  0:04:20s\n",
      "epoch 74 | loss: 0.30858 | val_0_auc: 0.89803 |  0:04:23s\n",
      "epoch 75 | loss: 0.31375 | val_0_auc: 0.8934  |  0:04:27s\n",
      "epoch 76 | loss: 0.31324 | val_0_auc: 0.89395 |  0:04:30s\n",
      "epoch 77 | loss: 0.31125 | val_0_auc: 0.89396 |  0:04:34s\n",
      "epoch 78 | loss: 0.30661 | val_0_auc: 0.89848 |  0:04:37s\n",
      "epoch 79 | loss: 0.30304 | val_0_auc: 0.89986 |  0:04:41s\n",
      "epoch 80 | loss: 0.30252 | val_0_auc: 0.9019  |  0:04:44s\n",
      "epoch 81 | loss: 0.29626 | val_0_auc: 0.90076 |  0:04:47s\n",
      "epoch 82 | loss: 0.29652 | val_0_auc: 0.90495 |  0:04:51s\n",
      "epoch 83 | loss: 0.29491 | val_0_auc: 0.90442 |  0:04:54s\n",
      "epoch 84 | loss: 0.29284 | val_0_auc: 0.90668 |  0:04:58s\n",
      "epoch 85 | loss: 0.28898 | val_0_auc: 0.90722 |  0:05:01s\n",
      "epoch 86 | loss: 0.28838 | val_0_auc: 0.90884 |  0:05:05s\n",
      "epoch 87 | loss: 0.28778 | val_0_auc: 0.90956 |  0:05:08s\n",
      "epoch 88 | loss: 0.28836 | val_0_auc: 0.91025 |  0:05:12s\n",
      "epoch 89 | loss: 0.28695 | val_0_auc: 0.90746 |  0:05:15s\n",
      "epoch 90 | loss: 0.28662 | val_0_auc: 0.90745 |  0:05:19s\n",
      "epoch 91 | loss: 0.28608 | val_0_auc: 0.90928 |  0:05:22s\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 71 and best_val_0_auc = 0.91042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.89205 | val_0_auc: 0.48487 |  0:00:04s\n",
      "epoch 1  | loss: 0.842   | val_0_auc: 0.50934 |  0:00:09s\n",
      "epoch 2  | loss: 0.84326 | val_0_auc: 0.50546 |  0:00:14s\n",
      "epoch 3  | loss: 0.80524 | val_0_auc: 0.49541 |  0:00:19s\n",
      "epoch 4  | loss: 0.80144 | val_0_auc: 0.49952 |  0:00:24s\n",
      "epoch 5  | loss: 0.77745 | val_0_auc: 0.5104  |  0:00:29s\n",
      "epoch 6  | loss: 0.77839 | val_0_auc: 0.50851 |  0:00:33s\n",
      "epoch 7  | loss: 0.77435 | val_0_auc: 0.50416 |  0:00:38s\n",
      "epoch 8  | loss: 0.74849 | val_0_auc: 0.50153 |  0:00:43s\n",
      "epoch 9  | loss: 0.75043 | val_0_auc: 0.51073 |  0:00:48s\n",
      "epoch 10 | loss: 0.73622 | val_0_auc: 0.48737 |  0:00:53s\n",
      "epoch 11 | loss: 0.72236 | val_0_auc: 0.50443 |  0:00:57s\n",
      "epoch 12 | loss: 0.71968 | val_0_auc: 0.51984 |  0:01:02s\n",
      "epoch 13 | loss: 0.70383 | val_0_auc: 0.51476 |  0:01:07s\n",
      "epoch 14 | loss: 0.68316 | val_0_auc: 0.51761 |  0:01:12s\n",
      "epoch 15 | loss: 0.68379 | val_0_auc: 0.52869 |  0:01:17s\n",
      "epoch 16 | loss: 0.68355 | val_0_auc: 0.53271 |  0:01:22s\n",
      "epoch 17 | loss: 0.66939 | val_0_auc: 0.50551 |  0:01:27s\n",
      "epoch 18 | loss: 0.67636 | val_0_auc: 0.5233  |  0:01:32s\n",
      "epoch 19 | loss: 0.66699 | val_0_auc: 0.5054  |  0:01:37s\n",
      "epoch 20 | loss: 0.65896 | val_0_auc: 0.52458 |  0:01:41s\n",
      "epoch 21 | loss: 0.65428 | val_0_auc: 0.50847 |  0:01:46s\n",
      "epoch 22 | loss: 0.64856 | val_0_auc: 0.54001 |  0:01:51s\n",
      "epoch 23 | loss: 0.63862 | val_0_auc: 0.54378 |  0:01:56s\n",
      "epoch 24 | loss: 0.63844 | val_0_auc: 0.54108 |  0:02:01s\n",
      "epoch 25 | loss: 0.64475 | val_0_auc: 0.52955 |  0:02:06s\n",
      "epoch 26 | loss: 0.63585 | val_0_auc: 0.53738 |  0:02:11s\n",
      "epoch 27 | loss: 0.64175 | val_0_auc: 0.52704 |  0:02:15s\n",
      "epoch 28 | loss: 0.63064 | val_0_auc: 0.52682 |  0:02:20s\n",
      "epoch 29 | loss: 0.62852 | val_0_auc: 0.54487 |  0:02:25s\n",
      "epoch 30 | loss: 0.61861 | val_0_auc: 0.5601  |  0:02:30s\n",
      "epoch 31 | loss: 0.61536 | val_0_auc: 0.54008 |  0:02:35s\n",
      "epoch 32 | loss: 0.61023 | val_0_auc: 0.54948 |  0:02:40s\n",
      "epoch 33 | loss: 0.6059  | val_0_auc: 0.55212 |  0:02:44s\n",
      "epoch 34 | loss: 0.59785 | val_0_auc: 0.5673  |  0:02:49s\n",
      "epoch 35 | loss: 0.59408 | val_0_auc: 0.56585 |  0:02:54s\n",
      "epoch 36 | loss: 0.59355 | val_0_auc: 0.58208 |  0:02:59s\n",
      "epoch 37 | loss: 0.59159 | val_0_auc: 0.56838 |  0:03:04s\n",
      "epoch 38 | loss: 0.58748 | val_0_auc: 0.57786 |  0:03:09s\n",
      "epoch 39 | loss: 0.58597 | val_0_auc: 0.60054 |  0:03:14s\n",
      "epoch 40 | loss: 0.58689 | val_0_auc: 0.58908 |  0:03:18s\n",
      "epoch 41 | loss: 0.58663 | val_0_auc: 0.59403 |  0:03:23s\n",
      "epoch 42 | loss: 0.58271 | val_0_auc: 0.59101 |  0:03:28s\n",
      "epoch 43 | loss: 0.57263 | val_0_auc: 0.59894 |  0:03:33s\n",
      "epoch 44 | loss: 0.57414 | val_0_auc: 0.61474 |  0:03:38s\n",
      "epoch 45 | loss: 0.57245 | val_0_auc: 0.60517 |  0:03:43s\n",
      "epoch 46 | loss: 0.5693  | val_0_auc: 0.58705 |  0:03:47s\n",
      "epoch 47 | loss: 0.57225 | val_0_auc: 0.60844 |  0:03:52s\n",
      "epoch 48 | loss: 0.56954 | val_0_auc: 0.61396 |  0:03:57s\n",
      "epoch 49 | loss: 0.56421 | val_0_auc: 0.60829 |  0:04:01s\n",
      "epoch 50 | loss: 0.5719  | val_0_auc: 0.61541 |  0:04:06s\n",
      "epoch 51 | loss: 0.56512 | val_0_auc: 0.60932 |  0:04:11s\n",
      "epoch 52 | loss: 0.5608  | val_0_auc: 0.62206 |  0:04:17s\n",
      "epoch 53 | loss: 0.56036 | val_0_auc: 0.60506 |  0:04:22s\n",
      "epoch 54 | loss: 0.55626 | val_0_auc: 0.62356 |  0:04:26s\n",
      "epoch 55 | loss: 0.55825 | val_0_auc: 0.61037 |  0:04:31s\n",
      "epoch 56 | loss: 0.55347 | val_0_auc: 0.61797 |  0:04:36s\n",
      "epoch 57 | loss: 0.55416 | val_0_auc: 0.61181 |  0:04:41s\n",
      "epoch 58 | loss: 0.54884 | val_0_auc: 0.62483 |  0:04:46s\n",
      "epoch 59 | loss: 0.54825 | val_0_auc: 0.61352 |  0:04:51s\n",
      "epoch 60 | loss: 0.54751 | val_0_auc: 0.64035 |  0:04:56s\n",
      "epoch 61 | loss: 0.54374 | val_0_auc: 0.62283 |  0:05:01s\n",
      "epoch 62 | loss: 0.54412 | val_0_auc: 0.61437 |  0:05:06s\n",
      "epoch 63 | loss: 0.54576 | val_0_auc: 0.61263 |  0:05:11s\n",
      "epoch 64 | loss: 0.54265 | val_0_auc: 0.62285 |  0:05:15s\n",
      "epoch 65 | loss: 0.54377 | val_0_auc: 0.62136 |  0:05:20s\n",
      "epoch 66 | loss: 0.53982 | val_0_auc: 0.63707 |  0:05:25s\n",
      "epoch 67 | loss: 0.54158 | val_0_auc: 0.63254 |  0:05:30s\n",
      "epoch 68 | loss: 0.53514 | val_0_auc: 0.63361 |  0:05:35s\n",
      "epoch 69 | loss: 0.52978 | val_0_auc: 0.633   |  0:05:40s\n",
      "epoch 70 | loss: 0.5323  | val_0_auc: 0.64911 |  0:05:44s\n",
      "epoch 71 | loss: 0.52954 | val_0_auc: 0.6336  |  0:05:49s\n",
      "epoch 72 | loss: 0.53039 | val_0_auc: 0.65457 |  0:05:54s\n",
      "epoch 73 | loss: 0.52816 | val_0_auc: 0.65482 |  0:05:59s\n",
      "epoch 74 | loss: 0.53032 | val_0_auc: 0.6551  |  0:06:04s\n",
      "epoch 75 | loss: 0.5289  | val_0_auc: 0.65083 |  0:06:09s\n",
      "epoch 76 | loss: 0.52844 | val_0_auc: 0.66751 |  0:06:13s\n",
      "epoch 77 | loss: 0.5224  | val_0_auc: 0.66314 |  0:06:18s\n",
      "epoch 78 | loss: 0.52389 | val_0_auc: 0.66656 |  0:06:23s\n",
      "epoch 79 | loss: 0.524   | val_0_auc: 0.66522 |  0:06:28s\n",
      "epoch 80 | loss: 0.5272  | val_0_auc: 0.66356 |  0:06:33s\n",
      "epoch 81 | loss: 0.52416 | val_0_auc: 0.68144 |  0:06:38s\n",
      "epoch 82 | loss: 0.5248  | val_0_auc: 0.67032 |  0:06:43s\n",
      "epoch 83 | loss: 0.52125 | val_0_auc: 0.6889  |  0:06:47s\n",
      "epoch 84 | loss: 0.5222  | val_0_auc: 0.67994 |  0:06:52s\n",
      "epoch 85 | loss: 0.52387 | val_0_auc: 0.68568 |  0:06:57s\n",
      "epoch 86 | loss: 0.52219 | val_0_auc: 0.69524 |  0:07:02s\n",
      "epoch 87 | loss: 0.52074 | val_0_auc: 0.6911  |  0:07:07s\n",
      "epoch 88 | loss: 0.52025 | val_0_auc: 0.67922 |  0:07:11s\n",
      "epoch 89 | loss: 0.51963 | val_0_auc: 0.69481 |  0:07:16s\n",
      "epoch 90 | loss: 0.51758 | val_0_auc: 0.6917  |  0:07:21s\n",
      "epoch 91 | loss: 0.52034 | val_0_auc: 0.67451 |  0:07:25s\n",
      "epoch 92 | loss: 0.52064 | val_0_auc: 0.67927 |  0:07:30s\n",
      "epoch 93 | loss: 0.52033 | val_0_auc: 0.68901 |  0:07:35s\n",
      "epoch 94 | loss: 0.52015 | val_0_auc: 0.69234 |  0:07:40s\n",
      "epoch 95 | loss: 0.51846 | val_0_auc: 0.6862  |  0:07:44s\n",
      "epoch 96 | loss: 0.51699 | val_0_auc: 0.69053 |  0:07:49s\n",
      "epoch 97 | loss: 0.51651 | val_0_auc: 0.67879 |  0:07:54s\n",
      "epoch 98 | loss: 0.51445 | val_0_auc: 0.6773  |  0:07:59s\n",
      "epoch 99 | loss: 0.51634 | val_0_auc: 0.6769  |  0:08:04s\n",
      "epoch 100| loss: 0.51718 | val_0_auc: 0.69622 |  0:08:08s\n",
      "epoch 101| loss: 0.51783 | val_0_auc: 0.68693 |  0:08:13s\n",
      "epoch 102| loss: 0.51634 | val_0_auc: 0.66634 |  0:08:18s\n",
      "epoch 103| loss: 0.51782 | val_0_auc: 0.69287 |  0:08:23s\n",
      "epoch 104| loss: 0.51519 | val_0_auc: 0.68536 |  0:08:27s\n",
      "epoch 105| loss: 0.5154  | val_0_auc: 0.68925 |  0:08:32s\n",
      "epoch 106| loss: 0.51893 | val_0_auc: 0.69583 |  0:08:37s\n",
      "epoch 107| loss: 0.51908 | val_0_auc: 0.69365 |  0:08:42s\n",
      "epoch 108| loss: 0.52273 | val_0_auc: 0.69691 |  0:08:47s\n",
      "epoch 109| loss: 0.51983 | val_0_auc: 0.69709 |  0:08:52s\n",
      "epoch 110| loss: 0.51634 | val_0_auc: 0.69557 |  0:08:56s\n",
      "epoch 111| loss: 0.51768 | val_0_auc: 0.69388 |  0:09:01s\n",
      "epoch 112| loss: 0.51712 | val_0_auc: 0.68984 |  0:09:06s\n",
      "epoch 113| loss: 0.51747 | val_0_auc: 0.70556 |  0:09:11s\n",
      "epoch 114| loss: 0.51513 | val_0_auc: 0.69933 |  0:09:15s\n",
      "epoch 115| loss: 0.51664 | val_0_auc: 0.68898 |  0:09:20s\n",
      "epoch 116| loss: 0.51708 | val_0_auc: 0.70215 |  0:09:25s\n",
      "epoch 117| loss: 0.51448 | val_0_auc: 0.699   |  0:09:30s\n",
      "epoch 118| loss: 0.51332 | val_0_auc: 0.68565 |  0:09:35s\n",
      "epoch 119| loss: 0.51623 | val_0_auc: 0.68713 |  0:09:40s\n",
      "epoch 120| loss: 0.51829 | val_0_auc: 0.68785 |  0:09:45s\n",
      "epoch 121| loss: 0.51635 | val_0_auc: 0.70585 |  0:09:50s\n",
      "epoch 122| loss: 0.51371 | val_0_auc: 0.69964 |  0:09:54s\n",
      "epoch 123| loss: 0.5142  | val_0_auc: 0.70521 |  0:09:59s\n",
      "epoch 124| loss: 0.51324 | val_0_auc: 0.69905 |  0:10:04s\n",
      "epoch 125| loss: 0.51348 | val_0_auc: 0.69301 |  0:10:09s\n",
      "epoch 126| loss: 0.51275 | val_0_auc: 0.69832 |  0:10:14s\n",
      "epoch 127| loss: 0.50996 | val_0_auc: 0.71675 |  0:10:19s\n",
      "epoch 128| loss: 0.51203 | val_0_auc: 0.70403 |  0:10:23s\n",
      "epoch 129| loss: 0.51091 | val_0_auc: 0.71708 |  0:10:28s\n",
      "epoch 130| loss: 0.51359 | val_0_auc: 0.7106  |  0:10:33s\n",
      "epoch 131| loss: 0.51359 | val_0_auc: 0.69713 |  0:10:38s\n",
      "epoch 132| loss: 0.51102 | val_0_auc: 0.7117  |  0:10:43s\n",
      "epoch 133| loss: 0.50869 | val_0_auc: 0.69781 |  0:10:48s\n",
      "epoch 134| loss: 0.51222 | val_0_auc: 0.693   |  0:10:52s\n",
      "epoch 135| loss: 0.50766 | val_0_auc: 0.69762 |  0:10:57s\n",
      "epoch 136| loss: 0.5131  | val_0_auc: 0.69843 |  0:11:02s\n",
      "epoch 137| loss: 0.50991 | val_0_auc: 0.70665 |  0:11:07s\n",
      "epoch 138| loss: 0.51482 | val_0_auc: 0.69702 |  0:11:12s\n",
      "epoch 139| loss: 0.50745 | val_0_auc: 0.70578 |  0:11:17s\n",
      "epoch 140| loss: 0.51108 | val_0_auc: 0.70069 |  0:11:21s\n",
      "epoch 141| loss: 0.50792 | val_0_auc: 0.69455 |  0:11:26s\n",
      "epoch 142| loss: 0.51139 | val_0_auc: 0.69377 |  0:11:31s\n",
      "epoch 143| loss: 0.50601 | val_0_auc: 0.69014 |  0:11:36s\n",
      "epoch 144| loss: 0.51131 | val_0_auc: 0.71541 |  0:11:41s\n",
      "epoch 145| loss: 0.50754 | val_0_auc: 0.69549 |  0:11:46s\n",
      "epoch 146| loss: 0.50681 | val_0_auc: 0.6913  |  0:11:51s\n",
      "epoch 147| loss: 0.51139 | val_0_auc: 0.71054 |  0:11:56s\n",
      "epoch 148| loss: 0.50787 | val_0_auc: 0.71182 |  0:12:01s\n",
      "epoch 149| loss: 0.50816 | val_0_auc: 0.69636 |  0:12:06s\n",
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 129 and best_val_0_auc = 0.71708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70384 | val_0_auc: 0.52171 |  0:00:05s\n",
      "epoch 1  | loss: 0.59949 | val_0_auc: 0.63212 |  0:00:10s\n",
      "epoch 2  | loss: 0.58469 | val_0_auc: 0.63893 |  0:00:15s\n",
      "epoch 3  | loss: 0.56648 | val_0_auc: 0.732   |  0:00:20s\n",
      "epoch 4  | loss: 0.54622 | val_0_auc: 0.69705 |  0:00:26s\n",
      "epoch 5  | loss: 0.52151 | val_0_auc: 0.66589 |  0:00:31s\n",
      "epoch 6  | loss: 0.49147 | val_0_auc: 0.6425  |  0:00:36s\n",
      "epoch 7  | loss: 0.46727 | val_0_auc: 0.68486 |  0:00:41s\n",
      "epoch 8  | loss: 0.45445 | val_0_auc: 0.64578 |  0:00:46s\n",
      "epoch 9  | loss: 0.45545 | val_0_auc: 0.78283 |  0:00:51s\n",
      "epoch 10 | loss: 0.45521 | val_0_auc: 0.78892 |  0:00:56s\n",
      "epoch 11 | loss: 0.42219 | val_0_auc: 0.84372 |  0:01:01s\n",
      "epoch 12 | loss: 0.40189 | val_0_auc: 0.83348 |  0:01:06s\n",
      "epoch 13 | loss: 0.41603 | val_0_auc: 0.81526 |  0:01:11s\n",
      "epoch 14 | loss: 0.40238 | val_0_auc: 0.85951 |  0:01:16s\n",
      "epoch 15 | loss: 0.38141 | val_0_auc: 0.86261 |  0:01:22s\n",
      "epoch 16 | loss: 0.37358 | val_0_auc: 0.86654 |  0:01:27s\n",
      "epoch 17 | loss: 0.36863 | val_0_auc: 0.85496 |  0:01:32s\n",
      "epoch 18 | loss: 0.3647  | val_0_auc: 0.86599 |  0:01:37s\n",
      "epoch 19 | loss: 0.35099 | val_0_auc: 0.87607 |  0:01:42s\n",
      "epoch 20 | loss: 0.34078 | val_0_auc: 0.88126 |  0:01:47s\n",
      "epoch 21 | loss: 0.33675 | val_0_auc: 0.88329 |  0:01:52s\n",
      "epoch 22 | loss: 0.33761 | val_0_auc: 0.87686 |  0:01:57s\n",
      "epoch 23 | loss: 0.3379  | val_0_auc: 0.88365 |  0:02:03s\n",
      "epoch 24 | loss: 0.34376 | val_0_auc: 0.87722 |  0:02:09s\n",
      "epoch 25 | loss: 0.34718 | val_0_auc: 0.8765  |  0:02:15s\n",
      "epoch 26 | loss: 0.35235 | val_0_auc: 0.87922 |  0:02:20s\n",
      "epoch 27 | loss: 0.35518 | val_0_auc: 0.8695  |  0:02:25s\n",
      "epoch 28 | loss: 0.35229 | val_0_auc: 0.87223 |  0:02:30s\n",
      "epoch 29 | loss: 0.3541  | val_0_auc: 0.87149 |  0:02:35s\n",
      "epoch 30 | loss: 0.35061 | val_0_auc: 0.8704  |  0:02:40s\n",
      "epoch 31 | loss: 0.34547 | val_0_auc: 0.87737 |  0:02:45s\n",
      "epoch 32 | loss: 0.34493 | val_0_auc: 0.83709 |  0:02:51s\n",
      "epoch 33 | loss: 0.34036 | val_0_auc: 0.87124 |  0:02:56s\n",
      "epoch 34 | loss: 0.33238 | val_0_auc: 0.86586 |  0:03:01s\n",
      "epoch 35 | loss: 0.3319  | val_0_auc: 0.87805 |  0:03:06s\n",
      "epoch 36 | loss: 0.33427 | val_0_auc: 0.88303 |  0:03:11s\n",
      "epoch 37 | loss: 0.3327  | val_0_auc: 0.88376 |  0:03:16s\n",
      "epoch 38 | loss: 0.33025 | val_0_auc: 0.88544 |  0:03:21s\n",
      "epoch 39 | loss: 0.33021 | val_0_auc: 0.88953 |  0:03:26s\n",
      "epoch 40 | loss: 0.32597 | val_0_auc: 0.88972 |  0:03:31s\n",
      "epoch 41 | loss: 0.32701 | val_0_auc: 0.8873  |  0:03:36s\n",
      "epoch 42 | loss: 0.32568 | val_0_auc: 0.89062 |  0:03:42s\n",
      "epoch 43 | loss: 0.32469 | val_0_auc: 0.88805 |  0:03:47s\n",
      "epoch 44 | loss: 0.32688 | val_0_auc: 0.88874 |  0:03:52s\n",
      "epoch 45 | loss: 0.32681 | val_0_auc: 0.89214 |  0:03:57s\n",
      "epoch 46 | loss: 0.32356 | val_0_auc: 0.89376 |  0:04:02s\n",
      "epoch 47 | loss: 0.32393 | val_0_auc: 0.89294 |  0:04:07s\n",
      "epoch 48 | loss: 0.32335 | val_0_auc: 0.89317 |  0:04:12s\n",
      "epoch 49 | loss: 0.32118 | val_0_auc: 0.89072 |  0:04:17s\n",
      "epoch 50 | loss: 0.32118 | val_0_auc: 0.89327 |  0:04:22s\n",
      "epoch 51 | loss: 0.32228 | val_0_auc: 0.88669 |  0:04:27s\n",
      "epoch 52 | loss: 0.33245 | val_0_auc: 0.87829 |  0:04:33s\n",
      "epoch 53 | loss: 0.35333 | val_0_auc: 0.85381 |  0:04:37s\n",
      "epoch 54 | loss: 0.35918 | val_0_auc: 0.85329 |  0:04:42s\n",
      "epoch 55 | loss: 0.35613 | val_0_auc: 0.86548 |  0:04:47s\n",
      "epoch 56 | loss: 0.34884 | val_0_auc: 0.86874 |  0:04:52s\n",
      "epoch 57 | loss: 0.34756 | val_0_auc: 0.86996 |  0:04:57s\n",
      "epoch 58 | loss: 0.3417  | val_0_auc: 0.87951 |  0:05:02s\n",
      "epoch 59 | loss: 0.3353  | val_0_auc: 0.87851 |  0:05:07s\n",
      "epoch 60 | loss: 0.33544 | val_0_auc: 0.85805 |  0:05:12s\n",
      "epoch 61 | loss: 0.33203 | val_0_auc: 0.85673 |  0:05:17s\n",
      "epoch 62 | loss: 0.32851 | val_0_auc: 0.85845 |  0:05:22s\n",
      "epoch 63 | loss: 0.32595 | val_0_auc: 0.86555 |  0:05:27s\n",
      "epoch 64 | loss: 0.33146 | val_0_auc: 0.8576  |  0:05:32s\n",
      "epoch 65 | loss: 0.33167 | val_0_auc: 0.88089 |  0:05:37s\n",
      "epoch 66 | loss: 0.33447 | val_0_auc: 0.87578 |  0:05:42s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_auc = 0.89376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.84839 | val_0_auc: 0.59564 |  0:00:01s\n",
      "epoch 1  | loss: 0.74281 | val_0_auc: 0.58714 |  0:00:03s\n",
      "epoch 2  | loss: 0.68125 | val_0_auc: 0.62616 |  0:00:05s\n",
      "epoch 3  | loss: 0.65336 | val_0_auc: 0.64649 |  0:00:07s\n",
      "epoch 4  | loss: 0.61834 | val_0_auc: 0.66381 |  0:00:09s\n",
      "epoch 5  | loss: 0.59837 | val_0_auc: 0.67989 |  0:00:11s\n",
      "epoch 6  | loss: 0.57863 | val_0_auc: 0.6892  |  0:00:13s\n",
      "epoch 7  | loss: 0.56983 | val_0_auc: 0.7025  |  0:00:15s\n",
      "epoch 8  | loss: 0.55197 | val_0_auc: 0.71508 |  0:00:16s\n",
      "epoch 9  | loss: 0.54073 | val_0_auc: 0.72034 |  0:00:18s\n",
      "epoch 10 | loss: 0.53528 | val_0_auc: 0.72152 |  0:00:20s\n",
      "epoch 11 | loss: 0.51933 | val_0_auc: 0.73417 |  0:00:22s\n",
      "epoch 12 | loss: 0.51512 | val_0_auc: 0.7372  |  0:00:24s\n",
      "epoch 13 | loss: 0.50552 | val_0_auc: 0.73895 |  0:00:26s\n",
      "epoch 14 | loss: 0.50508 | val_0_auc: 0.7526  |  0:00:28s\n",
      "epoch 15 | loss: 0.4989  | val_0_auc: 0.75051 |  0:00:30s\n",
      "epoch 16 | loss: 0.49651 | val_0_auc: 0.75546 |  0:00:32s\n",
      "epoch 17 | loss: 0.48778 | val_0_auc: 0.7619  |  0:00:33s\n",
      "epoch 18 | loss: 0.4839  | val_0_auc: 0.76126 |  0:00:35s\n",
      "epoch 19 | loss: 0.48388 | val_0_auc: 0.76405 |  0:00:37s\n",
      "epoch 20 | loss: 0.47816 | val_0_auc: 0.75949 |  0:00:39s\n",
      "epoch 21 | loss: 0.47072 | val_0_auc: 0.76937 |  0:00:41s\n",
      "epoch 22 | loss: 0.47325 | val_0_auc: 0.77016 |  0:00:43s\n",
      "epoch 23 | loss: 0.47012 | val_0_auc: 0.77002 |  0:00:45s\n",
      "epoch 24 | loss: 0.4692  | val_0_auc: 0.76557 |  0:00:47s\n",
      "epoch 25 | loss: 0.46856 | val_0_auc: 0.77425 |  0:00:48s\n",
      "epoch 26 | loss: 0.46528 | val_0_auc: 0.77297 |  0:00:50s\n",
      "epoch 27 | loss: 0.46297 | val_0_auc: 0.77476 |  0:00:52s\n",
      "epoch 28 | loss: 0.46233 | val_0_auc: 0.77898 |  0:00:54s\n",
      "epoch 29 | loss: 0.46254 | val_0_auc: 0.77399 |  0:00:56s\n",
      "epoch 30 | loss: 0.46353 | val_0_auc: 0.77634 |  0:00:58s\n",
      "epoch 31 | loss: 0.45857 | val_0_auc: 0.78266 |  0:01:01s\n",
      "epoch 32 | loss: 0.46231 | val_0_auc: 0.7793  |  0:01:03s\n",
      "epoch 33 | loss: 0.45544 | val_0_auc: 0.77908 |  0:01:05s\n",
      "epoch 34 | loss: 0.45508 | val_0_auc: 0.77878 |  0:01:07s\n",
      "epoch 35 | loss: 0.45722 | val_0_auc: 0.78275 |  0:01:09s\n",
      "epoch 36 | loss: 0.45536 | val_0_auc: 0.78335 |  0:01:11s\n",
      "epoch 37 | loss: 0.45278 | val_0_auc: 0.78376 |  0:01:13s\n",
      "epoch 38 | loss: 0.45373 | val_0_auc: 0.78042 |  0:01:14s\n",
      "epoch 39 | loss: 0.45263 | val_0_auc: 0.78356 |  0:01:16s\n",
      "epoch 40 | loss: 0.45117 | val_0_auc: 0.78712 |  0:01:18s\n",
      "epoch 41 | loss: 0.44992 | val_0_auc: 0.79039 |  0:01:20s\n",
      "epoch 42 | loss: 0.45035 | val_0_auc: 0.78728 |  0:01:22s\n",
      "epoch 43 | loss: 0.45294 | val_0_auc: 0.78791 |  0:01:24s\n",
      "epoch 44 | loss: 0.44935 | val_0_auc: 0.79188 |  0:01:26s\n",
      "epoch 45 | loss: 0.448   | val_0_auc: 0.79278 |  0:01:27s\n",
      "epoch 46 | loss: 0.44778 | val_0_auc: 0.7926  |  0:01:29s\n",
      "epoch 47 | loss: 0.44291 | val_0_auc: 0.79139 |  0:01:31s\n",
      "epoch 48 | loss: 0.44439 | val_0_auc: 0.79239 |  0:01:33s\n",
      "epoch 49 | loss: 0.4438  | val_0_auc: 0.79261 |  0:01:35s\n",
      "epoch 50 | loss: 0.4429  | val_0_auc: 0.79554 |  0:01:37s\n",
      "epoch 51 | loss: 0.44068 | val_0_auc: 0.79527 |  0:01:38s\n",
      "epoch 52 | loss: 0.4416  | val_0_auc: 0.79293 |  0:01:41s\n",
      "epoch 53 | loss: 0.44631 | val_0_auc: 0.79184 |  0:01:42s\n",
      "epoch 54 | loss: 0.44535 | val_0_auc: 0.79218 |  0:01:44s\n",
      "epoch 55 | loss: 0.44351 | val_0_auc: 0.7945  |  0:01:46s\n",
      "epoch 56 | loss: 0.44161 | val_0_auc: 0.79534 |  0:01:48s\n",
      "epoch 57 | loss: 0.44024 | val_0_auc: 0.79466 |  0:01:50s\n",
      "epoch 58 | loss: 0.44234 | val_0_auc: 0.79917 |  0:01:52s\n",
      "epoch 59 | loss: 0.44057 | val_0_auc: 0.79303 |  0:01:54s\n",
      "epoch 60 | loss: 0.44017 | val_0_auc: 0.7977  |  0:01:55s\n",
      "epoch 61 | loss: 0.44038 | val_0_auc: 0.79235 |  0:01:57s\n",
      "epoch 62 | loss: 0.43949 | val_0_auc: 0.79403 |  0:01:59s\n",
      "epoch 63 | loss: 0.43944 | val_0_auc: 0.79235 |  0:02:01s\n",
      "epoch 64 | loss: 0.44087 | val_0_auc: 0.79498 |  0:02:03s\n",
      "epoch 65 | loss: 0.43574 | val_0_auc: 0.79411 |  0:02:05s\n",
      "epoch 66 | loss: 0.43806 | val_0_auc: 0.79576 |  0:02:06s\n",
      "epoch 67 | loss: 0.43758 | val_0_auc: 0.7954  |  0:02:08s\n",
      "epoch 68 | loss: 0.43644 | val_0_auc: 0.79928 |  0:02:10s\n",
      "epoch 69 | loss: 0.4353  | val_0_auc: 0.80183 |  0:02:12s\n",
      "epoch 70 | loss: 0.43916 | val_0_auc: 0.79887 |  0:02:14s\n",
      "epoch 71 | loss: 0.43733 | val_0_auc: 0.80314 |  0:02:16s\n",
      "epoch 72 | loss: 0.43644 | val_0_auc: 0.8065  |  0:02:18s\n",
      "epoch 73 | loss: 0.43509 | val_0_auc: 0.80098 |  0:02:20s\n",
      "epoch 74 | loss: 0.43705 | val_0_auc: 0.80211 |  0:02:22s\n",
      "epoch 75 | loss: 0.43439 | val_0_auc: 0.79823 |  0:02:23s\n",
      "epoch 76 | loss: 0.43534 | val_0_auc: 0.80132 |  0:02:25s\n",
      "epoch 77 | loss: 0.43244 | val_0_auc: 0.80105 |  0:02:27s\n",
      "epoch 78 | loss: 0.43393 | val_0_auc: 0.80275 |  0:02:29s\n",
      "epoch 79 | loss: 0.43495 | val_0_auc: 0.80383 |  0:02:31s\n",
      "epoch 80 | loss: 0.43403 | val_0_auc: 0.80104 |  0:02:33s\n",
      "epoch 81 | loss: 0.43229 | val_0_auc: 0.80475 |  0:02:35s\n",
      "epoch 82 | loss: 0.4353  | val_0_auc: 0.80459 |  0:02:37s\n",
      "epoch 83 | loss: 0.43317 | val_0_auc: 0.80613 |  0:02:38s\n",
      "epoch 84 | loss: 0.43295 | val_0_auc: 0.80202 |  0:02:40s\n",
      "epoch 85 | loss: 0.43205 | val_0_auc: 0.80283 |  0:02:42s\n",
      "epoch 86 | loss: 0.43038 | val_0_auc: 0.8024  |  0:02:44s\n",
      "epoch 87 | loss: 0.43509 | val_0_auc: 0.80739 |  0:02:46s\n",
      "epoch 88 | loss: 0.43186 | val_0_auc: 0.80336 |  0:02:48s\n",
      "epoch 89 | loss: 0.43192 | val_0_auc: 0.80905 |  0:02:50s\n",
      "epoch 90 | loss: 0.43216 | val_0_auc: 0.80781 |  0:02:51s\n",
      "epoch 91 | loss: 0.42979 | val_0_auc: 0.80292 |  0:02:53s\n",
      "epoch 92 | loss: 0.43514 | val_0_auc: 0.80513 |  0:02:55s\n",
      "epoch 93 | loss: 0.4314  | val_0_auc: 0.80641 |  0:02:57s\n",
      "epoch 94 | loss: 0.43293 | val_0_auc: 0.80476 |  0:02:59s\n",
      "epoch 95 | loss: 0.43153 | val_0_auc: 0.80073 |  0:03:01s\n",
      "epoch 96 | loss: 0.42977 | val_0_auc: 0.80613 |  0:03:03s\n",
      "epoch 97 | loss: 0.43155 | val_0_auc: 0.80409 |  0:03:04s\n",
      "epoch 98 | loss: 0.43042 | val_0_auc: 0.80702 |  0:03:06s\n",
      "epoch 99 | loss: 0.4293  | val_0_auc: 0.80561 |  0:03:08s\n",
      "epoch 100| loss: 0.43145 | val_0_auc: 0.80632 |  0:03:10s\n",
      "epoch 101| loss: 0.42993 | val_0_auc: 0.80779 |  0:03:12s\n",
      "epoch 102| loss: 0.42943 | val_0_auc: 0.80751 |  0:03:14s\n",
      "epoch 103| loss: 0.43013 | val_0_auc: 0.81084 |  0:03:15s\n",
      "epoch 104| loss: 0.43026 | val_0_auc: 0.8042  |  0:03:17s\n",
      "epoch 105| loss: 0.4284  | val_0_auc: 0.80852 |  0:03:19s\n",
      "epoch 106| loss: 0.42755 | val_0_auc: 0.80361 |  0:03:21s\n",
      "epoch 107| loss: 0.42641 | val_0_auc: 0.80625 |  0:03:23s\n",
      "epoch 108| loss: 0.42794 | val_0_auc: 0.81005 |  0:03:25s\n",
      "epoch 109| loss: 0.43028 | val_0_auc: 0.80676 |  0:03:27s\n",
      "epoch 110| loss: 0.43069 | val_0_auc: 0.80574 |  0:03:28s\n",
      "epoch 111| loss: 0.4285  | val_0_auc: 0.80624 |  0:03:30s\n",
      "epoch 112| loss: 0.4283  | val_0_auc: 0.81021 |  0:03:32s\n",
      "epoch 113| loss: 0.42606 | val_0_auc: 0.80796 |  0:03:34s\n",
      "epoch 114| loss: 0.42939 | val_0_auc: 0.80487 |  0:03:36s\n",
      "epoch 115| loss: 0.42832 | val_0_auc: 0.80612 |  0:03:38s\n",
      "epoch 116| loss: 0.42891 | val_0_auc: 0.80112 |  0:03:40s\n",
      "epoch 117| loss: 0.42901 | val_0_auc: 0.79914 |  0:03:42s\n",
      "epoch 118| loss: 0.42885 | val_0_auc: 0.80199 |  0:03:43s\n",
      "epoch 119| loss: 0.42834 | val_0_auc: 0.80684 |  0:03:45s\n",
      "epoch 120| loss: 0.42729 | val_0_auc: 0.80252 |  0:03:47s\n",
      "epoch 121| loss: 0.4234  | val_0_auc: 0.80557 |  0:03:49s\n",
      "epoch 122| loss: 0.42705 | val_0_auc: 0.80817 |  0:03:51s\n",
      "epoch 123| loss: 0.42545 | val_0_auc: 0.80995 |  0:03:53s\n",
      "\n",
      "Early stopping occurred at epoch 123 with best_epoch = 103 and best_val_0_auc = 0.81084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68555 | val_0_auc: 0.59498 |  0:00:02s\n",
      "epoch 1  | loss: 0.56394 | val_0_auc: 0.67659 |  0:00:05s\n",
      "epoch 2  | loss: 0.53087 | val_0_auc: 0.72269 |  0:00:08s\n",
      "epoch 3  | loss: 0.50811 | val_0_auc: 0.76211 |  0:00:11s\n",
      "epoch 4  | loss: 0.4918  | val_0_auc: 0.78267 |  0:00:14s\n",
      "epoch 5  | loss: 0.48017 | val_0_auc: 0.78641 |  0:00:17s\n",
      "epoch 6  | loss: 0.4672  | val_0_auc: 0.79141 |  0:00:20s\n",
      "epoch 7  | loss: 0.45748 | val_0_auc: 0.78886 |  0:00:23s\n",
      "epoch 8  | loss: 0.45748 | val_0_auc: 0.80426 |  0:00:26s\n",
      "epoch 9  | loss: 0.44825 | val_0_auc: 0.81229 |  0:00:29s\n",
      "epoch 10 | loss: 0.44099 | val_0_auc: 0.82097 |  0:00:32s\n",
      "epoch 11 | loss: 0.42831 | val_0_auc: 0.83481 |  0:00:35s\n",
      "epoch 12 | loss: 0.42089 | val_0_auc: 0.82886 |  0:00:38s\n",
      "epoch 13 | loss: 0.4156  | val_0_auc: 0.83096 |  0:00:41s\n",
      "epoch 14 | loss: 0.42411 | val_0_auc: 0.83287 |  0:00:44s\n",
      "epoch 15 | loss: 0.41852 | val_0_auc: 0.83332 |  0:00:47s\n",
      "epoch 16 | loss: 0.40845 | val_0_auc: 0.83439 |  0:00:49s\n",
      "epoch 17 | loss: 0.40811 | val_0_auc: 0.83755 |  0:00:52s\n",
      "epoch 18 | loss: 0.40311 | val_0_auc: 0.84214 |  0:00:55s\n",
      "epoch 19 | loss: 0.39577 | val_0_auc: 0.85129 |  0:00:58s\n",
      "epoch 20 | loss: 0.38767 | val_0_auc: 0.85578 |  0:01:01s\n",
      "epoch 21 | loss: 0.38319 | val_0_auc: 0.85941 |  0:01:04s\n",
      "epoch 22 | loss: 0.37847 | val_0_auc: 0.86238 |  0:01:07s\n",
      "epoch 23 | loss: 0.37313 | val_0_auc: 0.85988 |  0:01:10s\n",
      "epoch 24 | loss: 0.37583 | val_0_auc: 0.85328 |  0:01:13s\n",
      "epoch 25 | loss: 0.38206 | val_0_auc: 0.85477 |  0:01:15s\n",
      "epoch 26 | loss: 0.37842 | val_0_auc: 0.85464 |  0:01:18s\n",
      "epoch 27 | loss: 0.37884 | val_0_auc: 0.85868 |  0:01:21s\n",
      "epoch 28 | loss: 0.37391 | val_0_auc: 0.8599  |  0:01:24s\n",
      "epoch 29 | loss: 0.37276 | val_0_auc: 0.86395 |  0:01:27s\n",
      "epoch 30 | loss: 0.37329 | val_0_auc: 0.86353 |  0:01:30s\n",
      "epoch 31 | loss: 0.36925 | val_0_auc: 0.86831 |  0:01:33s\n",
      "epoch 32 | loss: 0.36457 | val_0_auc: 0.86682 |  0:01:36s\n",
      "epoch 33 | loss: 0.36502 | val_0_auc: 0.86674 |  0:01:39s\n",
      "epoch 34 | loss: 0.35894 | val_0_auc: 0.87214 |  0:01:42s\n",
      "epoch 35 | loss: 0.35835 | val_0_auc: 0.8733  |  0:01:44s\n",
      "epoch 36 | loss: 0.35664 | val_0_auc: 0.87468 |  0:01:47s\n",
      "epoch 37 | loss: 0.35502 | val_0_auc: 0.87287 |  0:01:50s\n",
      "epoch 38 | loss: 0.35578 | val_0_auc: 0.86904 |  0:01:53s\n",
      "epoch 39 | loss: 0.35949 | val_0_auc: 0.87174 |  0:01:56s\n",
      "epoch 40 | loss: 0.35556 | val_0_auc: 0.87241 |  0:01:59s\n",
      "epoch 41 | loss: 0.35126 | val_0_auc: 0.87491 |  0:02:02s\n",
      "epoch 42 | loss: 0.34819 | val_0_auc: 0.87993 |  0:02:05s\n",
      "epoch 43 | loss: 0.34736 | val_0_auc: 0.88292 |  0:02:08s\n",
      "epoch 44 | loss: 0.33876 | val_0_auc: 0.88159 |  0:02:11s\n",
      "epoch 45 | loss: 0.33584 | val_0_auc: 0.88037 |  0:02:13s\n",
      "epoch 46 | loss: 0.33374 | val_0_auc: 0.88347 |  0:02:16s\n",
      "epoch 47 | loss: 0.33413 | val_0_auc: 0.88497 |  0:02:19s\n",
      "epoch 48 | loss: 0.33073 | val_0_auc: 0.88921 |  0:02:22s\n",
      "epoch 49 | loss: 0.32743 | val_0_auc: 0.88797 |  0:02:26s\n",
      "epoch 50 | loss: 0.32787 | val_0_auc: 0.88779 |  0:02:29s\n",
      "epoch 51 | loss: 0.32791 | val_0_auc: 0.88784 |  0:02:32s\n",
      "epoch 52 | loss: 0.32963 | val_0_auc: 0.88922 |  0:02:35s\n",
      "epoch 53 | loss: 0.32981 | val_0_auc: 0.8834  |  0:02:37s\n",
      "epoch 54 | loss: 0.32811 | val_0_auc: 0.88502 |  0:02:40s\n",
      "epoch 55 | loss: 0.32592 | val_0_auc: 0.89119 |  0:02:43s\n",
      "epoch 56 | loss: 0.32314 | val_0_auc: 0.89082 |  0:02:46s\n",
      "epoch 57 | loss: 0.3203  | val_0_auc: 0.89179 |  0:02:49s\n",
      "epoch 58 | loss: 0.31949 | val_0_auc: 0.89086 |  0:02:52s\n",
      "epoch 59 | loss: 0.31847 | val_0_auc: 0.89108 |  0:02:55s\n",
      "epoch 60 | loss: 0.31869 | val_0_auc: 0.89274 |  0:02:58s\n",
      "epoch 61 | loss: 0.32342 | val_0_auc: 0.88334 |  0:03:01s\n",
      "epoch 62 | loss: 0.32503 | val_0_auc: 0.87478 |  0:03:04s\n",
      "epoch 63 | loss: 0.32889 | val_0_auc: 0.87978 |  0:03:07s\n",
      "epoch 64 | loss: 0.32645 | val_0_auc: 0.88566 |  0:03:10s\n",
      "epoch 65 | loss: 0.32376 | val_0_auc: 0.88557 |  0:03:12s\n",
      "epoch 66 | loss: 0.3236  | val_0_auc: 0.88518 |  0:03:15s\n",
      "epoch 67 | loss: 0.32264 | val_0_auc: 0.8813  |  0:03:18s\n",
      "epoch 68 | loss: 0.3303  | val_0_auc: 0.88385 |  0:03:21s\n",
      "epoch 69 | loss: 0.32767 | val_0_auc: 0.88733 |  0:03:24s\n",
      "epoch 70 | loss: 0.32251 | val_0_auc: 0.89127 |  0:03:27s\n",
      "epoch 71 | loss: 0.32203 | val_0_auc: 0.89044 |  0:03:30s\n",
      "epoch 72 | loss: 0.32155 | val_0_auc: 0.89221 |  0:03:33s\n",
      "epoch 73 | loss: 0.32036 | val_0_auc: 0.89379 |  0:03:36s\n",
      "epoch 74 | loss: 0.31574 | val_0_auc: 0.89482 |  0:03:39s\n",
      "epoch 75 | loss: 0.31512 | val_0_auc: 0.89421 |  0:03:42s\n",
      "epoch 76 | loss: 0.31406 | val_0_auc: 0.89411 |  0:03:45s\n",
      "epoch 77 | loss: 0.31123 | val_0_auc: 0.89472 |  0:03:47s\n",
      "epoch 78 | loss: 0.31174 | val_0_auc: 0.89662 |  0:03:50s\n",
      "epoch 79 | loss: 0.31344 | val_0_auc: 0.89586 |  0:03:53s\n",
      "epoch 80 | loss: 0.31367 | val_0_auc: 0.89374 |  0:03:56s\n",
      "epoch 81 | loss: 0.31653 | val_0_auc: 0.89435 |  0:03:59s\n",
      "epoch 82 | loss: 0.31661 | val_0_auc: 0.89141 |  0:04:02s\n",
      "epoch 83 | loss: 0.33508 | val_0_auc: 0.87831 |  0:04:05s\n",
      "epoch 84 | loss: 0.346   | val_0_auc: 0.88263 |  0:04:08s\n",
      "epoch 85 | loss: 0.34009 | val_0_auc: 0.87822 |  0:04:11s\n",
      "epoch 86 | loss: 0.33043 | val_0_auc: 0.8822  |  0:04:14s\n",
      "epoch 87 | loss: 0.33442 | val_0_auc: 0.88748 |  0:04:17s\n",
      "epoch 88 | loss: 0.33128 | val_0_auc: 0.88643 |  0:04:19s\n",
      "epoch 89 | loss: 0.32673 | val_0_auc: 0.89511 |  0:04:22s\n",
      "epoch 90 | loss: 0.32667 | val_0_auc: 0.89496 |  0:04:25s\n",
      "epoch 91 | loss: 0.32183 | val_0_auc: 0.89375 |  0:04:28s\n",
      "epoch 92 | loss: 0.31991 | val_0_auc: 0.89389 |  0:04:31s\n",
      "epoch 93 | loss: 0.31372 | val_0_auc: 0.89775 |  0:04:34s\n",
      "epoch 94 | loss: 0.31343 | val_0_auc: 0.89972 |  0:04:37s\n",
      "epoch 95 | loss: 0.31457 | val_0_auc: 0.89899 |  0:04:40s\n",
      "epoch 96 | loss: 0.31512 | val_0_auc: 0.89511 |  0:04:43s\n",
      "epoch 97 | loss: 0.31861 | val_0_auc: 0.89905 |  0:04:46s\n",
      "epoch 98 | loss: 0.31555 | val_0_auc: 0.90014 |  0:04:49s\n",
      "epoch 99 | loss: 0.31336 | val_0_auc: 0.89742 |  0:04:52s\n",
      "epoch 100| loss: 0.31337 | val_0_auc: 0.8988  |  0:04:55s\n",
      "epoch 101| loss: 0.30795 | val_0_auc: 0.90063 |  0:04:58s\n",
      "epoch 102| loss: 0.30898 | val_0_auc: 0.89714 |  0:05:01s\n",
      "epoch 103| loss: 0.32214 | val_0_auc: 0.88221 |  0:05:03s\n",
      "epoch 104| loss: 0.32588 | val_0_auc: 0.8918  |  0:05:06s\n",
      "epoch 105| loss: 0.31938 | val_0_auc: 0.89084 |  0:05:09s\n",
      "epoch 106| loss: 0.31759 | val_0_auc: 0.89493 |  0:05:12s\n",
      "epoch 107| loss: 0.32479 | val_0_auc: 0.89577 |  0:05:15s\n",
      "epoch 108| loss: 0.31953 | val_0_auc: 0.8982  |  0:05:18s\n",
      "epoch 109| loss: 0.31325 | val_0_auc: 0.90202 |  0:05:21s\n",
      "epoch 110| loss: 0.30654 | val_0_auc: 0.90317 |  0:05:24s\n",
      "epoch 111| loss: 0.30897 | val_0_auc: 0.89952 |  0:05:27s\n",
      "epoch 112| loss: 0.30737 | val_0_auc: 0.89955 |  0:05:29s\n",
      "epoch 113| loss: 0.30967 | val_0_auc: 0.89996 |  0:05:32s\n",
      "epoch 114| loss: 0.31595 | val_0_auc: 0.89947 |  0:05:35s\n",
      "epoch 115| loss: 0.3153  | val_0_auc: 0.89815 |  0:05:38s\n",
      "epoch 116| loss: 0.31303 | val_0_auc: 0.90218 |  0:05:41s\n",
      "epoch 117| loss: 0.30804 | val_0_auc: 0.90278 |  0:05:44s\n",
      "epoch 118| loss: 0.30632 | val_0_auc: 0.90335 |  0:05:47s\n",
      "epoch 119| loss: 0.30453 | val_0_auc: 0.90273 |  0:05:50s\n",
      "epoch 120| loss: 0.30154 | val_0_auc: 0.90215 |  0:05:53s\n",
      "epoch 121| loss: 0.30114 | val_0_auc: 0.90083 |  0:05:56s\n",
      "epoch 122| loss: 0.30327 | val_0_auc: 0.89998 |  0:05:58s\n",
      "epoch 123| loss: 0.3043  | val_0_auc: 0.90124 |  0:06:01s\n",
      "epoch 124| loss: 0.30128 | val_0_auc: 0.90116 |  0:06:04s\n",
      "epoch 125| loss: 0.3009  | val_0_auc: 0.90223 |  0:06:07s\n",
      "epoch 126| loss: 0.30153 | val_0_auc: 0.9021  |  0:06:10s\n",
      "epoch 127| loss: 0.30322 | val_0_auc: 0.90191 |  0:06:13s\n",
      "epoch 128| loss: 0.30085 | val_0_auc: 0.90302 |  0:06:16s\n",
      "epoch 129| loss: 0.29973 | val_0_auc: 0.90365 |  0:06:19s\n",
      "epoch 130| loss: 0.29923 | val_0_auc: 0.899   |  0:06:22s\n",
      "epoch 131| loss: 0.30175 | val_0_auc: 0.89919 |  0:06:25s\n",
      "epoch 132| loss: 0.2989  | val_0_auc: 0.9004  |  0:06:28s\n",
      "epoch 133| loss: 0.30166 | val_0_auc: 0.90025 |  0:06:30s\n",
      "epoch 134| loss: 0.30129 | val_0_auc: 0.89887 |  0:06:33s\n",
      "epoch 135| loss: 0.3031  | val_0_auc: 0.90137 |  0:06:36s\n",
      "epoch 136| loss: 0.29917 | val_0_auc: 0.90296 |  0:06:39s\n",
      "epoch 137| loss: 0.30025 | val_0_auc: 0.90366 |  0:06:42s\n",
      "epoch 138| loss: 0.29542 | val_0_auc: 0.9036  |  0:06:45s\n",
      "epoch 139| loss: 0.29542 | val_0_auc: 0.90581 |  0:06:48s\n",
      "epoch 140| loss: 0.29818 | val_0_auc: 0.90215 |  0:06:51s\n",
      "epoch 141| loss: 0.3018  | val_0_auc: 0.90251 |  0:06:54s\n",
      "epoch 142| loss: 0.30156 | val_0_auc: 0.90636 |  0:06:57s\n",
      "epoch 143| loss: 0.29849 | val_0_auc: 0.90781 |  0:07:00s\n",
      "epoch 144| loss: 0.29223 | val_0_auc: 0.90917 |  0:07:03s\n",
      "epoch 145| loss: 0.29328 | val_0_auc: 0.90722 |  0:07:05s\n",
      "epoch 146| loss: 0.29446 | val_0_auc: 0.90731 |  0:07:08s\n",
      "epoch 147| loss: 0.29162 | val_0_auc: 0.90673 |  0:07:11s\n",
      "epoch 148| loss: 0.29182 | val_0_auc: 0.90735 |  0:07:14s\n",
      "epoch 149| loss: 0.29156 | val_0_auc: 0.90735 |  0:07:17s\n",
      "epoch 150| loss: 0.28987 | val_0_auc: 0.90808 |  0:07:20s\n",
      "epoch 151| loss: 0.29102 | val_0_auc: 0.91103 |  0:07:23s\n",
      "epoch 152| loss: 0.2896  | val_0_auc: 0.9107  |  0:07:26s\n",
      "epoch 153| loss: 0.2908  | val_0_auc: 0.9109  |  0:07:29s\n",
      "epoch 154| loss: 0.29286 | val_0_auc: 0.91052 |  0:07:32s\n",
      "epoch 155| loss: 0.28952 | val_0_auc: 0.91153 |  0:07:34s\n",
      "epoch 156| loss: 0.28846 | val_0_auc: 0.91225 |  0:07:37s\n",
      "epoch 157| loss: 0.28734 | val_0_auc: 0.90812 |  0:07:40s\n",
      "epoch 158| loss: 0.28812 | val_0_auc: 0.90557 |  0:07:43s\n",
      "epoch 159| loss: 0.28762 | val_0_auc: 0.90663 |  0:07:46s\n",
      "epoch 160| loss: 0.28911 | val_0_auc: 0.90602 |  0:07:49s\n",
      "epoch 161| loss: 0.28932 | val_0_auc: 0.90665 |  0:07:52s\n",
      "epoch 162| loss: 0.28916 | val_0_auc: 0.9047  |  0:07:56s\n",
      "epoch 163| loss: 0.28913 | val_0_auc: 0.90395 |  0:07:59s\n",
      "epoch 164| loss: 0.28658 | val_0_auc: 0.90606 |  0:08:02s\n",
      "epoch 165| loss: 0.28805 | val_0_auc: 0.90868 |  0:08:05s\n",
      "epoch 166| loss: 0.28646 | val_0_auc: 0.90909 |  0:08:08s\n",
      "epoch 167| loss: 0.28618 | val_0_auc: 0.90678 |  0:08:11s\n",
      "epoch 168| loss: 0.29011 | val_0_auc: 0.90555 |  0:08:14s\n",
      "epoch 169| loss: 0.28421 | val_0_auc: 0.90976 |  0:08:17s\n",
      "epoch 170| loss: 0.28625 | val_0_auc: 0.90782 |  0:08:20s\n",
      "epoch 171| loss: 0.28435 | val_0_auc: 0.90924 |  0:08:23s\n",
      "epoch 172| loss: 0.28262 | val_0_auc: 0.90975 |  0:08:26s\n",
      "epoch 173| loss: 0.28232 | val_0_auc: 0.91027 |  0:08:28s\n",
      "epoch 174| loss: 0.28455 | val_0_auc: 0.90707 |  0:08:31s\n",
      "epoch 175| loss: 0.28478 | val_0_auc: 0.90874 |  0:08:34s\n",
      "epoch 176| loss: 0.2847  | val_0_auc: 0.90848 |  0:08:37s\n",
      "\n",
      "Early stopping occurred at epoch 176 with best_epoch = 156 and best_val_0_auc = 0.91225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.80748 | val_0_auc: 0.56261 |  0:00:01s\n",
      "epoch 1  | loss: 1.4714  | val_0_auc: 0.55086 |  0:00:02s\n",
      "epoch 2  | loss: 1.1431  | val_0_auc: 0.61098 |  0:00:04s\n",
      "epoch 3  | loss: 0.89974 | val_0_auc: 0.61479 |  0:00:05s\n",
      "epoch 4  | loss: 0.75634 | val_0_auc: 0.65179 |  0:00:07s\n",
      "epoch 5  | loss: 0.66886 | val_0_auc: 0.6706  |  0:00:08s\n",
      "epoch 6  | loss: 0.6272  | val_0_auc: 0.68975 |  0:00:10s\n",
      "epoch 7  | loss: 0.59975 | val_0_auc: 0.68579 |  0:00:11s\n",
      "epoch 8  | loss: 0.57945 | val_0_auc: 0.69119 |  0:00:13s\n",
      "epoch 9  | loss: 0.56386 | val_0_auc: 0.7108  |  0:00:14s\n",
      "epoch 10 | loss: 0.56148 | val_0_auc: 0.70135 |  0:00:16s\n",
      "epoch 11 | loss: 0.54825 | val_0_auc: 0.71914 |  0:00:17s\n",
      "epoch 12 | loss: 0.5464  | val_0_auc: 0.71618 |  0:00:19s\n",
      "epoch 13 | loss: 0.53624 | val_0_auc: 0.73119 |  0:00:20s\n",
      "epoch 14 | loss: 0.53187 | val_0_auc: 0.72363 |  0:00:21s\n",
      "epoch 15 | loss: 0.52855 | val_0_auc: 0.72814 |  0:00:23s\n",
      "epoch 16 | loss: 0.52934 | val_0_auc: 0.73747 |  0:00:25s\n",
      "epoch 17 | loss: 0.52645 | val_0_auc: 0.74476 |  0:00:26s\n",
      "epoch 18 | loss: 0.52132 | val_0_auc: 0.743   |  0:00:28s\n",
      "epoch 19 | loss: 0.52257 | val_0_auc: 0.74588 |  0:00:29s\n",
      "epoch 20 | loss: 0.51557 | val_0_auc: 0.74096 |  0:00:31s\n",
      "epoch 21 | loss: 0.51585 | val_0_auc: 0.75644 |  0:00:32s\n",
      "epoch 22 | loss: 0.50963 | val_0_auc: 0.75431 |  0:00:34s\n",
      "epoch 23 | loss: 0.51004 | val_0_auc: 0.7631  |  0:00:35s\n",
      "epoch 24 | loss: 0.50867 | val_0_auc: 0.75946 |  0:00:36s\n",
      "epoch 25 | loss: 0.50613 | val_0_auc: 0.75031 |  0:00:38s\n",
      "epoch 26 | loss: 0.50708 | val_0_auc: 0.7636  |  0:00:39s\n",
      "epoch 27 | loss: 0.49925 | val_0_auc: 0.76356 |  0:00:41s\n",
      "epoch 28 | loss: 0.49685 | val_0_auc: 0.75727 |  0:00:42s\n",
      "epoch 29 | loss: 0.49545 | val_0_auc: 0.76267 |  0:00:44s\n",
      "epoch 30 | loss: 0.49401 | val_0_auc: 0.764   |  0:00:45s\n",
      "epoch 31 | loss: 0.49325 | val_0_auc: 0.76088 |  0:00:47s\n",
      "epoch 32 | loss: 0.48883 | val_0_auc: 0.75952 |  0:00:48s\n",
      "epoch 33 | loss: 0.48915 | val_0_auc: 0.77184 |  0:00:50s\n",
      "epoch 34 | loss: 0.49003 | val_0_auc: 0.76151 |  0:00:51s\n",
      "epoch 35 | loss: 0.48626 | val_0_auc: 0.7545  |  0:00:53s\n",
      "epoch 36 | loss: 0.487   | val_0_auc: 0.76614 |  0:00:54s\n",
      "epoch 37 | loss: 0.48341 | val_0_auc: 0.7604  |  0:00:56s\n",
      "epoch 38 | loss: 0.485   | val_0_auc: 0.76355 |  0:00:57s\n",
      "epoch 39 | loss: 0.48397 | val_0_auc: 0.75648 |  0:00:58s\n",
      "epoch 40 | loss: 0.47992 | val_0_auc: 0.76219 |  0:01:00s\n",
      "epoch 41 | loss: 0.48082 | val_0_auc: 0.76206 |  0:01:01s\n",
      "epoch 42 | loss: 0.48152 | val_0_auc: 0.77417 |  0:01:03s\n",
      "epoch 43 | loss: 0.48253 | val_0_auc: 0.76843 |  0:01:04s\n",
      "epoch 44 | loss: 0.48529 | val_0_auc: 0.77163 |  0:01:06s\n",
      "epoch 45 | loss: 0.48153 | val_0_auc: 0.75856 |  0:01:07s\n",
      "epoch 46 | loss: 0.48065 | val_0_auc: 0.7652  |  0:01:09s\n",
      "epoch 47 | loss: 0.48101 | val_0_auc: 0.77032 |  0:01:10s\n",
      "epoch 48 | loss: 0.47925 | val_0_auc: 0.77619 |  0:01:12s\n",
      "epoch 49 | loss: 0.48236 | val_0_auc: 0.76737 |  0:01:13s\n",
      "epoch 50 | loss: 0.48026 | val_0_auc: 0.76737 |  0:01:15s\n",
      "epoch 51 | loss: 0.47787 | val_0_auc: 0.76894 |  0:01:16s\n",
      "epoch 52 | loss: 0.47618 | val_0_auc: 0.76906 |  0:01:18s\n",
      "epoch 53 | loss: 0.48015 | val_0_auc: 0.76507 |  0:01:19s\n",
      "epoch 54 | loss: 0.47584 | val_0_auc: 0.77363 |  0:01:21s\n",
      "epoch 55 | loss: 0.48101 | val_0_auc: 0.77654 |  0:01:22s\n",
      "epoch 56 | loss: 0.47792 | val_0_auc: 0.77206 |  0:01:24s\n",
      "epoch 57 | loss: 0.4788  | val_0_auc: 0.77373 |  0:01:25s\n",
      "epoch 58 | loss: 0.4781  | val_0_auc: 0.77662 |  0:01:26s\n",
      "epoch 59 | loss: 0.47258 | val_0_auc: 0.77214 |  0:01:28s\n",
      "epoch 60 | loss: 0.47485 | val_0_auc: 0.77645 |  0:01:29s\n",
      "epoch 61 | loss: 0.47689 | val_0_auc: 0.77898 |  0:01:31s\n",
      "epoch 62 | loss: 0.47659 | val_0_auc: 0.77939 |  0:01:32s\n",
      "epoch 63 | loss: 0.47592 | val_0_auc: 0.78412 |  0:01:34s\n",
      "epoch 64 | loss: 0.47274 | val_0_auc: 0.78018 |  0:01:35s\n",
      "epoch 65 | loss: 0.4731  | val_0_auc: 0.78272 |  0:01:37s\n",
      "epoch 66 | loss: 0.47534 | val_0_auc: 0.7829  |  0:01:38s\n",
      "epoch 67 | loss: 0.47228 | val_0_auc: 0.78498 |  0:01:39s\n",
      "epoch 68 | loss: 0.47299 | val_0_auc: 0.77732 |  0:01:41s\n",
      "epoch 69 | loss: 0.47534 | val_0_auc: 0.78258 |  0:01:42s\n",
      "epoch 70 | loss: 0.47308 | val_0_auc: 0.78363 |  0:01:44s\n",
      "epoch 71 | loss: 0.47125 | val_0_auc: 0.77663 |  0:01:45s\n",
      "epoch 72 | loss: 0.47214 | val_0_auc: 0.78617 |  0:01:47s\n",
      "epoch 73 | loss: 0.47434 | val_0_auc: 0.77826 |  0:01:48s\n",
      "epoch 74 | loss: 0.471   | val_0_auc: 0.77946 |  0:01:50s\n",
      "epoch 75 | loss: 0.4735  | val_0_auc: 0.77954 |  0:01:51s\n",
      "epoch 76 | loss: 0.47164 | val_0_auc: 0.78314 |  0:01:53s\n",
      "epoch 77 | loss: 0.47044 | val_0_auc: 0.7766  |  0:01:54s\n",
      "epoch 78 | loss: 0.46979 | val_0_auc: 0.77841 |  0:01:55s\n",
      "epoch 79 | loss: 0.46926 | val_0_auc: 0.78759 |  0:01:57s\n",
      "epoch 80 | loss: 0.46842 | val_0_auc: 0.78533 |  0:01:58s\n",
      "epoch 81 | loss: 0.46747 | val_0_auc: 0.78324 |  0:02:00s\n",
      "epoch 82 | loss: 0.47021 | val_0_auc: 0.77546 |  0:02:01s\n",
      "epoch 83 | loss: 0.47055 | val_0_auc: 0.79069 |  0:02:03s\n",
      "epoch 84 | loss: 0.46736 | val_0_auc: 0.77788 |  0:02:04s\n",
      "epoch 85 | loss: 0.46735 | val_0_auc: 0.78313 |  0:02:06s\n",
      "epoch 86 | loss: 0.46614 | val_0_auc: 0.78812 |  0:02:07s\n",
      "epoch 87 | loss: 0.46779 | val_0_auc: 0.78575 |  0:02:09s\n",
      "epoch 88 | loss: 0.46921 | val_0_auc: 0.78101 |  0:02:10s\n",
      "epoch 89 | loss: 0.46409 | val_0_auc: 0.78302 |  0:02:12s\n",
      "epoch 90 | loss: 0.46595 | val_0_auc: 0.79192 |  0:02:13s\n",
      "epoch 91 | loss: 0.46627 | val_0_auc: 0.78514 |  0:02:14s\n",
      "epoch 92 | loss: 0.46734 | val_0_auc: 0.79345 |  0:02:16s\n",
      "epoch 93 | loss: 0.46823 | val_0_auc: 0.79173 |  0:02:17s\n",
      "epoch 94 | loss: 0.46812 | val_0_auc: 0.79203 |  0:02:19s\n",
      "epoch 95 | loss: 0.46511 | val_0_auc: 0.79833 |  0:02:20s\n",
      "epoch 96 | loss: 0.46534 | val_0_auc: 0.79399 |  0:02:22s\n",
      "epoch 97 | loss: 0.46592 | val_0_auc: 0.7979  |  0:02:23s\n",
      "epoch 98 | loss: 0.46672 | val_0_auc: 0.79926 |  0:02:25s\n",
      "epoch 99 | loss: 0.46687 | val_0_auc: 0.79428 |  0:02:26s\n",
      "epoch 100| loss: 0.46327 | val_0_auc: 0.79368 |  0:02:28s\n",
      "epoch 101| loss: 0.46385 | val_0_auc: 0.79682 |  0:02:29s\n",
      "epoch 102| loss: 0.46342 | val_0_auc: 0.79139 |  0:02:31s\n",
      "epoch 103| loss: 0.46426 | val_0_auc: 0.79219 |  0:02:32s\n",
      "epoch 104| loss: 0.46387 | val_0_auc: 0.7962  |  0:02:33s\n",
      "epoch 105| loss: 0.46387 | val_0_auc: 0.78926 |  0:02:35s\n",
      "epoch 106| loss: 0.45989 | val_0_auc: 0.78138 |  0:02:36s\n",
      "epoch 107| loss: 0.46036 | val_0_auc: 0.79499 |  0:02:38s\n",
      "epoch 108| loss: 0.46088 | val_0_auc: 0.79749 |  0:02:39s\n",
      "epoch 109| loss: 0.45896 | val_0_auc: 0.78561 |  0:02:41s\n",
      "epoch 110| loss: 0.459   | val_0_auc: 0.7931  |  0:02:42s\n",
      "epoch 111| loss: 0.46097 | val_0_auc: 0.79212 |  0:02:44s\n",
      "epoch 112| loss: 0.46276 | val_0_auc: 0.79337 |  0:02:45s\n",
      "epoch 113| loss: 0.46371 | val_0_auc: 0.79428 |  0:02:46s\n",
      "epoch 114| loss: 0.46039 | val_0_auc: 0.79505 |  0:02:48s\n",
      "epoch 115| loss: 0.45898 | val_0_auc: 0.79337 |  0:02:49s\n",
      "epoch 116| loss: 0.46033 | val_0_auc: 0.79078 |  0:02:51s\n",
      "epoch 117| loss: 0.45928 | val_0_auc: 0.78726 |  0:02:52s\n",
      "epoch 118| loss: 0.45973 | val_0_auc: 0.79056 |  0:02:54s\n",
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 98 and best_val_0_auc = 0.79926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08793 | val_0_auc: 0.51637 |  0:00:03s\n",
      "epoch 1  | loss: 0.94952 | val_0_auc: 0.53351 |  0:00:06s\n",
      "epoch 2  | loss: 0.86536 | val_0_auc: 0.55171 |  0:00:09s\n",
      "epoch 3  | loss: 0.7955  | val_0_auc: 0.56379 |  0:00:13s\n",
      "epoch 4  | loss: 0.73985 | val_0_auc: 0.57597 |  0:00:16s\n",
      "epoch 5  | loss: 0.70037 | val_0_auc: 0.6011  |  0:00:19s\n",
      "epoch 6  | loss: 0.6606  | val_0_auc: 0.63465 |  0:00:22s\n",
      "epoch 7  | loss: 0.61769 | val_0_auc: 0.64631 |  0:00:26s\n",
      "epoch 8  | loss: 0.60332 | val_0_auc: 0.65142 |  0:00:29s\n",
      "epoch 9  | loss: 0.57679 | val_0_auc: 0.67378 |  0:00:32s\n",
      "epoch 10 | loss: 0.55693 | val_0_auc: 0.70077 |  0:00:35s\n",
      "epoch 11 | loss: 0.55036 | val_0_auc: 0.70392 |  0:00:39s\n",
      "epoch 12 | loss: 0.52572 | val_0_auc: 0.72045 |  0:00:42s\n",
      "epoch 13 | loss: 0.50809 | val_0_auc: 0.72482 |  0:00:45s\n",
      "epoch 14 | loss: 0.50579 | val_0_auc: 0.73465 |  0:00:48s\n",
      "epoch 15 | loss: 0.49246 | val_0_auc: 0.74824 |  0:00:52s\n",
      "epoch 16 | loss: 0.48589 | val_0_auc: 0.76235 |  0:00:56s\n",
      "epoch 17 | loss: 0.47837 | val_0_auc: 0.77234 |  0:00:59s\n",
      "epoch 18 | loss: 0.47392 | val_0_auc: 0.77316 |  0:01:02s\n",
      "epoch 19 | loss: 0.46298 | val_0_auc: 0.7747  |  0:01:05s\n",
      "epoch 20 | loss: 0.46364 | val_0_auc: 0.78399 |  0:01:09s\n",
      "epoch 21 | loss: 0.4576  | val_0_auc: 0.78349 |  0:01:12s\n",
      "epoch 22 | loss: 0.45534 | val_0_auc: 0.79174 |  0:01:16s\n",
      "epoch 23 | loss: 0.44289 | val_0_auc: 0.793   |  0:01:19s\n",
      "epoch 24 | loss: 0.44269 | val_0_auc: 0.79691 |  0:01:22s\n",
      "epoch 25 | loss: 0.4366  | val_0_auc: 0.8012  |  0:01:25s\n",
      "epoch 26 | loss: 0.43605 | val_0_auc: 0.80124 |  0:01:28s\n",
      "epoch 27 | loss: 0.43432 | val_0_auc: 0.80127 |  0:01:32s\n",
      "epoch 28 | loss: 0.43283 | val_0_auc: 0.80909 |  0:01:35s\n",
      "epoch 29 | loss: 0.42693 | val_0_auc: 0.80821 |  0:01:38s\n",
      "epoch 30 | loss: 0.42821 | val_0_auc: 0.81146 |  0:01:42s\n",
      "epoch 31 | loss: 0.42048 | val_0_auc: 0.81411 |  0:01:45s\n",
      "epoch 32 | loss: 0.42181 | val_0_auc: 0.81522 |  0:01:48s\n",
      "epoch 33 | loss: 0.41559 | val_0_auc: 0.81748 |  0:01:51s\n",
      "epoch 34 | loss: 0.41694 | val_0_auc: 0.81818 |  0:01:55s\n",
      "epoch 35 | loss: 0.40998 | val_0_auc: 0.81658 |  0:01:58s\n",
      "epoch 36 | loss: 0.40964 | val_0_auc: 0.82259 |  0:02:01s\n",
      "epoch 37 | loss: 0.40844 | val_0_auc: 0.82529 |  0:02:04s\n",
      "epoch 38 | loss: 0.40923 | val_0_auc: 0.82427 |  0:02:07s\n",
      "epoch 39 | loss: 0.40785 | val_0_auc: 0.82907 |  0:02:11s\n",
      "epoch 40 | loss: 0.40342 | val_0_auc: 0.82511 |  0:02:14s\n",
      "epoch 41 | loss: 0.4064  | val_0_auc: 0.82548 |  0:02:17s\n",
      "epoch 42 | loss: 0.4034  | val_0_auc: 0.82937 |  0:02:21s\n",
      "epoch 43 | loss: 0.39986 | val_0_auc: 0.82961 |  0:02:24s\n",
      "epoch 44 | loss: 0.40179 | val_0_auc: 0.83187 |  0:02:27s\n",
      "epoch 45 | loss: 0.39673 | val_0_auc: 0.83007 |  0:02:30s\n",
      "epoch 46 | loss: 0.39758 | val_0_auc: 0.832   |  0:02:34s\n",
      "epoch 47 | loss: 0.39719 | val_0_auc: 0.83471 |  0:02:37s\n",
      "epoch 48 | loss: 0.39318 | val_0_auc: 0.83495 |  0:02:40s\n",
      "epoch 49 | loss: 0.3943  | val_0_auc: 0.83755 |  0:02:43s\n",
      "epoch 50 | loss: 0.39537 | val_0_auc: 0.83612 |  0:02:47s\n",
      "epoch 51 | loss: 0.39189 | val_0_auc: 0.83603 |  0:02:50s\n",
      "epoch 52 | loss: 0.38945 | val_0_auc: 0.84036 |  0:02:53s\n",
      "epoch 53 | loss: 0.38798 | val_0_auc: 0.83893 |  0:02:56s\n",
      "epoch 54 | loss: 0.38676 | val_0_auc: 0.84106 |  0:03:00s\n",
      "epoch 55 | loss: 0.3888  | val_0_auc: 0.84126 |  0:03:03s\n",
      "epoch 56 | loss: 0.39047 | val_0_auc: 0.84279 |  0:03:06s\n",
      "epoch 57 | loss: 0.38728 | val_0_auc: 0.84116 |  0:03:09s\n",
      "epoch 58 | loss: 0.3866  | val_0_auc: 0.84081 |  0:03:13s\n",
      "epoch 59 | loss: 0.3851  | val_0_auc: 0.84032 |  0:03:16s\n",
      "epoch 60 | loss: 0.38422 | val_0_auc: 0.84026 |  0:03:19s\n",
      "epoch 61 | loss: 0.38396 | val_0_auc: 0.84057 |  0:03:22s\n",
      "epoch 62 | loss: 0.3884  | val_0_auc: 0.8429  |  0:03:26s\n",
      "epoch 63 | loss: 0.38062 | val_0_auc: 0.84275 |  0:03:29s\n",
      "epoch 64 | loss: 0.38751 | val_0_auc: 0.84207 |  0:03:32s\n",
      "epoch 65 | loss: 0.38394 | val_0_auc: 0.83999 |  0:03:35s\n",
      "epoch 66 | loss: 0.3833  | val_0_auc: 0.84122 |  0:03:39s\n",
      "epoch 67 | loss: 0.38029 | val_0_auc: 0.84391 |  0:03:42s\n",
      "epoch 68 | loss: 0.38173 | val_0_auc: 0.84525 |  0:03:45s\n",
      "epoch 69 | loss: 0.38142 | val_0_auc: 0.84372 |  0:03:48s\n",
      "epoch 70 | loss: 0.3819  | val_0_auc: 0.84627 |  0:03:51s\n",
      "epoch 71 | loss: 0.38216 | val_0_auc: 0.84353 |  0:03:55s\n",
      "epoch 72 | loss: 0.37987 | val_0_auc: 0.84408 |  0:03:58s\n",
      "epoch 73 | loss: 0.377   | val_0_auc: 0.84523 |  0:04:01s\n",
      "epoch 74 | loss: 0.37781 | val_0_auc: 0.84622 |  0:04:04s\n",
      "epoch 75 | loss: 0.3812  | val_0_auc: 0.84643 |  0:04:07s\n",
      "epoch 76 | loss: 0.37548 | val_0_auc: 0.84673 |  0:04:10s\n",
      "epoch 77 | loss: 0.37749 | val_0_auc: 0.84573 |  0:04:14s\n",
      "epoch 78 | loss: 0.37746 | val_0_auc: 0.84693 |  0:04:17s\n",
      "epoch 79 | loss: 0.37606 | val_0_auc: 0.84803 |  0:04:20s\n",
      "epoch 80 | loss: 0.3748  | val_0_auc: 0.8471  |  0:04:23s\n",
      "epoch 81 | loss: 0.374   | val_0_auc: 0.84704 |  0:04:27s\n",
      "epoch 82 | loss: 0.37159 | val_0_auc: 0.8452  |  0:04:30s\n",
      "epoch 83 | loss: 0.37476 | val_0_auc: 0.8452  |  0:04:33s\n",
      "epoch 84 | loss: 0.37062 | val_0_auc: 0.84557 |  0:04:36s\n",
      "epoch 85 | loss: 0.37511 | val_0_auc: 0.84579 |  0:04:39s\n",
      "epoch 86 | loss: 0.37151 | val_0_auc: 0.84594 |  0:04:43s\n",
      "epoch 87 | loss: 0.37065 | val_0_auc: 0.84756 |  0:04:46s\n",
      "epoch 88 | loss: 0.36916 | val_0_auc: 0.84807 |  0:04:49s\n",
      "epoch 89 | loss: 0.37299 | val_0_auc: 0.84905 |  0:04:52s\n",
      "epoch 90 | loss: 0.36788 | val_0_auc: 0.85102 |  0:04:55s\n",
      "epoch 91 | loss: 0.36919 | val_0_auc: 0.85159 |  0:04:59s\n",
      "epoch 92 | loss: 0.36737 | val_0_auc: 0.85358 |  0:05:02s\n",
      "epoch 93 | loss: 0.36753 | val_0_auc: 0.85415 |  0:05:05s\n",
      "epoch 94 | loss: 0.36568 | val_0_auc: 0.85234 |  0:05:08s\n",
      "epoch 95 | loss: 0.36961 | val_0_auc: 0.85499 |  0:05:11s\n",
      "epoch 96 | loss: 0.36248 | val_0_auc: 0.85451 |  0:05:15s\n",
      "epoch 97 | loss: 0.36659 | val_0_auc: 0.85724 |  0:05:18s\n",
      "epoch 98 | loss: 0.36341 | val_0_auc: 0.85549 |  0:05:21s\n",
      "epoch 99 | loss: 0.36431 | val_0_auc: 0.85384 |  0:05:24s\n",
      "epoch 100| loss: 0.35806 | val_0_auc: 0.85364 |  0:05:28s\n",
      "epoch 101| loss: 0.35856 | val_0_auc: 0.85406 |  0:05:31s\n",
      "epoch 102| loss: 0.36345 | val_0_auc: 0.85483 |  0:05:34s\n",
      "epoch 103| loss: 0.36231 | val_0_auc: 0.85369 |  0:05:37s\n",
      "epoch 104| loss: 0.36166 | val_0_auc: 0.855   |  0:05:40s\n",
      "epoch 105| loss: 0.35802 | val_0_auc: 0.85624 |  0:05:43s\n",
      "epoch 106| loss: 0.35838 | val_0_auc: 0.85796 |  0:05:47s\n",
      "epoch 107| loss: 0.35812 | val_0_auc: 0.8576  |  0:05:50s\n",
      "epoch 108| loss: 0.35809 | val_0_auc: 0.85671 |  0:05:53s\n",
      "epoch 109| loss: 0.36168 | val_0_auc: 0.85481 |  0:05:57s\n",
      "epoch 110| loss: 0.3623  | val_0_auc: 0.85742 |  0:06:00s\n",
      "epoch 111| loss: 0.3593  | val_0_auc: 0.85813 |  0:06:03s\n",
      "epoch 112| loss: 0.35701 | val_0_auc: 0.85834 |  0:06:06s\n",
      "epoch 113| loss: 0.35688 | val_0_auc: 0.85851 |  0:06:09s\n",
      "epoch 114| loss: 0.35399 | val_0_auc: 0.85851 |  0:06:13s\n",
      "epoch 115| loss: 0.3543  | val_0_auc: 0.85929 |  0:06:16s\n",
      "epoch 116| loss: 0.35519 | val_0_auc: 0.86051 |  0:06:19s\n",
      "epoch 117| loss: 0.35687 | val_0_auc: 0.85987 |  0:06:22s\n",
      "epoch 118| loss: 0.35381 | val_0_auc: 0.86014 |  0:06:25s\n",
      "epoch 119| loss: 0.35447 | val_0_auc: 0.86055 |  0:06:29s\n",
      "epoch 120| loss: 0.35502 | val_0_auc: 0.85984 |  0:06:32s\n",
      "epoch 121| loss: 0.35395 | val_0_auc: 0.85968 |  0:06:35s\n",
      "epoch 122| loss: 0.35364 | val_0_auc: 0.86098 |  0:06:38s\n",
      "epoch 123| loss: 0.34927 | val_0_auc: 0.86217 |  0:06:41s\n",
      "epoch 124| loss: 0.35049 | val_0_auc: 0.86187 |  0:06:44s\n",
      "epoch 125| loss: 0.35099 | val_0_auc: 0.86233 |  0:06:48s\n",
      "epoch 126| loss: 0.3513  | val_0_auc: 0.86447 |  0:06:51s\n",
      "epoch 127| loss: 0.35171 | val_0_auc: 0.86333 |  0:06:54s\n",
      "epoch 128| loss: 0.34635 | val_0_auc: 0.86345 |  0:06:57s\n",
      "epoch 129| loss: 0.3467  | val_0_auc: 0.86348 |  0:07:00s\n",
      "epoch 130| loss: 0.34828 | val_0_auc: 0.86379 |  0:07:04s\n",
      "epoch 131| loss: 0.34861 | val_0_auc: 0.86207 |  0:07:07s\n",
      "epoch 132| loss: 0.34843 | val_0_auc: 0.86263 |  0:07:10s\n",
      "epoch 133| loss: 0.34719 | val_0_auc: 0.86175 |  0:07:13s\n",
      "epoch 134| loss: 0.34187 | val_0_auc: 0.86149 |  0:07:16s\n",
      "epoch 135| loss: 0.34575 | val_0_auc: 0.86163 |  0:07:20s\n",
      "epoch 136| loss: 0.34858 | val_0_auc: 0.86134 |  0:07:23s\n",
      "epoch 137| loss: 0.34429 | val_0_auc: 0.85975 |  0:07:26s\n",
      "epoch 138| loss: 0.34327 | val_0_auc: 0.85857 |  0:07:30s\n",
      "epoch 139| loss: 0.34531 | val_0_auc: 0.85789 |  0:07:33s\n",
      "epoch 140| loss: 0.34579 | val_0_auc: 0.85827 |  0:07:36s\n",
      "epoch 141| loss: 0.34549 | val_0_auc: 0.85885 |  0:07:39s\n",
      "epoch 142| loss: 0.34411 | val_0_auc: 0.85981 |  0:07:42s\n",
      "epoch 143| loss: 0.34271 | val_0_auc: 0.86023 |  0:07:46s\n",
      "epoch 144| loss: 0.34205 | val_0_auc: 0.86047 |  0:07:49s\n",
      "epoch 145| loss: 0.34087 | val_0_auc: 0.86203 |  0:07:52s\n",
      "epoch 146| loss: 0.34138 | val_0_auc: 0.86265 |  0:07:55s\n",
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 126 and best_val_0_auc = 0.86447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67609 | val_0_auc: 0.7131  |  0:00:03s\n",
      "epoch 1  | loss: 0.45017 | val_0_auc: 0.82046 |  0:00:06s\n",
      "epoch 2  | loss: 0.39367 | val_0_auc: 0.8554  |  0:00:09s\n",
      "epoch 3  | loss: 0.37106 | val_0_auc: 0.86783 |  0:00:11s\n",
      "epoch 4  | loss: 0.34958 | val_0_auc: 0.87896 |  0:00:14s\n",
      "epoch 5  | loss: 0.34212 | val_0_auc: 0.8843  |  0:00:17s\n",
      "epoch 6  | loss: 0.32586 | val_0_auc: 0.8867  |  0:00:21s\n",
      "epoch 7  | loss: 0.31789 | val_0_auc: 0.8947  |  0:00:24s\n",
      "epoch 8  | loss: 0.31004 | val_0_auc: 0.89598 |  0:00:27s\n",
      "epoch 9  | loss: 0.3035  | val_0_auc: 0.89856 |  0:00:30s\n",
      "epoch 10 | loss: 0.30005 | val_0_auc: 0.90195 |  0:00:32s\n",
      "epoch 11 | loss: 0.29509 | val_0_auc: 0.90322 |  0:00:35s\n",
      "epoch 12 | loss: 0.29169 | val_0_auc: 0.90737 |  0:00:38s\n",
      "epoch 13 | loss: 0.28693 | val_0_auc: 0.90678 |  0:00:41s\n",
      "epoch 14 | loss: 0.2834  | val_0_auc: 0.90886 |  0:00:44s\n",
      "epoch 15 | loss: 0.27976 | val_0_auc: 0.90754 |  0:00:47s\n",
      "epoch 16 | loss: 0.27826 | val_0_auc: 0.90749 |  0:00:50s\n",
      "epoch 17 | loss: 0.27506 | val_0_auc: 0.90811 |  0:00:53s\n",
      "epoch 18 | loss: 0.27291 | val_0_auc: 0.90885 |  0:00:56s\n",
      "epoch 19 | loss: 0.26938 | val_0_auc: 0.91093 |  0:00:59s\n",
      "epoch 20 | loss: 0.26568 | val_0_auc: 0.90832 |  0:01:02s\n",
      "epoch 21 | loss: 0.26044 | val_0_auc: 0.90859 |  0:01:05s\n",
      "epoch 22 | loss: 0.26245 | val_0_auc: 0.90699 |  0:01:08s\n",
      "epoch 23 | loss: 0.25695 | val_0_auc: 0.9122  |  0:01:11s\n",
      "epoch 24 | loss: 0.2533  | val_0_auc: 0.91141 |  0:01:14s\n",
      "epoch 25 | loss: 0.24964 | val_0_auc: 0.9114  |  0:01:17s\n",
      "epoch 26 | loss: 0.24909 | val_0_auc: 0.90938 |  0:01:20s\n",
      "epoch 27 | loss: 0.246   | val_0_auc: 0.90933 |  0:01:23s\n",
      "epoch 28 | loss: 0.24456 | val_0_auc: 0.90811 |  0:01:25s\n",
      "epoch 29 | loss: 0.2393  | val_0_auc: 0.90861 |  0:01:28s\n",
      "epoch 30 | loss: 0.238   | val_0_auc: 0.90813 |  0:01:31s\n",
      "epoch 31 | loss: 0.23353 | val_0_auc: 0.90777 |  0:01:34s\n",
      "epoch 32 | loss: 0.23184 | val_0_auc: 0.90934 |  0:01:37s\n",
      "epoch 33 | loss: 0.23064 | val_0_auc: 0.90472 |  0:01:40s\n",
      "epoch 34 | loss: 0.22816 | val_0_auc: 0.90743 |  0:01:43s\n",
      "epoch 35 | loss: 0.2225  | val_0_auc: 0.90661 |  0:01:46s\n",
      "epoch 36 | loss: 0.22091 | val_0_auc: 0.90479 |  0:01:49s\n",
      "epoch 37 | loss: 0.21993 | val_0_auc: 0.90441 |  0:01:52s\n",
      "epoch 38 | loss: 0.21825 | val_0_auc: 0.89864 |  0:01:55s\n",
      "epoch 39 | loss: 0.21563 | val_0_auc: 0.90575 |  0:01:58s\n",
      "epoch 40 | loss: 0.21174 | val_0_auc: 0.90367 |  0:02:01s\n",
      "epoch 41 | loss: 0.20476 | val_0_auc: 0.9002  |  0:02:04s\n",
      "epoch 42 | loss: 0.20755 | val_0_auc: 0.89945 |  0:02:07s\n",
      "epoch 43 | loss: 0.20589 | val_0_auc: 0.90525 |  0:02:10s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.55615 | val_0_auc: 0.68467 |  0:00:01s\n",
      "epoch 1  | loss: 0.434   | val_0_auc: 0.79024 |  0:00:02s\n",
      "epoch 2  | loss: 0.38043 | val_0_auc: 0.83728 |  0:00:04s\n",
      "epoch 3  | loss: 0.34259 | val_0_auc: 0.85507 |  0:00:05s\n",
      "epoch 4  | loss: 0.31936 | val_0_auc: 0.86514 |  0:00:07s\n",
      "epoch 5  | loss: 0.30458 | val_0_auc: 0.87793 |  0:00:08s\n",
      "epoch 6  | loss: 0.2976  | val_0_auc: 0.88569 |  0:00:10s\n",
      "epoch 7  | loss: 0.29201 | val_0_auc: 0.89146 |  0:00:11s\n",
      "epoch 8  | loss: 0.28038 | val_0_auc: 0.89547 |  0:00:13s\n",
      "epoch 9  | loss: 0.27648 | val_0_auc: 0.89771 |  0:00:14s\n",
      "epoch 10 | loss: 0.27039 | val_0_auc: 0.90345 |  0:00:16s\n",
      "epoch 11 | loss: 0.26289 | val_0_auc: 0.90611 |  0:00:18s\n",
      "epoch 12 | loss: 0.25731 | val_0_auc: 0.90163 |  0:00:19s\n",
      "epoch 13 | loss: 0.25514 | val_0_auc: 0.90612 |  0:00:20s\n",
      "epoch 14 | loss: 0.24994 | val_0_auc: 0.90382 |  0:00:22s\n",
      "epoch 15 | loss: 0.24363 | val_0_auc: 0.89896 |  0:00:24s\n",
      "epoch 16 | loss: 0.23931 | val_0_auc: 0.89489 |  0:00:25s\n",
      "epoch 17 | loss: 0.24127 | val_0_auc: 0.89694 |  0:00:27s\n",
      "epoch 18 | loss: 0.23783 | val_0_auc: 0.89201 |  0:00:28s\n",
      "epoch 19 | loss: 0.236   | val_0_auc: 0.89802 |  0:00:29s\n",
      "epoch 20 | loss: 0.23249 | val_0_auc: 0.89541 |  0:00:31s\n",
      "epoch 21 | loss: 0.22716 | val_0_auc: 0.89567 |  0:00:32s\n",
      "epoch 22 | loss: 0.22068 | val_0_auc: 0.89563 |  0:00:34s\n",
      "epoch 23 | loss: 0.21809 | val_0_auc: 0.89518 |  0:00:35s\n",
      "epoch 24 | loss: 0.2144  | val_0_auc: 0.89375 |  0:00:37s\n",
      "epoch 25 | loss: 0.21621 | val_0_auc: 0.89301 |  0:00:38s\n",
      "epoch 26 | loss: 0.20648 | val_0_auc: 0.89373 |  0:00:40s\n",
      "epoch 27 | loss: 0.21119 | val_0_auc: 0.88959 |  0:00:43s\n",
      "epoch 28 | loss: 0.20763 | val_0_auc: 0.89264 |  0:00:44s\n",
      "epoch 29 | loss: 0.20015 | val_0_auc: 0.88763 |  0:00:46s\n",
      "epoch 30 | loss: 0.20327 | val_0_auc: 0.88706 |  0:00:47s\n",
      "epoch 31 | loss: 0.20216 | val_0_auc: 0.88522 |  0:00:49s\n",
      "epoch 32 | loss: 0.19536 | val_0_auc: 0.88247 |  0:00:50s\n",
      "epoch 33 | loss: 0.19261 | val_0_auc: 0.88128 |  0:00:52s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.90612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.93042 | val_0_auc: 0.51422 |  0:00:03s\n",
      "epoch 1  | loss: 0.8371  | val_0_auc: 0.51466 |  0:00:06s\n",
      "epoch 2  | loss: 0.77186 | val_0_auc: 0.53329 |  0:00:09s\n",
      "epoch 3  | loss: 0.7181  | val_0_auc: 0.53895 |  0:00:12s\n",
      "epoch 4  | loss: 0.67593 | val_0_auc: 0.57323 |  0:00:16s\n",
      "epoch 5  | loss: 0.6453  | val_0_auc: 0.57903 |  0:00:19s\n",
      "epoch 6  | loss: 0.61357 | val_0_auc: 0.61463 |  0:00:22s\n",
      "epoch 7  | loss: 0.59036 | val_0_auc: 0.62678 |  0:00:25s\n",
      "epoch 8  | loss: 0.58159 | val_0_auc: 0.63334 |  0:00:28s\n",
      "epoch 9  | loss: 0.5736  | val_0_auc: 0.6582  |  0:00:32s\n",
      "epoch 10 | loss: 0.55956 | val_0_auc: 0.69175 |  0:00:35s\n",
      "epoch 11 | loss: 0.54628 | val_0_auc: 0.68683 |  0:00:38s\n",
      "epoch 12 | loss: 0.54063 | val_0_auc: 0.70048 |  0:00:41s\n",
      "epoch 13 | loss: 0.53649 | val_0_auc: 0.69101 |  0:00:44s\n",
      "epoch 14 | loss: 0.53087 | val_0_auc: 0.70831 |  0:00:47s\n",
      "epoch 15 | loss: 0.52675 | val_0_auc: 0.70261 |  0:00:50s\n",
      "epoch 16 | loss: 0.52375 | val_0_auc: 0.71206 |  0:00:54s\n",
      "epoch 17 | loss: 0.51869 | val_0_auc: 0.71994 |  0:00:57s\n",
      "epoch 18 | loss: 0.51417 | val_0_auc: 0.72207 |  0:01:00s\n",
      "epoch 19 | loss: 0.51224 | val_0_auc: 0.72886 |  0:01:03s\n",
      "epoch 20 | loss: 0.50878 | val_0_auc: 0.73082 |  0:01:06s\n",
      "epoch 21 | loss: 0.50319 | val_0_auc: 0.72249 |  0:01:10s\n",
      "epoch 22 | loss: 0.50275 | val_0_auc: 0.73785 |  0:01:13s\n",
      "epoch 23 | loss: 0.49737 | val_0_auc: 0.7421  |  0:01:16s\n",
      "epoch 24 | loss: 0.49776 | val_0_auc: 0.73304 |  0:01:19s\n",
      "epoch 25 | loss: 0.49678 | val_0_auc: 0.73566 |  0:01:23s\n",
      "epoch 26 | loss: 0.4917  | val_0_auc: 0.74586 |  0:01:26s\n",
      "epoch 27 | loss: 0.49151 | val_0_auc: 0.73973 |  0:01:29s\n",
      "epoch 28 | loss: 0.49245 | val_0_auc: 0.74094 |  0:01:32s\n",
      "epoch 29 | loss: 0.49002 | val_0_auc: 0.75406 |  0:01:35s\n",
      "epoch 30 | loss: 0.48998 | val_0_auc: 0.75212 |  0:01:38s\n",
      "epoch 31 | loss: 0.48738 | val_0_auc: 0.74278 |  0:01:42s\n",
      "epoch 32 | loss: 0.48754 | val_0_auc: 0.75438 |  0:01:45s\n",
      "epoch 33 | loss: 0.48382 | val_0_auc: 0.74962 |  0:01:48s\n",
      "epoch 34 | loss: 0.48506 | val_0_auc: 0.75948 |  0:01:51s\n",
      "epoch 35 | loss: 0.48549 | val_0_auc: 0.75696 |  0:01:54s\n",
      "epoch 36 | loss: 0.48338 | val_0_auc: 0.76143 |  0:01:58s\n",
      "epoch 37 | loss: 0.4795  | val_0_auc: 0.75685 |  0:02:01s\n",
      "epoch 38 | loss: 0.47549 | val_0_auc: 0.76468 |  0:02:05s\n",
      "epoch 39 | loss: 0.48123 | val_0_auc: 0.76091 |  0:02:08s\n",
      "epoch 40 | loss: 0.48066 | val_0_auc: 0.76104 |  0:02:11s\n",
      "epoch 41 | loss: 0.47788 | val_0_auc: 0.76315 |  0:02:14s\n",
      "epoch 42 | loss: 0.47751 | val_0_auc: 0.76319 |  0:02:18s\n",
      "epoch 43 | loss: 0.47737 | val_0_auc: 0.76325 |  0:02:21s\n",
      "epoch 44 | loss: 0.47229 | val_0_auc: 0.75821 |  0:02:24s\n",
      "epoch 45 | loss: 0.47698 | val_0_auc: 0.76656 |  0:02:27s\n",
      "epoch 46 | loss: 0.47435 | val_0_auc: 0.75789 |  0:02:31s\n",
      "epoch 47 | loss: 0.47116 | val_0_auc: 0.76306 |  0:02:34s\n",
      "epoch 48 | loss: 0.47379 | val_0_auc: 0.76619 |  0:02:37s\n",
      "epoch 49 | loss: 0.4721  | val_0_auc: 0.76419 |  0:02:41s\n",
      "epoch 50 | loss: 0.46836 | val_0_auc: 0.7672  |  0:02:44s\n",
      "epoch 51 | loss: 0.46665 | val_0_auc: 0.7723  |  0:02:47s\n",
      "epoch 52 | loss: 0.47047 | val_0_auc: 0.77366 |  0:02:50s\n",
      "epoch 53 | loss: 0.46419 | val_0_auc: 0.77037 |  0:02:53s\n",
      "epoch 54 | loss: 0.46943 | val_0_auc: 0.7702  |  0:02:56s\n",
      "epoch 55 | loss: 0.46708 | val_0_auc: 0.77491 |  0:03:00s\n",
      "epoch 56 | loss: 0.46631 | val_0_auc: 0.78259 |  0:03:03s\n",
      "epoch 57 | loss: 0.46378 | val_0_auc: 0.77811 |  0:03:06s\n",
      "epoch 58 | loss: 0.4639  | val_0_auc: 0.77581 |  0:03:09s\n",
      "epoch 59 | loss: 0.46255 | val_0_auc: 0.77929 |  0:03:12s\n",
      "epoch 60 | loss: 0.46537 | val_0_auc: 0.77289 |  0:03:15s\n",
      "epoch 61 | loss: 0.46488 | val_0_auc: 0.78458 |  0:03:19s\n",
      "epoch 62 | loss: 0.46262 | val_0_auc: 0.78103 |  0:03:22s\n",
      "epoch 63 | loss: 0.46478 | val_0_auc: 0.78089 |  0:03:25s\n",
      "epoch 64 | loss: 0.46346 | val_0_auc: 0.77222 |  0:03:28s\n",
      "epoch 65 | loss: 0.461   | val_0_auc: 0.78162 |  0:03:31s\n",
      "epoch 66 | loss: 0.46221 | val_0_auc: 0.77422 |  0:03:34s\n",
      "epoch 67 | loss: 0.46048 | val_0_auc: 0.78323 |  0:03:37s\n",
      "epoch 68 | loss: 0.45958 | val_0_auc: 0.78686 |  0:03:40s\n",
      "epoch 69 | loss: 0.46271 | val_0_auc: 0.78272 |  0:03:44s\n",
      "epoch 70 | loss: 0.46151 | val_0_auc: 0.78651 |  0:03:47s\n",
      "epoch 71 | loss: 0.46141 | val_0_auc: 0.78411 |  0:03:50s\n",
      "epoch 72 | loss: 0.45948 | val_0_auc: 0.78668 |  0:03:53s\n",
      "epoch 73 | loss: 0.45897 | val_0_auc: 0.77765 |  0:03:56s\n",
      "epoch 74 | loss: 0.46237 | val_0_auc: 0.78563 |  0:03:59s\n",
      "epoch 75 | loss: 0.45722 | val_0_auc: 0.78498 |  0:04:02s\n",
      "epoch 76 | loss: 0.45547 | val_0_auc: 0.78653 |  0:04:06s\n",
      "epoch 77 | loss: 0.45941 | val_0_auc: 0.78857 |  0:04:09s\n",
      "epoch 78 | loss: 0.45832 | val_0_auc: 0.78627 |  0:04:12s\n",
      "epoch 79 | loss: 0.4578  | val_0_auc: 0.78496 |  0:04:15s\n",
      "epoch 80 | loss: 0.45892 | val_0_auc: 0.78887 |  0:04:18s\n",
      "epoch 81 | loss: 0.45745 | val_0_auc: 0.78493 |  0:04:21s\n",
      "epoch 82 | loss: 0.45777 | val_0_auc: 0.78526 |  0:04:25s\n",
      "epoch 83 | loss: 0.45905 | val_0_auc: 0.78441 |  0:04:28s\n",
      "epoch 84 | loss: 0.45932 | val_0_auc: 0.78898 |  0:04:31s\n",
      "epoch 85 | loss: 0.45673 | val_0_auc: 0.77847 |  0:04:34s\n",
      "epoch 86 | loss: 0.45815 | val_0_auc: 0.78503 |  0:04:37s\n",
      "epoch 87 | loss: 0.45568 | val_0_auc: 0.7815  |  0:04:40s\n",
      "epoch 88 | loss: 0.45506 | val_0_auc: 0.78836 |  0:04:43s\n",
      "epoch 89 | loss: 0.45642 | val_0_auc: 0.78865 |  0:04:47s\n",
      "epoch 90 | loss: 0.45498 | val_0_auc: 0.78619 |  0:04:50s\n",
      "epoch 91 | loss: 0.45672 | val_0_auc: 0.78275 |  0:04:53s\n",
      "epoch 92 | loss: 0.45796 | val_0_auc: 0.78215 |  0:04:56s\n",
      "epoch 93 | loss: 0.45579 | val_0_auc: 0.78304 |  0:04:59s\n",
      "epoch 94 | loss: 0.45666 | val_0_auc: 0.78383 |  0:05:02s\n",
      "epoch 95 | loss: 0.455   | val_0_auc: 0.7844  |  0:05:05s\n",
      "epoch 96 | loss: 0.45477 | val_0_auc: 0.79025 |  0:05:08s\n",
      "epoch 97 | loss: 0.45391 | val_0_auc: 0.7826  |  0:05:12s\n",
      "epoch 98 | loss: 0.4543  | val_0_auc: 0.78069 |  0:05:15s\n",
      "epoch 99 | loss: 0.45358 | val_0_auc: 0.78044 |  0:05:18s\n",
      "epoch 100| loss: 0.45257 | val_0_auc: 0.7877  |  0:05:21s\n",
      "epoch 101| loss: 0.45426 | val_0_auc: 0.78461 |  0:05:24s\n",
      "epoch 102| loss: 0.45357 | val_0_auc: 0.78483 |  0:05:28s\n",
      "epoch 103| loss: 0.45343 | val_0_auc: 0.78509 |  0:05:31s\n",
      "epoch 104| loss: 0.45421 | val_0_auc: 0.78408 |  0:05:34s\n",
      "epoch 105| loss: 0.4555  | val_0_auc: 0.78467 |  0:05:37s\n",
      "epoch 106| loss: 0.45137 | val_0_auc: 0.77885 |  0:05:40s\n",
      "epoch 107| loss: 0.45357 | val_0_auc: 0.78531 |  0:05:43s\n",
      "epoch 108| loss: 0.45495 | val_0_auc: 0.77657 |  0:05:46s\n",
      "epoch 109| loss: 0.45495 | val_0_auc: 0.78467 |  0:05:49s\n",
      "epoch 110| loss: 0.45253 | val_0_auc: 0.78321 |  0:05:52s\n",
      "epoch 111| loss: 0.45165 | val_0_auc: 0.78111 |  0:05:56s\n",
      "epoch 112| loss: 0.45494 | val_0_auc: 0.78524 |  0:05:59s\n",
      "epoch 113| loss: 0.45348 | val_0_auc: 0.7812  |  0:06:02s\n",
      "epoch 114| loss: 0.45148 | val_0_auc: 0.78581 |  0:06:05s\n",
      "epoch 115| loss: 0.4506  | val_0_auc: 0.79106 |  0:06:08s\n",
      "epoch 116| loss: 0.45047 | val_0_auc: 0.78095 |  0:06:12s\n",
      "epoch 117| loss: 0.45    | val_0_auc: 0.78735 |  0:06:15s\n",
      "epoch 118| loss: 0.45184 | val_0_auc: 0.79009 |  0:06:18s\n",
      "epoch 119| loss: 0.45173 | val_0_auc: 0.78149 |  0:06:21s\n",
      "epoch 120| loss: 0.4513  | val_0_auc: 0.78303 |  0:06:24s\n",
      "epoch 121| loss: 0.45047 | val_0_auc: 0.78841 |  0:06:27s\n",
      "epoch 122| loss: 0.45131 | val_0_auc: 0.78665 |  0:06:31s\n",
      "epoch 123| loss: 0.44906 | val_0_auc: 0.79109 |  0:06:34s\n",
      "epoch 124| loss: 0.44866 | val_0_auc: 0.78398 |  0:06:37s\n",
      "epoch 125| loss: 0.45205 | val_0_auc: 0.78417 |  0:06:40s\n",
      "epoch 126| loss: 0.45542 | val_0_auc: 0.78305 |  0:06:43s\n",
      "epoch 127| loss: 0.45261 | val_0_auc: 0.7894  |  0:06:46s\n",
      "epoch 128| loss: 0.45006 | val_0_auc: 0.78625 |  0:06:49s\n",
      "epoch 129| loss: 0.44952 | val_0_auc: 0.78299 |  0:06:52s\n",
      "epoch 130| loss: 0.44867 | val_0_auc: 0.78659 |  0:06:56s\n",
      "epoch 131| loss: 0.44981 | val_0_auc: 0.78658 |  0:06:59s\n",
      "epoch 132| loss: 0.44762 | val_0_auc: 0.78755 |  0:07:02s\n",
      "epoch 133| loss: 0.44799 | val_0_auc: 0.79201 |  0:07:05s\n",
      "epoch 134| loss: 0.45124 | val_0_auc: 0.78747 |  0:07:08s\n",
      "epoch 135| loss: 0.45092 | val_0_auc: 0.78746 |  0:07:11s\n",
      "epoch 136| loss: 0.44681 | val_0_auc: 0.79222 |  0:07:14s\n",
      "epoch 137| loss: 0.44866 | val_0_auc: 0.79176 |  0:07:18s\n",
      "epoch 138| loss: 0.4464  | val_0_auc: 0.7925  |  0:07:21s\n",
      "epoch 139| loss: 0.44742 | val_0_auc: 0.79933 |  0:07:24s\n",
      "epoch 140| loss: 0.44579 | val_0_auc: 0.79355 |  0:07:27s\n",
      "epoch 141| loss: 0.44625 | val_0_auc: 0.78999 |  0:07:31s\n",
      "epoch 142| loss: 0.44904 | val_0_auc: 0.79697 |  0:07:34s\n",
      "epoch 143| loss: 0.44619 | val_0_auc: 0.79707 |  0:07:37s\n",
      "epoch 144| loss: 0.4476  | val_0_auc: 0.79278 |  0:07:40s\n",
      "epoch 145| loss: 0.44405 | val_0_auc: 0.79515 |  0:07:43s\n",
      "epoch 146| loss: 0.44673 | val_0_auc: 0.79688 |  0:07:46s\n",
      "epoch 147| loss: 0.44708 | val_0_auc: 0.79342 |  0:07:49s\n",
      "epoch 148| loss: 0.44675 | val_0_auc: 0.7981  |  0:07:53s\n",
      "epoch 149| loss: 0.44485 | val_0_auc: 0.79877 |  0:07:56s\n",
      "epoch 150| loss: 0.44764 | val_0_auc: 0.79883 |  0:07:59s\n",
      "epoch 151| loss: 0.44432 | val_0_auc: 0.79462 |  0:08:02s\n",
      "epoch 152| loss: 0.44436 | val_0_auc: 0.79731 |  0:08:05s\n",
      "epoch 153| loss: 0.44647 | val_0_auc: 0.79582 |  0:08:08s\n",
      "epoch 154| loss: 0.44455 | val_0_auc: 0.79874 |  0:08:11s\n",
      "epoch 155| loss: 0.44272 | val_0_auc: 0.79871 |  0:08:15s\n",
      "epoch 156| loss: 0.44564 | val_0_auc: 0.79443 |  0:08:18s\n",
      "epoch 157| loss: 0.44357 | val_0_auc: 0.79716 |  0:08:21s\n",
      "epoch 158| loss: 0.44599 | val_0_auc: 0.79727 |  0:08:24s\n",
      "epoch 159| loss: 0.44541 | val_0_auc: 0.79499 |  0:08:27s\n",
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 139 and best_val_0_auc = 0.79933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.10255 | val_0_auc: 0.49132 |  0:00:02s\n",
      "epoch 1  | loss: 1.65561 | val_0_auc: 0.51077 |  0:00:05s\n",
      "epoch 2  | loss: 1.26085 | val_0_auc: 0.53201 |  0:00:07s\n",
      "epoch 3  | loss: 0.9951  | val_0_auc: 0.5548  |  0:00:10s\n",
      "epoch 4  | loss: 0.83255 | val_0_auc: 0.57895 |  0:00:13s\n",
      "epoch 5  | loss: 0.71947 | val_0_auc: 0.60455 |  0:00:16s\n",
      "epoch 6  | loss: 0.66295 | val_0_auc: 0.6298  |  0:00:18s\n",
      "epoch 7  | loss: 0.62325 | val_0_auc: 0.66125 |  0:00:21s\n",
      "epoch 8  | loss: 0.59086 | val_0_auc: 0.68774 |  0:00:24s\n",
      "epoch 9  | loss: 0.57173 | val_0_auc: 0.71215 |  0:00:26s\n",
      "epoch 10 | loss: 0.55647 | val_0_auc: 0.73556 |  0:00:29s\n",
      "epoch 11 | loss: 0.53912 | val_0_auc: 0.75313 |  0:00:32s\n",
      "epoch 12 | loss: 0.5165  | val_0_auc: 0.77209 |  0:00:34s\n",
      "epoch 13 | loss: 0.50941 | val_0_auc: 0.79157 |  0:00:37s\n",
      "epoch 14 | loss: 0.48611 | val_0_auc: 0.80184 |  0:00:40s\n",
      "epoch 15 | loss: 0.47675 | val_0_auc: 0.81629 |  0:00:42s\n",
      "epoch 16 | loss: 0.46982 | val_0_auc: 0.82488 |  0:00:45s\n",
      "epoch 17 | loss: 0.45885 | val_0_auc: 0.82457 |  0:00:48s\n",
      "epoch 18 | loss: 0.4447  | val_0_auc: 0.83264 |  0:00:50s\n",
      "epoch 19 | loss: 0.43474 | val_0_auc: 0.83664 |  0:00:53s\n",
      "epoch 20 | loss: 0.43018 | val_0_auc: 0.84151 |  0:00:56s\n",
      "epoch 21 | loss: 0.42807 | val_0_auc: 0.84159 |  0:00:58s\n",
      "epoch 22 | loss: 0.41984 | val_0_auc: 0.84584 |  0:01:01s\n",
      "epoch 23 | loss: 0.41526 | val_0_auc: 0.84712 |  0:01:04s\n",
      "epoch 24 | loss: 0.41075 | val_0_auc: 0.84423 |  0:01:06s\n",
      "epoch 25 | loss: 0.40427 | val_0_auc: 0.84651 |  0:01:09s\n",
      "epoch 26 | loss: 0.39976 | val_0_auc: 0.85063 |  0:01:12s\n",
      "epoch 27 | loss: 0.39662 | val_0_auc: 0.85272 |  0:01:14s\n",
      "epoch 28 | loss: 0.39522 | val_0_auc: 0.85723 |  0:01:18s\n",
      "epoch 29 | loss: 0.38929 | val_0_auc: 0.85943 |  0:01:21s\n",
      "epoch 30 | loss: 0.39274 | val_0_auc: 0.85869 |  0:01:23s\n",
      "epoch 31 | loss: 0.38607 | val_0_auc: 0.86186 |  0:01:26s\n",
      "epoch 32 | loss: 0.38396 | val_0_auc: 0.86343 |  0:01:29s\n",
      "epoch 33 | loss: 0.37729 | val_0_auc: 0.86419 |  0:01:32s\n",
      "epoch 34 | loss: 0.37442 | val_0_auc: 0.86436 |  0:01:34s\n",
      "epoch 35 | loss: 0.37715 | val_0_auc: 0.86462 |  0:01:37s\n",
      "epoch 36 | loss: 0.37123 | val_0_auc: 0.86844 |  0:01:40s\n",
      "epoch 37 | loss: 0.37018 | val_0_auc: 0.86675 |  0:01:43s\n",
      "epoch 38 | loss: 0.37157 | val_0_auc: 0.86908 |  0:01:46s\n",
      "epoch 39 | loss: 0.3694  | val_0_auc: 0.87104 |  0:01:48s\n",
      "epoch 40 | loss: 0.36508 | val_0_auc: 0.87024 |  0:01:51s\n",
      "epoch 41 | loss: 0.36197 | val_0_auc: 0.86956 |  0:01:54s\n",
      "epoch 42 | loss: 0.36258 | val_0_auc: 0.87191 |  0:01:56s\n",
      "epoch 43 | loss: 0.36294 | val_0_auc: 0.87163 |  0:01:59s\n",
      "epoch 44 | loss: 0.36194 | val_0_auc: 0.87185 |  0:02:02s\n",
      "epoch 45 | loss: 0.35859 | val_0_auc: 0.87327 |  0:02:04s\n",
      "epoch 46 | loss: 0.35509 | val_0_auc: 0.87337 |  0:02:07s\n",
      "epoch 47 | loss: 0.35034 | val_0_auc: 0.87269 |  0:02:10s\n",
      "epoch 48 | loss: 0.35204 | val_0_auc: 0.87406 |  0:02:12s\n",
      "epoch 49 | loss: 0.35324 | val_0_auc: 0.87249 |  0:02:15s\n",
      "epoch 50 | loss: 0.35087 | val_0_auc: 0.87381 |  0:02:18s\n",
      "epoch 51 | loss: 0.34767 | val_0_auc: 0.87561 |  0:02:21s\n",
      "epoch 52 | loss: 0.34824 | val_0_auc: 0.87568 |  0:02:23s\n",
      "epoch 53 | loss: 0.33887 | val_0_auc: 0.87499 |  0:02:26s\n",
      "epoch 54 | loss: 0.34455 | val_0_auc: 0.87721 |  0:02:29s\n",
      "epoch 55 | loss: 0.34208 | val_0_auc: 0.87776 |  0:02:31s\n",
      "epoch 56 | loss: 0.3421  | val_0_auc: 0.87819 |  0:02:34s\n",
      "epoch 57 | loss: 0.34429 | val_0_auc: 0.8801  |  0:02:37s\n",
      "epoch 58 | loss: 0.34067 | val_0_auc: 0.87934 |  0:02:39s\n",
      "epoch 59 | loss: 0.33637 | val_0_auc: 0.88141 |  0:02:42s\n",
      "epoch 60 | loss: 0.33678 | val_0_auc: 0.88176 |  0:02:45s\n",
      "epoch 61 | loss: 0.33504 | val_0_auc: 0.88131 |  0:02:48s\n",
      "epoch 62 | loss: 0.33715 | val_0_auc: 0.88084 |  0:02:51s\n",
      "epoch 63 | loss: 0.33729 | val_0_auc: 0.88245 |  0:02:54s\n",
      "epoch 64 | loss: 0.33365 | val_0_auc: 0.88063 |  0:02:56s\n",
      "epoch 65 | loss: 0.33435 | val_0_auc: 0.88268 |  0:02:59s\n",
      "epoch 66 | loss: 0.33303 | val_0_auc: 0.8831  |  0:03:02s\n",
      "epoch 67 | loss: 0.32965 | val_0_auc: 0.88388 |  0:03:04s\n",
      "epoch 68 | loss: 0.33397 | val_0_auc: 0.8827  |  0:03:07s\n",
      "epoch 69 | loss: 0.33217 | val_0_auc: 0.88439 |  0:03:10s\n",
      "epoch 70 | loss: 0.32932 | val_0_auc: 0.88418 |  0:03:12s\n",
      "epoch 71 | loss: 0.32993 | val_0_auc: 0.88493 |  0:03:15s\n",
      "epoch 72 | loss: 0.33094 | val_0_auc: 0.8842  |  0:03:18s\n",
      "epoch 73 | loss: 0.32714 | val_0_auc: 0.88596 |  0:03:20s\n",
      "epoch 74 | loss: 0.32905 | val_0_auc: 0.88478 |  0:03:23s\n",
      "epoch 75 | loss: 0.32602 | val_0_auc: 0.88716 |  0:03:26s\n",
      "epoch 76 | loss: 0.32528 | val_0_auc: 0.88591 |  0:03:28s\n",
      "epoch 77 | loss: 0.32694 | val_0_auc: 0.88705 |  0:03:31s\n",
      "epoch 78 | loss: 0.32255 | val_0_auc: 0.88757 |  0:03:33s\n",
      "epoch 79 | loss: 0.32304 | val_0_auc: 0.88889 |  0:03:36s\n",
      "epoch 80 | loss: 0.3228  | val_0_auc: 0.88893 |  0:03:39s\n",
      "epoch 81 | loss: 0.32457 | val_0_auc: 0.88854 |  0:03:42s\n",
      "epoch 82 | loss: 0.31928 | val_0_auc: 0.88825 |  0:03:44s\n",
      "epoch 83 | loss: 0.32207 | val_0_auc: 0.88784 |  0:03:47s\n",
      "epoch 84 | loss: 0.32106 | val_0_auc: 0.88817 |  0:03:49s\n",
      "epoch 85 | loss: 0.31792 | val_0_auc: 0.88925 |  0:03:52s\n",
      "epoch 86 | loss: 0.31959 | val_0_auc: 0.89048 |  0:03:55s\n",
      "epoch 87 | loss: 0.31963 | val_0_auc: 0.89117 |  0:03:57s\n",
      "epoch 88 | loss: 0.31848 | val_0_auc: 0.89175 |  0:04:00s\n",
      "epoch 89 | loss: 0.31535 | val_0_auc: 0.89036 |  0:04:03s\n",
      "epoch 90 | loss: 0.31563 | val_0_auc: 0.89042 |  0:04:05s\n",
      "epoch 91 | loss: 0.3134  | val_0_auc: 0.88907 |  0:04:08s\n",
      "epoch 92 | loss: 0.31459 | val_0_auc: 0.89008 |  0:04:11s\n",
      "epoch 93 | loss: 0.31484 | val_0_auc: 0.89049 |  0:04:13s\n",
      "epoch 94 | loss: 0.31315 | val_0_auc: 0.89026 |  0:04:16s\n",
      "epoch 95 | loss: 0.31318 | val_0_auc: 0.89146 |  0:04:19s\n",
      "epoch 96 | loss: 0.31141 | val_0_auc: 0.89201 |  0:04:21s\n",
      "epoch 97 | loss: 0.31352 | val_0_auc: 0.8914  |  0:04:24s\n",
      "epoch 98 | loss: 0.30963 | val_0_auc: 0.8927  |  0:04:26s\n",
      "epoch 99 | loss: 0.31078 | val_0_auc: 0.89448 |  0:04:29s\n",
      "epoch 100| loss: 0.31159 | val_0_auc: 0.89244 |  0:04:32s\n",
      "epoch 101| loss: 0.30818 | val_0_auc: 0.89315 |  0:04:34s\n",
      "epoch 102| loss: 0.30505 | val_0_auc: 0.89223 |  0:04:37s\n",
      "epoch 103| loss: 0.30633 | val_0_auc: 0.89236 |  0:04:40s\n",
      "epoch 104| loss: 0.30802 | val_0_auc: 0.89416 |  0:04:42s\n",
      "epoch 105| loss: 0.30571 | val_0_auc: 0.89262 |  0:04:45s\n",
      "epoch 106| loss: 0.30637 | val_0_auc: 0.89349 |  0:04:47s\n",
      "epoch 107| loss: 0.30874 | val_0_auc: 0.89398 |  0:04:50s\n",
      "epoch 108| loss: 0.30514 | val_0_auc: 0.89492 |  0:04:53s\n",
      "epoch 109| loss: 0.30325 | val_0_auc: 0.89464 |  0:04:56s\n",
      "epoch 110| loss: 0.30732 | val_0_auc: 0.89443 |  0:04:58s\n",
      "epoch 111| loss: 0.30437 | val_0_auc: 0.89442 |  0:05:01s\n",
      "epoch 112| loss: 0.30138 | val_0_auc: 0.89592 |  0:05:04s\n",
      "epoch 113| loss: 0.30215 | val_0_auc: 0.89566 |  0:05:06s\n",
      "epoch 114| loss: 0.30495 | val_0_auc: 0.8955  |  0:05:09s\n",
      "epoch 115| loss: 0.30243 | val_0_auc: 0.89595 |  0:05:11s\n",
      "epoch 116| loss: 0.29978 | val_0_auc: 0.89689 |  0:05:14s\n",
      "epoch 117| loss: 0.30092 | val_0_auc: 0.89849 |  0:05:17s\n",
      "epoch 118| loss: 0.29998 | val_0_auc: 0.89794 |  0:05:19s\n",
      "epoch 119| loss: 0.29836 | val_0_auc: 0.89779 |  0:05:22s\n",
      "epoch 120| loss: 0.29931 | val_0_auc: 0.89748 |  0:05:25s\n",
      "epoch 121| loss: 0.30001 | val_0_auc: 0.89734 |  0:05:27s\n",
      "epoch 122| loss: 0.30011 | val_0_auc: 0.89705 |  0:05:30s\n",
      "epoch 123| loss: 0.29684 | val_0_auc: 0.89727 |  0:05:33s\n",
      "epoch 124| loss: 0.29764 | val_0_auc: 0.89664 |  0:05:35s\n",
      "epoch 125| loss: 0.29686 | val_0_auc: 0.89771 |  0:05:38s\n",
      "epoch 126| loss: 0.29612 | val_0_auc: 0.89742 |  0:05:40s\n",
      "epoch 127| loss: 0.29802 | val_0_auc: 0.89681 |  0:05:43s\n",
      "epoch 128| loss: 0.29572 | val_0_auc: 0.89562 |  0:05:46s\n",
      "epoch 129| loss: 0.29488 | val_0_auc: 0.89643 |  0:05:48s\n",
      "epoch 130| loss: 0.29755 | val_0_auc: 0.89673 |  0:05:51s\n",
      "epoch 131| loss: 0.29488 | val_0_auc: 0.89594 |  0:05:54s\n",
      "epoch 132| loss: 0.29295 | val_0_auc: 0.89647 |  0:05:56s\n",
      "epoch 133| loss: 0.29631 | val_0_auc: 0.89758 |  0:05:59s\n",
      "epoch 134| loss: 0.29263 | val_0_auc: 0.89767 |  0:06:02s\n",
      "epoch 135| loss: 0.29063 | val_0_auc: 0.89677 |  0:06:04s\n",
      "epoch 136| loss: 0.29134 | val_0_auc: 0.89723 |  0:06:07s\n",
      "epoch 137| loss: 0.29283 | val_0_auc: 0.89604 |  0:06:09s\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_val_0_auc = 0.89849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71459 | val_0_auc: 0.60148 |  0:00:01s\n",
      "epoch 1  | loss: 0.52607 | val_0_auc: 0.76301 |  0:00:02s\n",
      "epoch 2  | loss: 0.4497  | val_0_auc: 0.80806 |  0:00:03s\n",
      "epoch 3  | loss: 0.39026 | val_0_auc: 0.84062 |  0:00:05s\n",
      "epoch 4  | loss: 0.36118 | val_0_auc: 0.85884 |  0:00:06s\n",
      "epoch 5  | loss: 0.34442 | val_0_auc: 0.86667 |  0:00:07s\n",
      "epoch 6  | loss: 0.3354  | val_0_auc: 0.8726  |  0:00:09s\n",
      "epoch 7  | loss: 0.32992 | val_0_auc: 0.8776  |  0:00:10s\n",
      "epoch 8  | loss: 0.32264 | val_0_auc: 0.88347 |  0:00:11s\n",
      "epoch 9  | loss: 0.3193  | val_0_auc: 0.88155 |  0:00:12s\n",
      "epoch 10 | loss: 0.3174  | val_0_auc: 0.88123 |  0:00:14s\n",
      "epoch 11 | loss: 0.31512 | val_0_auc: 0.88745 |  0:00:15s\n",
      "epoch 12 | loss: 0.31056 | val_0_auc: 0.89252 |  0:00:16s\n",
      "epoch 13 | loss: 0.30456 | val_0_auc: 0.89288 |  0:00:17s\n",
      "epoch 14 | loss: 0.30316 | val_0_auc: 0.89318 |  0:00:18s\n",
      "epoch 15 | loss: 0.29824 | val_0_auc: 0.89362 |  0:00:20s\n",
      "epoch 16 | loss: 0.29925 | val_0_auc: 0.89775 |  0:00:21s\n",
      "epoch 17 | loss: 0.29638 | val_0_auc: 0.89971 |  0:00:22s\n",
      "epoch 18 | loss: 0.29845 | val_0_auc: 0.90035 |  0:00:24s\n",
      "epoch 19 | loss: 0.29453 | val_0_auc: 0.8988  |  0:00:25s\n",
      "epoch 20 | loss: 0.29274 | val_0_auc: 0.90032 |  0:00:26s\n",
      "epoch 21 | loss: 0.29075 | val_0_auc: 0.89562 |  0:00:27s\n",
      "epoch 22 | loss: 0.29001 | val_0_auc: 0.89535 |  0:00:29s\n",
      "epoch 23 | loss: 0.2878  | val_0_auc: 0.89506 |  0:00:30s\n",
      "epoch 24 | loss: 0.28626 | val_0_auc: 0.89538 |  0:00:32s\n",
      "epoch 25 | loss: 0.29175 | val_0_auc: 0.89849 |  0:00:33s\n",
      "epoch 26 | loss: 0.28684 | val_0_auc: 0.89972 |  0:00:34s\n",
      "epoch 27 | loss: 0.28604 | val_0_auc: 0.90002 |  0:00:35s\n",
      "epoch 28 | loss: 0.2867  | val_0_auc: 0.9016  |  0:00:37s\n",
      "epoch 29 | loss: 0.28474 | val_0_auc: 0.89731 |  0:00:38s\n",
      "epoch 30 | loss: 0.2853  | val_0_auc: 0.90407 |  0:00:39s\n",
      "epoch 31 | loss: 0.28177 | val_0_auc: 0.90129 |  0:00:40s\n",
      "epoch 32 | loss: 0.28258 | val_0_auc: 0.87471 |  0:00:42s\n",
      "epoch 33 | loss: 0.27989 | val_0_auc: 0.89833 |  0:00:43s\n",
      "epoch 34 | loss: 0.27857 | val_0_auc: 0.88969 |  0:00:44s\n",
      "epoch 35 | loss: 0.27694 | val_0_auc: 0.8985  |  0:00:45s\n",
      "epoch 36 | loss: 0.27887 | val_0_auc: 0.89986 |  0:00:47s\n",
      "epoch 37 | loss: 0.27735 | val_0_auc: 0.90171 |  0:00:48s\n",
      "epoch 38 | loss: 0.27973 | val_0_auc: 0.89959 |  0:00:49s\n",
      "epoch 39 | loss: 0.2786  | val_0_auc: 0.9029  |  0:00:50s\n",
      "epoch 40 | loss: 0.27935 | val_0_auc: 0.90075 |  0:00:52s\n",
      "epoch 41 | loss: 0.28287 | val_0_auc: 0.90079 |  0:00:53s\n",
      "epoch 42 | loss: 0.28177 | val_0_auc: 0.90301 |  0:00:54s\n",
      "epoch 43 | loss: 0.27961 | val_0_auc: 0.89798 |  0:00:55s\n",
      "epoch 44 | loss: 0.27972 | val_0_auc: 0.89561 |  0:00:57s\n",
      "epoch 45 | loss: 0.28046 | val_0_auc: 0.90077 |  0:00:58s\n",
      "epoch 46 | loss: 0.275   | val_0_auc: 0.89643 |  0:00:59s\n",
      "epoch 47 | loss: 0.27895 | val_0_auc: 0.90361 |  0:01:00s\n",
      "epoch 48 | loss: 0.27784 | val_0_auc: 0.90172 |  0:01:02s\n",
      "epoch 49 | loss: 0.27804 | val_0_auc: 0.90234 |  0:01:03s\n",
      "epoch 50 | loss: 0.27705 | val_0_auc: 0.9001  |  0:01:04s\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_0_auc = 0.90407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.76621 | val_0_auc: 0.50169 |  0:00:01s\n",
      "epoch 1  | loss: 0.69386 | val_0_auc: 0.49797 |  0:00:02s\n",
      "epoch 2  | loss: 0.64245 | val_0_auc: 0.5678  |  0:00:03s\n",
      "epoch 3  | loss: 0.61128 | val_0_auc: 0.63058 |  0:00:05s\n",
      "epoch 4  | loss: 0.59249 | val_0_auc: 0.66814 |  0:00:06s\n",
      "epoch 5  | loss: 0.57668 | val_0_auc: 0.70221 |  0:00:07s\n",
      "epoch 6  | loss: 0.5591  | val_0_auc: 0.72243 |  0:00:09s\n",
      "epoch 7  | loss: 0.54582 | val_0_auc: 0.73354 |  0:00:10s\n",
      "epoch 8  | loss: 0.53501 | val_0_auc: 0.74356 |  0:00:11s\n",
      "epoch 9  | loss: 0.52189 | val_0_auc: 0.7507  |  0:00:12s\n",
      "epoch 10 | loss: 0.51088 | val_0_auc: 0.75541 |  0:00:14s\n",
      "epoch 11 | loss: 0.50228 | val_0_auc: 0.76256 |  0:00:15s\n",
      "epoch 12 | loss: 0.49391 | val_0_auc: 0.769   |  0:00:16s\n",
      "epoch 13 | loss: 0.48478 | val_0_auc: 0.77449 |  0:00:18s\n",
      "epoch 14 | loss: 0.47788 | val_0_auc: 0.78615 |  0:00:19s\n",
      "epoch 15 | loss: 0.47124 | val_0_auc: 0.79131 |  0:00:20s\n",
      "epoch 16 | loss: 0.46488 | val_0_auc: 0.79655 |  0:00:22s\n",
      "epoch 17 | loss: 0.45859 | val_0_auc: 0.79833 |  0:00:23s\n",
      "epoch 18 | loss: 0.45611 | val_0_auc: 0.80321 |  0:00:24s\n",
      "epoch 19 | loss: 0.44964 | val_0_auc: 0.80617 |  0:00:25s\n",
      "epoch 20 | loss: 0.44349 | val_0_auc: 0.80791 |  0:00:27s\n",
      "epoch 21 | loss: 0.44162 | val_0_auc: 0.80982 |  0:00:28s\n",
      "epoch 22 | loss: 0.43778 | val_0_auc: 0.81223 |  0:00:29s\n",
      "epoch 23 | loss: 0.43598 | val_0_auc: 0.81485 |  0:00:31s\n",
      "epoch 24 | loss: 0.43205 | val_0_auc: 0.81565 |  0:00:32s\n",
      "epoch 25 | loss: 0.4286  | val_0_auc: 0.81487 |  0:00:33s\n",
      "epoch 26 | loss: 0.42689 | val_0_auc: 0.81285 |  0:00:34s\n",
      "epoch 27 | loss: 0.42238 | val_0_auc: 0.81655 |  0:00:36s\n",
      "epoch 28 | loss: 0.42132 | val_0_auc: 0.81824 |  0:00:37s\n",
      "epoch 29 | loss: 0.41761 | val_0_auc: 0.81954 |  0:00:38s\n",
      "epoch 30 | loss: 0.41544 | val_0_auc: 0.81934 |  0:00:40s\n",
      "epoch 31 | loss: 0.41496 | val_0_auc: 0.82123 |  0:00:41s\n",
      "epoch 32 | loss: 0.41405 | val_0_auc: 0.81496 |  0:00:42s\n",
      "epoch 33 | loss: 0.40903 | val_0_auc: 0.8253  |  0:00:43s\n",
      "epoch 34 | loss: 0.40782 | val_0_auc: 0.82353 |  0:00:45s\n",
      "epoch 35 | loss: 0.40213 | val_0_auc: 0.82108 |  0:00:46s\n",
      "epoch 36 | loss: 0.40436 | val_0_auc: 0.82148 |  0:00:47s\n",
      "epoch 37 | loss: 0.40099 | val_0_auc: 0.79706 |  0:00:48s\n",
      "epoch 38 | loss: 0.399   | val_0_auc: 0.81714 |  0:00:50s\n",
      "epoch 39 | loss: 0.39488 | val_0_auc: 0.81833 |  0:00:51s\n",
      "epoch 40 | loss: 0.39212 | val_0_auc: 0.82537 |  0:00:52s\n",
      "epoch 41 | loss: 0.38893 | val_0_auc: 0.83242 |  0:00:54s\n",
      "epoch 42 | loss: 0.3885  | val_0_auc: 0.83025 |  0:00:55s\n",
      "epoch 43 | loss: 0.38696 | val_0_auc: 0.83942 |  0:00:56s\n",
      "epoch 44 | loss: 0.38456 | val_0_auc: 0.84264 |  0:00:57s\n",
      "epoch 45 | loss: 0.38429 | val_0_auc: 0.84298 |  0:00:59s\n",
      "epoch 46 | loss: 0.38296 | val_0_auc: 0.84297 |  0:01:00s\n",
      "epoch 47 | loss: 0.38142 | val_0_auc: 0.84498 |  0:01:01s\n",
      "epoch 48 | loss: 0.37833 | val_0_auc: 0.84743 |  0:01:03s\n",
      "epoch 49 | loss: 0.37629 | val_0_auc: 0.84884 |  0:01:04s\n",
      "epoch 50 | loss: 0.37858 | val_0_auc: 0.85067 |  0:01:05s\n",
      "epoch 51 | loss: 0.37721 | val_0_auc: 0.85064 |  0:01:06s\n",
      "epoch 52 | loss: 0.37566 | val_0_auc: 0.85058 |  0:01:08s\n",
      "epoch 53 | loss: 0.37257 | val_0_auc: 0.85118 |  0:01:09s\n",
      "epoch 54 | loss: 0.37396 | val_0_auc: 0.85217 |  0:01:10s\n",
      "epoch 55 | loss: 0.37304 | val_0_auc: 0.85194 |  0:01:12s\n",
      "epoch 56 | loss: 0.37108 | val_0_auc: 0.85197 |  0:01:13s\n",
      "epoch 57 | loss: 0.36947 | val_0_auc: 0.85359 |  0:01:14s\n",
      "epoch 58 | loss: 0.36686 | val_0_auc: 0.85359 |  0:01:15s\n",
      "epoch 59 | loss: 0.36819 | val_0_auc: 0.8545  |  0:01:17s\n",
      "epoch 60 | loss: 0.3669  | val_0_auc: 0.85306 |  0:01:18s\n",
      "epoch 61 | loss: 0.36541 | val_0_auc: 0.85528 |  0:01:19s\n",
      "epoch 62 | loss: 0.36619 | val_0_auc: 0.85691 |  0:01:21s\n",
      "epoch 63 | loss: 0.36516 | val_0_auc: 0.85687 |  0:01:22s\n",
      "epoch 64 | loss: 0.36321 | val_0_auc: 0.85723 |  0:01:23s\n",
      "epoch 65 | loss: 0.36381 | val_0_auc: 0.85751 |  0:01:24s\n",
      "epoch 66 | loss: 0.36334 | val_0_auc: 0.85934 |  0:01:26s\n",
      "epoch 67 | loss: 0.36149 | val_0_auc: 0.85969 |  0:01:27s\n",
      "epoch 68 | loss: 0.36316 | val_0_auc: 0.86083 |  0:01:28s\n",
      "epoch 69 | loss: 0.36113 | val_0_auc: 0.85957 |  0:01:29s\n",
      "epoch 70 | loss: 0.35977 | val_0_auc: 0.86066 |  0:01:31s\n",
      "epoch 71 | loss: 0.36004 | val_0_auc: 0.86168 |  0:01:32s\n",
      "epoch 72 | loss: 0.35826 | val_0_auc: 0.86222 |  0:01:33s\n",
      "epoch 73 | loss: 0.35803 | val_0_auc: 0.8634  |  0:01:35s\n",
      "epoch 74 | loss: 0.35828 | val_0_auc: 0.8631  |  0:01:36s\n",
      "epoch 75 | loss: 0.35449 | val_0_auc: 0.86462 |  0:01:37s\n",
      "epoch 76 | loss: 0.35723 | val_0_auc: 0.864   |  0:01:39s\n",
      "epoch 77 | loss: 0.35409 | val_0_auc: 0.86346 |  0:01:40s\n",
      "epoch 78 | loss: 0.35711 | val_0_auc: 0.86382 |  0:01:41s\n",
      "epoch 79 | loss: 0.35358 | val_0_auc: 0.86426 |  0:01:42s\n",
      "epoch 80 | loss: 0.35263 | val_0_auc: 0.86363 |  0:01:44s\n",
      "epoch 81 | loss: 0.35251 | val_0_auc: 0.86414 |  0:01:45s\n",
      "epoch 82 | loss: 0.3538  | val_0_auc: 0.86367 |  0:01:46s\n",
      "epoch 83 | loss: 0.35099 | val_0_auc: 0.86487 |  0:01:48s\n",
      "epoch 84 | loss: 0.35243 | val_0_auc: 0.86379 |  0:01:49s\n",
      "epoch 85 | loss: 0.35001 | val_0_auc: 0.86229 |  0:01:50s\n",
      "epoch 86 | loss: 0.35211 | val_0_auc: 0.86299 |  0:01:51s\n",
      "epoch 87 | loss: 0.3515  | val_0_auc: 0.86423 |  0:01:53s\n",
      "epoch 88 | loss: 0.34996 | val_0_auc: 0.86463 |  0:01:54s\n",
      "epoch 89 | loss: 0.34786 | val_0_auc: 0.86432 |  0:01:55s\n",
      "epoch 90 | loss: 0.34924 | val_0_auc: 0.86422 |  0:01:57s\n",
      "epoch 91 | loss: 0.34824 | val_0_auc: 0.86412 |  0:01:58s\n",
      "epoch 92 | loss: 0.34845 | val_0_auc: 0.86523 |  0:01:59s\n",
      "epoch 93 | loss: 0.34725 | val_0_auc: 0.86776 |  0:02:00s\n",
      "epoch 94 | loss: 0.34738 | val_0_auc: 0.86802 |  0:02:02s\n",
      "epoch 95 | loss: 0.34854 | val_0_auc: 0.8687  |  0:02:03s\n",
      "epoch 96 | loss: 0.3484  | val_0_auc: 0.86846 |  0:02:04s\n",
      "epoch 97 | loss: 0.34701 | val_0_auc: 0.86819 |  0:02:05s\n",
      "epoch 98 | loss: 0.34626 | val_0_auc: 0.86819 |  0:02:07s\n",
      "epoch 99 | loss: 0.34411 | val_0_auc: 0.86704 |  0:02:08s\n",
      "epoch 100| loss: 0.34306 | val_0_auc: 0.8683  |  0:02:09s\n",
      "epoch 101| loss: 0.34279 | val_0_auc: 0.87024 |  0:02:11s\n",
      "epoch 102| loss: 0.34392 | val_0_auc: 0.86776 |  0:02:12s\n",
      "epoch 103| loss: 0.34319 | val_0_auc: 0.86908 |  0:02:13s\n",
      "epoch 104| loss: 0.34112 | val_0_auc: 0.86829 |  0:02:14s\n",
      "epoch 105| loss: 0.34329 | val_0_auc: 0.86999 |  0:02:16s\n",
      "epoch 106| loss: 0.34191 | val_0_auc: 0.87158 |  0:02:17s\n",
      "epoch 107| loss: 0.34134 | val_0_auc: 0.87012 |  0:02:18s\n",
      "epoch 108| loss: 0.34061 | val_0_auc: 0.87027 |  0:02:20s\n",
      "epoch 109| loss: 0.33885 | val_0_auc: 0.8688  |  0:02:21s\n",
      "epoch 110| loss: 0.34275 | val_0_auc: 0.8696  |  0:02:22s\n",
      "epoch 111| loss: 0.33909 | val_0_auc: 0.87032 |  0:02:23s\n",
      "epoch 112| loss: 0.33688 | val_0_auc: 0.87135 |  0:02:25s\n",
      "epoch 113| loss: 0.33898 | val_0_auc: 0.87115 |  0:02:26s\n",
      "epoch 114| loss: 0.33717 | val_0_auc: 0.87216 |  0:02:27s\n",
      "epoch 115| loss: 0.33845 | val_0_auc: 0.87178 |  0:02:29s\n",
      "epoch 116| loss: 0.33829 | val_0_auc: 0.8705  |  0:02:30s\n",
      "epoch 117| loss: 0.33757 | val_0_auc: 0.87026 |  0:02:31s\n",
      "epoch 118| loss: 0.33918 | val_0_auc: 0.87133 |  0:02:32s\n",
      "epoch 119| loss: 0.33668 | val_0_auc: 0.86871 |  0:02:34s\n",
      "epoch 120| loss: 0.33822 | val_0_auc: 0.87135 |  0:02:35s\n",
      "epoch 121| loss: 0.33657 | val_0_auc: 0.8724  |  0:02:36s\n",
      "epoch 122| loss: 0.33792 | val_0_auc: 0.87338 |  0:02:38s\n",
      "epoch 123| loss: 0.33656 | val_0_auc: 0.87342 |  0:02:39s\n",
      "epoch 124| loss: 0.33691 | val_0_auc: 0.87306 |  0:02:40s\n",
      "epoch 125| loss: 0.33622 | val_0_auc: 0.87313 |  0:02:41s\n",
      "epoch 126| loss: 0.33588 | val_0_auc: 0.87369 |  0:02:43s\n",
      "epoch 127| loss: 0.33387 | val_0_auc: 0.87468 |  0:02:44s\n",
      "epoch 128| loss: 0.33432 | val_0_auc: 0.87481 |  0:02:45s\n",
      "epoch 129| loss: 0.33364 | val_0_auc: 0.87477 |  0:02:47s\n",
      "epoch 130| loss: 0.33308 | val_0_auc: 0.87434 |  0:02:48s\n",
      "epoch 131| loss: 0.33338 | val_0_auc: 0.87488 |  0:02:49s\n",
      "epoch 132| loss: 0.33261 | val_0_auc: 0.87585 |  0:02:50s\n",
      "epoch 133| loss: 0.33253 | val_0_auc: 0.87606 |  0:02:52s\n",
      "epoch 134| loss: 0.33191 | val_0_auc: 0.87545 |  0:02:53s\n",
      "epoch 135| loss: 0.3343  | val_0_auc: 0.87597 |  0:02:54s\n",
      "epoch 136| loss: 0.33159 | val_0_auc: 0.87504 |  0:02:56s\n",
      "epoch 137| loss: 0.33097 | val_0_auc: 0.8762  |  0:02:57s\n",
      "epoch 138| loss: 0.33029 | val_0_auc: 0.87654 |  0:02:58s\n",
      "epoch 139| loss: 0.33032 | val_0_auc: 0.87508 |  0:03:00s\n",
      "epoch 140| loss: 0.32953 | val_0_auc: 0.87596 |  0:03:01s\n",
      "epoch 141| loss: 0.32952 | val_0_auc: 0.87726 |  0:03:02s\n",
      "epoch 142| loss: 0.32891 | val_0_auc: 0.87685 |  0:03:03s\n",
      "epoch 143| loss: 0.32751 | val_0_auc: 0.87718 |  0:03:05s\n",
      "epoch 144| loss: 0.32851 | val_0_auc: 0.87563 |  0:03:06s\n",
      "epoch 145| loss: 0.32727 | val_0_auc: 0.87745 |  0:03:07s\n",
      "epoch 146| loss: 0.32795 | val_0_auc: 0.8772  |  0:03:09s\n",
      "epoch 147| loss: 0.32815 | val_0_auc: 0.87594 |  0:03:10s\n",
      "epoch 148| loss: 0.32788 | val_0_auc: 0.87596 |  0:03:11s\n",
      "epoch 149| loss: 0.32896 | val_0_auc: 0.87778 |  0:03:13s\n",
      "epoch 150| loss: 0.32942 | val_0_auc: 0.87772 |  0:03:14s\n",
      "epoch 151| loss: 0.32654 | val_0_auc: 0.87751 |  0:03:15s\n",
      "epoch 152| loss: 0.32595 | val_0_auc: 0.87707 |  0:03:16s\n",
      "epoch 153| loss: 0.32529 | val_0_auc: 0.877   |  0:03:18s\n",
      "epoch 154| loss: 0.32653 | val_0_auc: 0.87673 |  0:03:19s\n",
      "epoch 155| loss: 0.3269  | val_0_auc: 0.87837 |  0:03:20s\n",
      "epoch 156| loss: 0.32605 | val_0_auc: 0.87702 |  0:03:21s\n",
      "epoch 157| loss: 0.32576 | val_0_auc: 0.87747 |  0:03:23s\n",
      "epoch 158| loss: 0.32595 | val_0_auc: 0.87841 |  0:03:24s\n",
      "epoch 159| loss: 0.32701 | val_0_auc: 0.87794 |  0:03:25s\n",
      "epoch 160| loss: 0.32644 | val_0_auc: 0.87855 |  0:03:27s\n",
      "epoch 161| loss: 0.3241  | val_0_auc: 0.87827 |  0:03:28s\n",
      "epoch 162| loss: 0.324   | val_0_auc: 0.87784 |  0:03:29s\n",
      "epoch 163| loss: 0.32601 | val_0_auc: 0.87864 |  0:03:31s\n",
      "epoch 164| loss: 0.32524 | val_0_auc: 0.87866 |  0:03:32s\n",
      "epoch 165| loss: 0.32403 | val_0_auc: 0.87936 |  0:03:33s\n",
      "epoch 166| loss: 0.32568 | val_0_auc: 0.87859 |  0:03:34s\n",
      "epoch 167| loss: 0.32376 | val_0_auc: 0.87916 |  0:03:36s\n",
      "epoch 168| loss: 0.3249  | val_0_auc: 0.87801 |  0:03:37s\n",
      "epoch 169| loss: 0.32535 | val_0_auc: 0.87777 |  0:03:38s\n",
      "epoch 170| loss: 0.32441 | val_0_auc: 0.87723 |  0:03:40s\n",
      "epoch 171| loss: 0.32357 | val_0_auc: 0.87687 |  0:03:41s\n",
      "epoch 172| loss: 0.32445 | val_0_auc: 0.87247 |  0:03:42s\n",
      "epoch 173| loss: 0.32288 | val_0_auc: 0.8776  |  0:03:44s\n",
      "epoch 174| loss: 0.32314 | val_0_auc: 0.8779  |  0:03:45s\n",
      "epoch 175| loss: 0.32296 | val_0_auc: 0.87777 |  0:03:46s\n",
      "epoch 176| loss: 0.32443 | val_0_auc: 0.87808 |  0:03:47s\n",
      "epoch 177| loss: 0.32324 | val_0_auc: 0.87877 |  0:03:49s\n",
      "epoch 178| loss: 0.32278 | val_0_auc: 0.87862 |  0:03:50s\n",
      "epoch 179| loss: 0.32343 | val_0_auc: 0.87877 |  0:03:51s\n",
      "epoch 180| loss: 0.32304 | val_0_auc: 0.87841 |  0:03:53s\n",
      "epoch 181| loss: 0.32236 | val_0_auc: 0.87792 |  0:03:54s\n",
      "epoch 182| loss: 0.32294 | val_0_auc: 0.87884 |  0:03:55s\n",
      "epoch 183| loss: 0.32079 | val_0_auc: 0.87931 |  0:03:56s\n",
      "epoch 184| loss: 0.3236  | val_0_auc: 0.87903 |  0:03:58s\n",
      "epoch 185| loss: 0.32289 | val_0_auc: 0.87938 |  0:03:59s\n",
      "epoch 186| loss: 0.32164 | val_0_auc: 0.87946 |  0:04:00s\n",
      "epoch 187| loss: 0.32138 | val_0_auc: 0.87983 |  0:04:02s\n",
      "epoch 188| loss: 0.32326 | val_0_auc: 0.88061 |  0:04:04s\n",
      "epoch 189| loss: 0.32192 | val_0_auc: 0.88073 |  0:04:05s\n",
      "epoch 190| loss: 0.32384 | val_0_auc: 0.88015 |  0:04:06s\n",
      "epoch 191| loss: 0.3225  | val_0_auc: 0.88014 |  0:04:07s\n",
      "epoch 192| loss: 0.32231 | val_0_auc: 0.87956 |  0:04:09s\n",
      "epoch 193| loss: 0.32015 | val_0_auc: 0.87966 |  0:04:10s\n",
      "epoch 194| loss: 0.32145 | val_0_auc: 0.87916 |  0:04:11s\n",
      "epoch 195| loss: 0.32185 | val_0_auc: 0.8795  |  0:04:13s\n",
      "epoch 196| loss: 0.32011 | val_0_auc: 0.88015 |  0:04:14s\n",
      "epoch 197| loss: 0.32015 | val_0_auc: 0.87893 |  0:04:15s\n",
      "epoch 198| loss: 0.32056 | val_0_auc: 0.88019 |  0:04:17s\n",
      "epoch 199| loss: 0.32062 | val_0_auc: 0.87915 |  0:04:18s\n",
      "epoch 200| loss: 0.32032 | val_0_auc: 0.87834 |  0:04:19s\n",
      "epoch 201| loss: 0.32025 | val_0_auc: 0.87916 |  0:04:20s\n",
      "epoch 202| loss: 0.32055 | val_0_auc: 0.87903 |  0:04:22s\n",
      "epoch 203| loss: 0.32084 | val_0_auc: 0.87874 |  0:04:23s\n",
      "epoch 204| loss: 0.32009 | val_0_auc: 0.87928 |  0:04:24s\n",
      "epoch 205| loss: 0.31948 | val_0_auc: 0.87895 |  0:04:26s\n",
      "epoch 206| loss: 0.32028 | val_0_auc: 0.87958 |  0:04:27s\n",
      "epoch 207| loss: 0.32035 | val_0_auc: 0.87872 |  0:04:29s\n",
      "epoch 208| loss: 0.32137 | val_0_auc: 0.88    |  0:04:30s\n",
      "epoch 209| loss: 0.32082 | val_0_auc: 0.87979 |  0:04:31s\n",
      "\n",
      "Early stopping occurred at epoch 209 with best_epoch = 189 and best_val_0_auc = 0.88073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.99824 | val_0_auc: 0.55702 |  0:00:02s\n",
      "epoch 1  | loss: 0.62498 | val_0_auc: 0.64336 |  0:00:04s\n",
      "epoch 2  | loss: 0.53484 | val_0_auc: 0.69481 |  0:00:06s\n",
      "epoch 3  | loss: 0.49917 | val_0_auc: 0.72858 |  0:00:09s\n",
      "epoch 4  | loss: 0.47941 | val_0_auc: 0.75437 |  0:00:11s\n",
      "epoch 5  | loss: 0.46509 | val_0_auc: 0.76683 |  0:00:13s\n",
      "epoch 6  | loss: 0.45867 | val_0_auc: 0.78272 |  0:00:16s\n",
      "epoch 7  | loss: 0.44851 | val_0_auc: 0.79312 |  0:00:18s\n",
      "epoch 8  | loss: 0.44431 | val_0_auc: 0.80311 |  0:00:20s\n",
      "epoch 9  | loss: 0.43696 | val_0_auc: 0.81032 |  0:00:22s\n",
      "epoch 10 | loss: 0.43531 | val_0_auc: 0.81143 |  0:00:25s\n",
      "epoch 11 | loss: 0.43037 | val_0_auc: 0.81658 |  0:00:27s\n",
      "epoch 12 | loss: 0.42498 | val_0_auc: 0.82245 |  0:00:29s\n",
      "epoch 13 | loss: 0.41944 | val_0_auc: 0.82643 |  0:00:31s\n",
      "epoch 14 | loss: 0.41824 | val_0_auc: 0.83003 |  0:00:34s\n",
      "epoch 15 | loss: 0.41761 | val_0_auc: 0.83055 |  0:00:36s\n",
      "epoch 16 | loss: 0.4159  | val_0_auc: 0.83249 |  0:00:38s\n",
      "epoch 17 | loss: 0.4089  | val_0_auc: 0.83357 |  0:00:40s\n",
      "epoch 18 | loss: 0.40867 | val_0_auc: 0.83366 |  0:00:43s\n",
      "epoch 19 | loss: 0.4065  | val_0_auc: 0.83619 |  0:00:45s\n",
      "epoch 20 | loss: 0.40578 | val_0_auc: 0.83925 |  0:00:47s\n",
      "epoch 21 | loss: 0.40314 | val_0_auc: 0.83847 |  0:00:49s\n",
      "epoch 22 | loss: 0.40591 | val_0_auc: 0.84117 |  0:00:52s\n",
      "epoch 23 | loss: 0.39921 | val_0_auc: 0.84023 |  0:00:54s\n",
      "epoch 24 | loss: 0.39846 | val_0_auc: 0.84082 |  0:00:56s\n",
      "epoch 25 | loss: 0.39759 | val_0_auc: 0.84253 |  0:00:58s\n",
      "epoch 26 | loss: 0.39158 | val_0_auc: 0.8455  |  0:01:01s\n",
      "epoch 27 | loss: 0.39109 | val_0_auc: 0.84572 |  0:01:03s\n",
      "epoch 28 | loss: 0.39055 | val_0_auc: 0.84681 |  0:01:05s\n",
      "epoch 29 | loss: 0.38996 | val_0_auc: 0.84826 |  0:01:07s\n",
      "epoch 30 | loss: 0.38679 | val_0_auc: 0.84898 |  0:01:10s\n",
      "epoch 31 | loss: 0.38442 | val_0_auc: 0.84896 |  0:01:12s\n",
      "epoch 32 | loss: 0.3845  | val_0_auc: 0.85117 |  0:01:14s\n",
      "epoch 33 | loss: 0.38177 | val_0_auc: 0.85191 |  0:01:16s\n",
      "epoch 34 | loss: 0.3824  | val_0_auc: 0.85088 |  0:01:19s\n",
      "epoch 35 | loss: 0.37814 | val_0_auc: 0.85315 |  0:01:21s\n",
      "epoch 36 | loss: 0.37716 | val_0_auc: 0.85391 |  0:01:23s\n",
      "epoch 37 | loss: 0.37799 | val_0_auc: 0.85387 |  0:01:25s\n",
      "epoch 38 | loss: 0.3767  | val_0_auc: 0.85304 |  0:01:28s\n",
      "epoch 39 | loss: 0.37586 | val_0_auc: 0.85514 |  0:01:30s\n",
      "epoch 40 | loss: 0.37454 | val_0_auc: 0.85531 |  0:01:32s\n",
      "epoch 41 | loss: 0.37218 | val_0_auc: 0.85464 |  0:01:34s\n",
      "epoch 42 | loss: 0.37005 | val_0_auc: 0.85568 |  0:01:37s\n",
      "epoch 43 | loss: 0.37039 | val_0_auc: 0.85591 |  0:01:39s\n",
      "epoch 44 | loss: 0.36819 | val_0_auc: 0.85665 |  0:01:41s\n",
      "epoch 45 | loss: 0.36761 | val_0_auc: 0.8562  |  0:01:43s\n",
      "epoch 46 | loss: 0.36579 | val_0_auc: 0.85748 |  0:01:46s\n",
      "epoch 47 | loss: 0.36704 | val_0_auc: 0.85637 |  0:01:48s\n",
      "epoch 48 | loss: 0.3676  | val_0_auc: 0.85834 |  0:01:50s\n",
      "epoch 49 | loss: 0.36563 | val_0_auc: 0.85773 |  0:01:52s\n",
      "epoch 50 | loss: 0.36645 | val_0_auc: 0.86066 |  0:01:55s\n",
      "epoch 51 | loss: 0.36314 | val_0_auc: 0.85923 |  0:01:57s\n",
      "epoch 52 | loss: 0.36305 | val_0_auc: 0.86189 |  0:01:59s\n",
      "epoch 53 | loss: 0.36131 | val_0_auc: 0.86087 |  0:02:02s\n",
      "epoch 54 | loss: 0.35973 | val_0_auc: 0.86119 |  0:02:04s\n",
      "epoch 55 | loss: 0.35988 | val_0_auc: 0.85956 |  0:02:06s\n",
      "epoch 56 | loss: 0.35874 | val_0_auc: 0.86434 |  0:02:08s\n",
      "epoch 57 | loss: 0.3588  | val_0_auc: 0.86377 |  0:02:11s\n",
      "epoch 58 | loss: 0.35676 | val_0_auc: 0.8639  |  0:02:13s\n",
      "epoch 59 | loss: 0.35553 | val_0_auc: 0.86463 |  0:02:15s\n",
      "epoch 60 | loss: 0.35455 | val_0_auc: 0.86284 |  0:02:17s\n",
      "epoch 61 | loss: 0.35569 | val_0_auc: 0.86439 |  0:02:20s\n",
      "epoch 62 | loss: 0.35292 | val_0_auc: 0.86275 |  0:02:22s\n",
      "epoch 63 | loss: 0.3524  | val_0_auc: 0.86333 |  0:02:24s\n",
      "epoch 64 | loss: 0.35265 | val_0_auc: 0.8638  |  0:02:26s\n",
      "epoch 65 | loss: 0.35137 | val_0_auc: 0.86304 |  0:02:29s\n",
      "epoch 66 | loss: 0.3515  | val_0_auc: 0.86457 |  0:02:31s\n",
      "epoch 67 | loss: 0.34832 | val_0_auc: 0.86541 |  0:02:33s\n",
      "epoch 68 | loss: 0.34919 | val_0_auc: 0.86368 |  0:02:36s\n",
      "epoch 69 | loss: 0.34854 | val_0_auc: 0.86699 |  0:02:38s\n",
      "epoch 70 | loss: 0.34597 | val_0_auc: 0.86747 |  0:02:40s\n",
      "epoch 71 | loss: 0.34703 | val_0_auc: 0.86574 |  0:02:43s\n",
      "epoch 72 | loss: 0.34445 | val_0_auc: 0.86679 |  0:02:45s\n",
      "epoch 73 | loss: 0.34541 | val_0_auc: 0.86747 |  0:02:47s\n",
      "epoch 74 | loss: 0.34476 | val_0_auc: 0.8657  |  0:02:49s\n",
      "epoch 75 | loss: 0.34251 | val_0_auc: 0.86766 |  0:02:51s\n",
      "epoch 76 | loss: 0.34478 | val_0_auc: 0.86714 |  0:02:54s\n",
      "epoch 77 | loss: 0.34147 | val_0_auc: 0.86807 |  0:02:56s\n",
      "epoch 78 | loss: 0.34211 | val_0_auc: 0.86681 |  0:02:58s\n",
      "epoch 79 | loss: 0.34011 | val_0_auc: 0.86889 |  0:03:01s\n",
      "epoch 80 | loss: 0.34067 | val_0_auc: 0.86776 |  0:03:03s\n",
      "epoch 81 | loss: 0.34103 | val_0_auc: 0.86936 |  0:03:05s\n",
      "epoch 82 | loss: 0.33864 | val_0_auc: 0.86919 |  0:03:07s\n",
      "epoch 83 | loss: 0.33953 | val_0_auc: 0.86992 |  0:03:10s\n",
      "epoch 84 | loss: 0.33716 | val_0_auc: 0.86988 |  0:03:12s\n",
      "epoch 85 | loss: 0.33526 | val_0_auc: 0.86933 |  0:03:14s\n",
      "epoch 86 | loss: 0.33335 | val_0_auc: 0.86989 |  0:03:16s\n",
      "epoch 87 | loss: 0.3357  | val_0_auc: 0.86959 |  0:03:19s\n",
      "epoch 88 | loss: 0.33739 | val_0_auc: 0.87147 |  0:03:21s\n",
      "epoch 89 | loss: 0.33173 | val_0_auc: 0.87067 |  0:03:23s\n",
      "epoch 90 | loss: 0.33143 | val_0_auc: 0.87108 |  0:03:25s\n",
      "epoch 91 | loss: 0.33083 | val_0_auc: 0.87241 |  0:03:28s\n",
      "epoch 92 | loss: 0.33047 | val_0_auc: 0.87365 |  0:03:30s\n",
      "epoch 93 | loss: 0.33228 | val_0_auc: 0.87355 |  0:03:32s\n",
      "epoch 94 | loss: 0.32861 | val_0_auc: 0.87503 |  0:03:34s\n",
      "epoch 95 | loss: 0.32763 | val_0_auc: 0.87485 |  0:03:37s\n",
      "epoch 96 | loss: 0.32977 | val_0_auc: 0.87506 |  0:03:39s\n",
      "epoch 97 | loss: 0.32584 | val_0_auc: 0.87585 |  0:03:42s\n",
      "epoch 98 | loss: 0.32676 | val_0_auc: 0.87481 |  0:03:44s\n",
      "epoch 99 | loss: 0.32505 | val_0_auc: 0.87461 |  0:03:46s\n",
      "epoch 100| loss: 0.32284 | val_0_auc: 0.8744  |  0:03:49s\n",
      "epoch 101| loss: 0.32674 | val_0_auc: 0.87494 |  0:03:51s\n",
      "epoch 102| loss: 0.32403 | val_0_auc: 0.87824 |  0:03:53s\n",
      "epoch 103| loss: 0.32318 | val_0_auc: 0.87593 |  0:03:55s\n",
      "epoch 104| loss: 0.32241 | val_0_auc: 0.87499 |  0:03:58s\n",
      "epoch 105| loss: 0.32431 | val_0_auc: 0.87541 |  0:04:00s\n",
      "epoch 106| loss: 0.32167 | val_0_auc: 0.87565 |  0:04:02s\n",
      "epoch 107| loss: 0.32008 | val_0_auc: 0.8749  |  0:04:04s\n",
      "epoch 108| loss: 0.31922 | val_0_auc: 0.87621 |  0:04:07s\n",
      "epoch 109| loss: 0.31795 | val_0_auc: 0.87535 |  0:04:09s\n",
      "epoch 110| loss: 0.3171  | val_0_auc: 0.87706 |  0:04:11s\n",
      "epoch 111| loss: 0.31798 | val_0_auc: 0.8769  |  0:04:14s\n",
      "epoch 112| loss: 0.31574 | val_0_auc: 0.87655 |  0:04:16s\n",
      "epoch 113| loss: 0.31888 | val_0_auc: 0.87596 |  0:04:18s\n",
      "epoch 114| loss: 0.31636 | val_0_auc: 0.87611 |  0:04:20s\n",
      "epoch 115| loss: 0.31737 | val_0_auc: 0.87593 |  0:04:22s\n",
      "epoch 116| loss: 0.31503 | val_0_auc: 0.87702 |  0:04:25s\n",
      "epoch 117| loss: 0.31412 | val_0_auc: 0.87699 |  0:04:27s\n",
      "epoch 118| loss: 0.31189 | val_0_auc: 0.87589 |  0:04:29s\n",
      "epoch 119| loss: 0.31265 | val_0_auc: 0.87667 |  0:04:32s\n",
      "epoch 120| loss: 0.31229 | val_0_auc: 0.877   |  0:04:34s\n",
      "epoch 121| loss: 0.31149 | val_0_auc: 0.87808 |  0:04:36s\n",
      "epoch 122| loss: 0.31246 | val_0_auc: 0.87661 |  0:04:38s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_val_0_auc = 0.87824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.91487 | val_0_auc: 0.57146 |  0:00:02s\n",
      "epoch 1  | loss: 0.65165 | val_0_auc: 0.65209 |  0:00:05s\n",
      "epoch 2  | loss: 0.56162 | val_0_auc: 0.71671 |  0:00:08s\n",
      "epoch 3  | loss: 0.53036 | val_0_auc: 0.74724 |  0:00:11s\n",
      "epoch 4  | loss: 0.5043  | val_0_auc: 0.77644 |  0:00:14s\n",
      "epoch 5  | loss: 0.49808 | val_0_auc: 0.79458 |  0:00:17s\n",
      "epoch 6  | loss: 0.48653 | val_0_auc: 0.79425 |  0:00:21s\n",
      "epoch 7  | loss: 0.48773 | val_0_auc: 0.79703 |  0:00:24s\n",
      "epoch 8  | loss: 0.47586 | val_0_auc: 0.82319 |  0:00:27s\n",
      "epoch 9  | loss: 0.46772 | val_0_auc: 0.81854 |  0:00:29s\n",
      "epoch 10 | loss: 0.46386 | val_0_auc: 0.81802 |  0:00:32s\n",
      "epoch 11 | loss: 0.467   | val_0_auc: 0.81905 |  0:00:35s\n",
      "epoch 12 | loss: 0.52196 | val_0_auc: 0.78922 |  0:00:38s\n",
      "epoch 13 | loss: 0.47958 | val_0_auc: 0.81369 |  0:00:41s\n",
      "epoch 14 | loss: 0.4631  | val_0_auc: 0.82165 |  0:00:44s\n",
      "epoch 15 | loss: 0.44883 | val_0_auc: 0.824   |  0:00:47s\n",
      "epoch 16 | loss: 0.45304 | val_0_auc: 0.82766 |  0:00:50s\n",
      "epoch 17 | loss: 0.44758 | val_0_auc: 0.8341  |  0:00:53s\n",
      "epoch 18 | loss: 0.43496 | val_0_auc: 0.83997 |  0:00:56s\n",
      "epoch 19 | loss: 0.42726 | val_0_auc: 0.84517 |  0:00:59s\n",
      "epoch 20 | loss: 0.4196  | val_0_auc: 0.845   |  0:01:02s\n",
      "epoch 21 | loss: 0.40934 | val_0_auc: 0.84635 |  0:01:05s\n",
      "epoch 22 | loss: 0.40549 | val_0_auc: 0.843   |  0:01:08s\n",
      "epoch 23 | loss: 0.40767 | val_0_auc: 0.85073 |  0:01:10s\n",
      "epoch 24 | loss: 0.40686 | val_0_auc: 0.85628 |  0:01:13s\n",
      "epoch 25 | loss: 0.39922 | val_0_auc: 0.85722 |  0:01:16s\n",
      "epoch 26 | loss: 0.39734 | val_0_auc: 0.86033 |  0:01:19s\n",
      "epoch 27 | loss: 0.39567 | val_0_auc: 0.86207 |  0:01:22s\n",
      "epoch 28 | loss: 0.39122 | val_0_auc: 0.86108 |  0:01:25s\n",
      "epoch 29 | loss: 0.38816 | val_0_auc: 0.86461 |  0:01:28s\n",
      "epoch 30 | loss: 0.38638 | val_0_auc: 0.86176 |  0:01:31s\n",
      "epoch 31 | loss: 0.38264 | val_0_auc: 0.86399 |  0:01:34s\n",
      "epoch 32 | loss: 0.37689 | val_0_auc: 0.86865 |  0:01:37s\n",
      "epoch 33 | loss: 0.37525 | val_0_auc: 0.86822 |  0:01:40s\n",
      "epoch 34 | loss: 0.37935 | val_0_auc: 0.8622  |  0:01:43s\n",
      "epoch 35 | loss: 0.37729 | val_0_auc: 0.86466 |  0:01:46s\n",
      "epoch 36 | loss: 0.37582 | val_0_auc: 0.86409 |  0:01:49s\n",
      "epoch 37 | loss: 0.37276 | val_0_auc: 0.86551 |  0:01:51s\n",
      "epoch 38 | loss: 0.3692  | val_0_auc: 0.86965 |  0:01:54s\n",
      "epoch 39 | loss: 0.36418 | val_0_auc: 0.87177 |  0:01:57s\n",
      "epoch 40 | loss: 0.36301 | val_0_auc: 0.86932 |  0:02:00s\n",
      "epoch 41 | loss: 0.36241 | val_0_auc: 0.86766 |  0:02:03s\n",
      "epoch 42 | loss: 0.35958 | val_0_auc: 0.86775 |  0:02:06s\n",
      "epoch 43 | loss: 0.35906 | val_0_auc: 0.86896 |  0:02:09s\n",
      "epoch 44 | loss: 0.35817 | val_0_auc: 0.87024 |  0:02:12s\n",
      "epoch 45 | loss: 0.3574  | val_0_auc: 0.87649 |  0:02:15s\n",
      "epoch 46 | loss: 0.35186 | val_0_auc: 0.87323 |  0:02:18s\n",
      "epoch 47 | loss: 0.35241 | val_0_auc: 0.87385 |  0:02:21s\n",
      "epoch 48 | loss: 0.35002 | val_0_auc: 0.87494 |  0:02:24s\n",
      "epoch 49 | loss: 0.34771 | val_0_auc: 0.8759  |  0:02:27s\n",
      "epoch 50 | loss: 0.34532 | val_0_auc: 0.87881 |  0:02:30s\n",
      "epoch 51 | loss: 0.34395 | val_0_auc: 0.87969 |  0:02:33s\n",
      "epoch 52 | loss: 0.33908 | val_0_auc: 0.88063 |  0:02:36s\n",
      "epoch 53 | loss: 0.33668 | val_0_auc: 0.88164 |  0:02:38s\n",
      "epoch 54 | loss: 0.33798 | val_0_auc: 0.87893 |  0:02:41s\n",
      "epoch 55 | loss: 0.34177 | val_0_auc: 0.87881 |  0:02:44s\n",
      "epoch 56 | loss: 0.33909 | val_0_auc: 0.8827  |  0:02:47s\n",
      "epoch 57 | loss: 0.33843 | val_0_auc: 0.88227 |  0:02:50s\n",
      "epoch 58 | loss: 0.33715 | val_0_auc: 0.88566 |  0:02:53s\n",
      "epoch 59 | loss: 0.33552 | val_0_auc: 0.88575 |  0:02:56s\n",
      "epoch 60 | loss: 0.33699 | val_0_auc: 0.88581 |  0:02:59s\n",
      "epoch 61 | loss: 0.33464 | val_0_auc: 0.8877  |  0:03:02s\n",
      "epoch 62 | loss: 0.33127 | val_0_auc: 0.88665 |  0:03:05s\n",
      "epoch 63 | loss: 0.32984 | val_0_auc: 0.88909 |  0:03:08s\n",
      "epoch 64 | loss: 0.33047 | val_0_auc: 0.89003 |  0:03:11s\n",
      "epoch 65 | loss: 0.32653 | val_0_auc: 0.89047 |  0:03:14s\n",
      "epoch 66 | loss: 0.32707 | val_0_auc: 0.8919  |  0:03:16s\n",
      "epoch 67 | loss: 0.32396 | val_0_auc: 0.89203 |  0:03:19s\n",
      "epoch 68 | loss: 0.32567 | val_0_auc: 0.89125 |  0:03:22s\n",
      "epoch 69 | loss: 0.32176 | val_0_auc: 0.89347 |  0:03:25s\n",
      "epoch 70 | loss: 0.32227 | val_0_auc: 0.89391 |  0:03:28s\n",
      "epoch 71 | loss: 0.32029 | val_0_auc: 0.89382 |  0:03:31s\n",
      "epoch 72 | loss: 0.31772 | val_0_auc: 0.89336 |  0:03:34s\n",
      "epoch 73 | loss: 0.31832 | val_0_auc: 0.89372 |  0:03:37s\n",
      "epoch 74 | loss: 0.31725 | val_0_auc: 0.89436 |  0:03:40s\n",
      "epoch 75 | loss: 0.31758 | val_0_auc: 0.89504 |  0:03:43s\n",
      "epoch 76 | loss: 0.31841 | val_0_auc: 0.89547 |  0:03:46s\n",
      "epoch 77 | loss: 0.31614 | val_0_auc: 0.89652 |  0:03:49s\n",
      "epoch 78 | loss: 0.31759 | val_0_auc: 0.89671 |  0:03:51s\n",
      "epoch 79 | loss: 0.31464 | val_0_auc: 0.8973  |  0:03:54s\n",
      "epoch 80 | loss: 0.31392 | val_0_auc: 0.89415 |  0:03:57s\n",
      "epoch 81 | loss: 0.31528 | val_0_auc: 0.89415 |  0:04:00s\n",
      "epoch 82 | loss: 0.31447 | val_0_auc: 0.89516 |  0:04:03s\n",
      "epoch 83 | loss: 0.31266 | val_0_auc: 0.89627 |  0:04:06s\n",
      "epoch 84 | loss: 0.31357 | val_0_auc: 0.89589 |  0:04:09s\n",
      "epoch 85 | loss: 0.31275 | val_0_auc: 0.89814 |  0:04:12s\n",
      "epoch 86 | loss: 0.31284 | val_0_auc: 0.89523 |  0:04:15s\n",
      "epoch 87 | loss: 0.31105 | val_0_auc: 0.89767 |  0:04:18s\n",
      "epoch 88 | loss: 0.31117 | val_0_auc: 0.89857 |  0:04:21s\n",
      "epoch 89 | loss: 0.30991 | val_0_auc: 0.89741 |  0:04:24s\n",
      "epoch 90 | loss: 0.30883 | val_0_auc: 0.89682 |  0:04:27s\n",
      "epoch 91 | loss: 0.30783 | val_0_auc: 0.89749 |  0:04:29s\n",
      "epoch 92 | loss: 0.30878 | val_0_auc: 0.89562 |  0:04:32s\n",
      "epoch 93 | loss: 0.30869 | val_0_auc: 0.8972  |  0:04:35s\n",
      "epoch 94 | loss: 0.30687 | val_0_auc: 0.89962 |  0:04:38s\n",
      "epoch 95 | loss: 0.30627 | val_0_auc: 0.89888 |  0:04:41s\n",
      "epoch 96 | loss: 0.30464 | val_0_auc: 0.89783 |  0:04:44s\n",
      "epoch 97 | loss: 0.30276 | val_0_auc: 0.89841 |  0:04:47s\n",
      "epoch 98 | loss: 0.30208 | val_0_auc: 0.89856 |  0:04:51s\n",
      "epoch 99 | loss: 0.30339 | val_0_auc: 0.89949 |  0:04:54s\n",
      "epoch 100| loss: 0.3008  | val_0_auc: 0.90051 |  0:04:57s\n",
      "epoch 101| loss: 0.30021 | val_0_auc: 0.90022 |  0:05:00s\n",
      "epoch 102| loss: 0.30063 | val_0_auc: 0.90063 |  0:05:03s\n",
      "epoch 103| loss: 0.30168 | val_0_auc: 0.90098 |  0:05:06s\n",
      "epoch 104| loss: 0.30003 | val_0_auc: 0.90165 |  0:05:09s\n",
      "epoch 105| loss: 0.29788 | val_0_auc: 0.90255 |  0:05:12s\n",
      "epoch 106| loss: 0.297   | val_0_auc: 0.90196 |  0:05:15s\n",
      "epoch 107| loss: 0.29639 | val_0_auc: 0.90232 |  0:05:18s\n",
      "epoch 108| loss: 0.29678 | val_0_auc: 0.90113 |  0:05:21s\n",
      "epoch 109| loss: 0.3003  | val_0_auc: 0.90219 |  0:05:24s\n",
      "epoch 110| loss: 0.30077 | val_0_auc: 0.8998  |  0:05:27s\n",
      "epoch 111| loss: 0.30305 | val_0_auc: 0.90178 |  0:05:30s\n",
      "epoch 112| loss: 0.3005  | val_0_auc: 0.90251 |  0:05:33s\n",
      "epoch 113| loss: 0.29888 | val_0_auc: 0.90131 |  0:05:36s\n",
      "epoch 114| loss: 0.29851 | val_0_auc: 0.90296 |  0:05:39s\n",
      "epoch 115| loss: 0.29756 | val_0_auc: 0.90293 |  0:05:41s\n",
      "epoch 116| loss: 0.29998 | val_0_auc: 0.90134 |  0:05:44s\n",
      "epoch 117| loss: 0.29995 | val_0_auc: 0.90274 |  0:05:47s\n",
      "epoch 118| loss: 0.29835 | val_0_auc: 0.90448 |  0:05:50s\n",
      "epoch 119| loss: 0.2973  | val_0_auc: 0.90417 |  0:05:53s\n",
      "epoch 120| loss: 0.29711 | val_0_auc: 0.90382 |  0:05:56s\n",
      "epoch 121| loss: 0.29545 | val_0_auc: 0.90309 |  0:05:59s\n",
      "epoch 122| loss: 0.29296 | val_0_auc: 0.90473 |  0:06:02s\n",
      "epoch 123| loss: 0.29564 | val_0_auc: 0.90466 |  0:06:04s\n",
      "epoch 124| loss: 0.29448 | val_0_auc: 0.90516 |  0:06:07s\n",
      "epoch 125| loss: 0.29662 | val_0_auc: 0.90569 |  0:06:10s\n",
      "epoch 126| loss: 0.2943  | val_0_auc: 0.9058  |  0:06:13s\n",
      "epoch 127| loss: 0.29149 | val_0_auc: 0.90707 |  0:06:16s\n",
      "epoch 128| loss: 0.28948 | val_0_auc: 0.90519 |  0:06:19s\n",
      "epoch 129| loss: 0.29269 | val_0_auc: 0.90447 |  0:06:22s\n",
      "epoch 130| loss: 0.28869 | val_0_auc: 0.9056  |  0:06:25s\n",
      "epoch 131| loss: 0.29244 | val_0_auc: 0.90597 |  0:06:28s\n",
      "epoch 132| loss: 0.29124 | val_0_auc: 0.90698 |  0:06:31s\n",
      "epoch 133| loss: 0.29206 | val_0_auc: 0.90638 |  0:06:34s\n",
      "epoch 134| loss: 0.29354 | val_0_auc: 0.90586 |  0:06:37s\n",
      "epoch 135| loss: 0.29276 | val_0_auc: 0.90443 |  0:06:40s\n",
      "epoch 136| loss: 0.29539 | val_0_auc: 0.90562 |  0:06:43s\n",
      "epoch 137| loss: 0.29633 | val_0_auc: 0.90532 |  0:06:45s\n",
      "epoch 138| loss: 0.29525 | val_0_auc: 0.90568 |  0:06:48s\n",
      "epoch 139| loss: 0.29356 | val_0_auc: 0.90675 |  0:06:51s\n",
      "epoch 140| loss: 0.29062 | val_0_auc: 0.90751 |  0:06:54s\n",
      "epoch 141| loss: 0.29053 | val_0_auc: 0.90695 |  0:06:57s\n",
      "epoch 142| loss: 0.29015 | val_0_auc: 0.90638 |  0:07:00s\n",
      "epoch 143| loss: 0.29055 | val_0_auc: 0.9065  |  0:07:03s\n",
      "epoch 144| loss: 0.28887 | val_0_auc: 0.90707 |  0:07:06s\n",
      "epoch 145| loss: 0.28843 | val_0_auc: 0.90532 |  0:07:09s\n",
      "epoch 146| loss: 0.28974 | val_0_auc: 0.90489 |  0:07:12s\n",
      "epoch 147| loss: 0.28789 | val_0_auc: 0.90489 |  0:07:14s\n",
      "epoch 148| loss: 0.28701 | val_0_auc: 0.90467 |  0:07:17s\n",
      "epoch 149| loss: 0.28726 | val_0_auc: 0.90501 |  0:07:20s\n",
      "epoch 150| loss: 0.28678 | val_0_auc: 0.90572 |  0:07:23s\n",
      "epoch 151| loss: 0.2861  | val_0_auc: 0.90597 |  0:07:26s\n",
      "epoch 152| loss: 0.2859  | val_0_auc: 0.90655 |  0:07:29s\n",
      "epoch 153| loss: 0.28393 | val_0_auc: 0.9066  |  0:07:32s\n",
      "epoch 154| loss: 0.28272 | val_0_auc: 0.90668 |  0:07:35s\n",
      "epoch 155| loss: 0.28282 | val_0_auc: 0.90514 |  0:07:38s\n",
      "epoch 156| loss: 0.28199 | val_0_auc: 0.9062  |  0:07:41s\n",
      "epoch 157| loss: 0.28306 | val_0_auc: 0.90642 |  0:07:43s\n",
      "epoch 158| loss: 0.28303 | val_0_auc: 0.90625 |  0:07:46s\n",
      "epoch 159| loss: 0.28202 | val_0_auc: 0.90653 |  0:07:49s\n",
      "epoch 160| loss: 0.28273 | val_0_auc: 0.90735 |  0:07:52s\n",
      "\n",
      "Early stopping occurred at epoch 160 with best_epoch = 140 and best_val_0_auc = 0.90751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.97081 | val_0_auc: 0.4691  |  0:00:02s\n",
      "epoch 1  | loss: 0.79969 | val_0_auc: 0.50489 |  0:00:04s\n",
      "epoch 2  | loss: 0.75745 | val_0_auc: 0.51766 |  0:00:06s\n",
      "epoch 3  | loss: 0.72937 | val_0_auc: 0.52368 |  0:00:08s\n",
      "epoch 4  | loss: 0.71522 | val_0_auc: 0.51904 |  0:00:10s\n",
      "epoch 5  | loss: 0.70827 | val_0_auc: 0.5222  |  0:00:12s\n",
      "epoch 6  | loss: 0.68212 | val_0_auc: 0.54223 |  0:00:14s\n",
      "epoch 7  | loss: 0.63415 | val_0_auc: 0.56282 |  0:00:16s\n",
      "epoch 8  | loss: 0.54913 | val_0_auc: 0.61558 |  0:00:18s\n",
      "epoch 9  | loss: 0.52291 | val_0_auc: 0.64196 |  0:00:20s\n",
      "epoch 10 | loss: 0.51681 | val_0_auc: 0.64064 |  0:00:22s\n",
      "epoch 11 | loss: 0.51048 | val_0_auc: 0.64687 |  0:00:24s\n",
      "epoch 12 | loss: 0.50628 | val_0_auc: 0.65195 |  0:00:26s\n",
      "epoch 13 | loss: 0.50195 | val_0_auc: 0.65774 |  0:00:28s\n",
      "epoch 14 | loss: 0.49923 | val_0_auc: 0.65471 |  0:00:30s\n",
      "epoch 15 | loss: 0.49754 | val_0_auc: 0.66049 |  0:00:32s\n",
      "epoch 16 | loss: 0.49209 | val_0_auc: 0.66723 |  0:00:34s\n",
      "epoch 17 | loss: 0.48828 | val_0_auc: 0.68066 |  0:00:36s\n",
      "epoch 18 | loss: 0.48403 | val_0_auc: 0.68448 |  0:00:38s\n",
      "epoch 19 | loss: 0.47973 | val_0_auc: 0.6955  |  0:00:40s\n",
      "epoch 20 | loss: 0.47681 | val_0_auc: 0.69107 |  0:00:42s\n",
      "epoch 21 | loss: 0.47127 | val_0_auc: 0.70317 |  0:00:44s\n",
      "epoch 22 | loss: 0.46553 | val_0_auc: 0.71274 |  0:00:46s\n",
      "epoch 23 | loss: 0.4602  | val_0_auc: 0.72786 |  0:00:48s\n",
      "epoch 24 | loss: 0.45456 | val_0_auc: 0.73623 |  0:00:50s\n",
      "epoch 25 | loss: 0.44889 | val_0_auc: 0.75178 |  0:00:52s\n",
      "epoch 26 | loss: 0.43787 | val_0_auc: 0.77902 |  0:00:55s\n",
      "epoch 27 | loss: 0.42249 | val_0_auc: 0.81314 |  0:00:56s\n",
      "epoch 28 | loss: 0.39971 | val_0_auc: 0.83543 |  0:00:58s\n",
      "epoch 29 | loss: 0.38345 | val_0_auc: 0.85053 |  0:01:01s\n",
      "epoch 30 | loss: 0.3749  | val_0_auc: 0.8568  |  0:01:03s\n",
      "epoch 31 | loss: 0.36686 | val_0_auc: 0.86448 |  0:01:05s\n",
      "epoch 32 | loss: 0.36057 | val_0_auc: 0.86458 |  0:01:07s\n",
      "epoch 33 | loss: 0.3533  | val_0_auc: 0.86485 |  0:01:09s\n",
      "epoch 34 | loss: 0.34546 | val_0_auc: 0.87194 |  0:01:10s\n",
      "epoch 35 | loss: 0.3436  | val_0_auc: 0.87221 |  0:01:13s\n",
      "epoch 36 | loss: 0.33938 | val_0_auc: 0.87181 |  0:01:15s\n",
      "epoch 37 | loss: 0.33209 | val_0_auc: 0.87894 |  0:01:16s\n",
      "epoch 38 | loss: 0.32876 | val_0_auc: 0.87981 |  0:01:19s\n",
      "epoch 39 | loss: 0.32894 | val_0_auc: 0.88145 |  0:01:21s\n",
      "epoch 40 | loss: 0.32362 | val_0_auc: 0.88935 |  0:01:23s\n",
      "epoch 41 | loss: 0.31783 | val_0_auc: 0.89695 |  0:01:25s\n",
      "epoch 42 | loss: 0.31134 | val_0_auc: 0.89641 |  0:01:27s\n",
      "epoch 43 | loss: 0.30839 | val_0_auc: 0.89878 |  0:01:29s\n",
      "epoch 44 | loss: 0.30657 | val_0_auc: 0.89979 |  0:01:31s\n",
      "epoch 45 | loss: 0.30814 | val_0_auc: 0.8995  |  0:01:33s\n",
      "epoch 46 | loss: 0.30521 | val_0_auc: 0.89846 |  0:01:35s\n",
      "epoch 47 | loss: 0.30255 | val_0_auc: 0.90107 |  0:01:37s\n",
      "epoch 48 | loss: 0.30156 | val_0_auc: 0.90236 |  0:01:39s\n",
      "epoch 49 | loss: 0.30031 | val_0_auc: 0.90175 |  0:01:41s\n",
      "epoch 50 | loss: 0.30636 | val_0_auc: 0.89953 |  0:01:43s\n",
      "epoch 51 | loss: 0.30276 | val_0_auc: 0.9     |  0:01:45s\n",
      "epoch 52 | loss: 0.30014 | val_0_auc: 0.90156 |  0:01:47s\n",
      "epoch 53 | loss: 0.29526 | val_0_auc: 0.90213 |  0:01:49s\n",
      "epoch 54 | loss: 0.29688 | val_0_auc: 0.90183 |  0:01:51s\n",
      "epoch 55 | loss: 0.29432 | val_0_auc: 0.90237 |  0:01:53s\n",
      "epoch 56 | loss: 0.29748 | val_0_auc: 0.90408 |  0:01:55s\n",
      "epoch 57 | loss: 0.29734 | val_0_auc: 0.90392 |  0:01:57s\n",
      "epoch 58 | loss: 0.29191 | val_0_auc: 0.90625 |  0:01:59s\n",
      "epoch 59 | loss: 0.29298 | val_0_auc: 0.90754 |  0:02:01s\n",
      "epoch 60 | loss: 0.29085 | val_0_auc: 0.91068 |  0:02:03s\n",
      "epoch 61 | loss: 0.29183 | val_0_auc: 0.91002 |  0:02:05s\n",
      "epoch 62 | loss: 0.2883  | val_0_auc: 0.9111  |  0:02:07s\n",
      "epoch 63 | loss: 0.28868 | val_0_auc: 0.90888 |  0:02:09s\n",
      "epoch 64 | loss: 0.28848 | val_0_auc: 0.91214 |  0:02:11s\n",
      "epoch 65 | loss: 0.28388 | val_0_auc: 0.91222 |  0:02:13s\n",
      "epoch 66 | loss: 0.28165 | val_0_auc: 0.91165 |  0:02:15s\n",
      "epoch 67 | loss: 0.28343 | val_0_auc: 0.91341 |  0:02:17s\n",
      "epoch 68 | loss: 0.28375 | val_0_auc: 0.91477 |  0:02:19s\n",
      "epoch 69 | loss: 0.28194 | val_0_auc: 0.91255 |  0:02:21s\n",
      "epoch 70 | loss: 0.28071 | val_0_auc: 0.91273 |  0:02:23s\n",
      "epoch 71 | loss: 0.28044 | val_0_auc: 0.91461 |  0:02:25s\n",
      "epoch 72 | loss: 0.27896 | val_0_auc: 0.91326 |  0:02:27s\n",
      "epoch 73 | loss: 0.27739 | val_0_auc: 0.91393 |  0:02:29s\n",
      "epoch 74 | loss: 0.27738 | val_0_auc: 0.91042 |  0:02:31s\n",
      "epoch 75 | loss: 0.27784 | val_0_auc: 0.9117  |  0:02:34s\n",
      "epoch 76 | loss: 0.28085 | val_0_auc: 0.90992 |  0:02:36s\n",
      "epoch 77 | loss: 0.27857 | val_0_auc: 0.91041 |  0:02:38s\n",
      "epoch 78 | loss: 0.27656 | val_0_auc: 0.90995 |  0:02:40s\n",
      "epoch 79 | loss: 0.27429 | val_0_auc: 0.91057 |  0:02:42s\n",
      "epoch 80 | loss: 0.27509 | val_0_auc: 0.90985 |  0:02:44s\n",
      "epoch 81 | loss: 0.27495 | val_0_auc: 0.91236 |  0:02:46s\n",
      "epoch 82 | loss: 0.2737  | val_0_auc: 0.91364 |  0:02:48s\n",
      "epoch 83 | loss: 0.27287 | val_0_auc: 0.91284 |  0:02:50s\n",
      "epoch 84 | loss: 0.27033 | val_0_auc: 0.91347 |  0:02:52s\n",
      "epoch 85 | loss: 0.27076 | val_0_auc: 0.90982 |  0:02:54s\n",
      "epoch 86 | loss: 0.27005 | val_0_auc: 0.91276 |  0:02:56s\n",
      "epoch 87 | loss: 0.27127 | val_0_auc: 0.91302 |  0:02:58s\n",
      "epoch 88 | loss: 0.27336 | val_0_auc: 0.9105  |  0:03:00s\n",
      "\n",
      "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_0_auc = 0.91477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78949 | val_0_auc: 0.66879 |  0:00:00s\n",
      "epoch 1  | loss: 0.59382 | val_0_auc: 0.78085 |  0:00:01s\n",
      "epoch 2  | loss: 0.50311 | val_0_auc: 0.8175  |  0:00:02s\n",
      "epoch 3  | loss: 0.46314 | val_0_auc: 0.82463 |  0:00:03s\n",
      "epoch 4  | loss: 0.43711 | val_0_auc: 0.82536 |  0:00:04s\n",
      "epoch 5  | loss: 0.42081 | val_0_auc: 0.83253 |  0:00:05s\n",
      "epoch 6  | loss: 0.40554 | val_0_auc: 0.83886 |  0:00:06s\n",
      "epoch 7  | loss: 0.39347 | val_0_auc: 0.84205 |  0:00:07s\n",
      "epoch 8  | loss: 0.3829  | val_0_auc: 0.85698 |  0:00:07s\n",
      "epoch 9  | loss: 0.37488 | val_0_auc: 0.86535 |  0:00:08s\n",
      "epoch 10 | loss: 0.36545 | val_0_auc: 0.86615 |  0:00:09s\n",
      "epoch 11 | loss: 0.36251 | val_0_auc: 0.87352 |  0:00:10s\n",
      "epoch 12 | loss: 0.35146 | val_0_auc: 0.87109 |  0:00:11s\n",
      "epoch 13 | loss: 0.34562 | val_0_auc: 0.8793  |  0:00:12s\n",
      "epoch 14 | loss: 0.33164 | val_0_auc: 0.89448 |  0:00:13s\n",
      "epoch 15 | loss: 0.32549 | val_0_auc: 0.89708 |  0:00:13s\n",
      "epoch 16 | loss: 0.31798 | val_0_auc: 0.89752 |  0:00:14s\n",
      "epoch 17 | loss: 0.31383 | val_0_auc: 0.90332 |  0:00:15s\n",
      "epoch 18 | loss: 0.31129 | val_0_auc: 0.90333 |  0:00:16s\n",
      "epoch 19 | loss: 0.3087  | val_0_auc: 0.90431 |  0:00:17s\n",
      "epoch 20 | loss: 0.3086  | val_0_auc: 0.90592 |  0:00:18s\n",
      "epoch 21 | loss: 0.3075  | val_0_auc: 0.9056  |  0:00:19s\n",
      "epoch 22 | loss: 0.304   | val_0_auc: 0.905   |  0:00:20s\n",
      "epoch 23 | loss: 0.3042  | val_0_auc: 0.90528 |  0:00:21s\n",
      "epoch 24 | loss: 0.30142 | val_0_auc: 0.90772 |  0:00:22s\n",
      "epoch 25 | loss: 0.3018  | val_0_auc: 0.90733 |  0:00:22s\n",
      "epoch 26 | loss: 0.29926 | val_0_auc: 0.90939 |  0:00:23s\n",
      "epoch 27 | loss: 0.2978  | val_0_auc: 0.90423 |  0:00:24s\n",
      "epoch 28 | loss: 0.29936 | val_0_auc: 0.90883 |  0:00:25s\n",
      "epoch 29 | loss: 0.29739 | val_0_auc: 0.90789 |  0:00:26s\n",
      "epoch 30 | loss: 0.29566 | val_0_auc: 0.90995 |  0:00:27s\n",
      "epoch 31 | loss: 0.29501 | val_0_auc: 0.90949 |  0:00:28s\n",
      "epoch 32 | loss: 0.29076 | val_0_auc: 0.90997 |  0:00:28s\n",
      "epoch 33 | loss: 0.29271 | val_0_auc: 0.90744 |  0:00:29s\n",
      "epoch 34 | loss: 0.29008 | val_0_auc: 0.90964 |  0:00:30s\n",
      "epoch 35 | loss: 0.29183 | val_0_auc: 0.90823 |  0:00:31s\n",
      "epoch 36 | loss: 0.29175 | val_0_auc: 0.90904 |  0:00:32s\n",
      "epoch 37 | loss: 0.29126 | val_0_auc: 0.90691 |  0:00:33s\n",
      "epoch 38 | loss: 0.28913 | val_0_auc: 0.90814 |  0:00:34s\n",
      "epoch 39 | loss: 0.28708 | val_0_auc: 0.905   |  0:00:35s\n",
      "epoch 40 | loss: 0.28988 | val_0_auc: 0.90733 |  0:00:36s\n",
      "epoch 41 | loss: 0.28833 | val_0_auc: 0.90649 |  0:00:36s\n",
      "epoch 42 | loss: 0.28514 | val_0_auc: 0.90523 |  0:00:37s\n",
      "epoch 43 | loss: 0.28658 | val_0_auc: 0.90656 |  0:00:38s\n",
      "epoch 44 | loss: 0.28722 | val_0_auc: 0.90546 |  0:00:39s\n",
      "epoch 45 | loss: 0.28446 | val_0_auc: 0.90354 |  0:00:40s\n",
      "epoch 46 | loss: 0.28514 | val_0_auc: 0.90785 |  0:00:41s\n",
      "epoch 47 | loss: 0.28582 | val_0_auc: 0.90463 |  0:00:42s\n",
      "epoch 48 | loss: 0.28174 | val_0_auc: 0.90642 |  0:00:42s\n",
      "epoch 49 | loss: 0.28095 | val_0_auc: 0.90633 |  0:00:43s\n",
      "epoch 50 | loss: 0.28077 | val_0_auc: 0.90693 |  0:00:44s\n",
      "epoch 51 | loss: 0.28033 | val_0_auc: 0.90838 |  0:00:45s\n",
      "epoch 52 | loss: 0.28129 | val_0_auc: 0.9093  |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_0_auc = 0.90997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.56156 | val_0_auc: 0.68537 |  0:00:01s\n",
      "epoch 1  | loss: 0.4349  | val_0_auc: 0.80781 |  0:00:02s\n",
      "epoch 2  | loss: 0.36276 | val_0_auc: 0.85228 |  0:00:04s\n",
      "epoch 3  | loss: 0.33206 | val_0_auc: 0.87615 |  0:00:05s\n",
      "epoch 4  | loss: 0.31045 | val_0_auc: 0.89141 |  0:00:07s\n",
      "epoch 5  | loss: 0.29879 | val_0_auc: 0.89483 |  0:00:08s\n",
      "epoch 6  | loss: 0.29183 | val_0_auc: 0.90489 |  0:00:09s\n",
      "epoch 7  | loss: 0.28269 | val_0_auc: 0.90977 |  0:00:11s\n",
      "epoch 8  | loss: 0.27977 | val_0_auc: 0.9101  |  0:00:12s\n",
      "epoch 9  | loss: 0.2739  | val_0_auc: 0.91271 |  0:00:14s\n",
      "epoch 10 | loss: 0.26824 | val_0_auc: 0.91389 |  0:00:15s\n",
      "epoch 11 | loss: 0.26893 | val_0_auc: 0.91722 |  0:00:17s\n",
      "epoch 12 | loss: 0.26374 | val_0_auc: 0.91432 |  0:00:18s\n",
      "epoch 13 | loss: 0.2615  | val_0_auc: 0.91705 |  0:00:19s\n",
      "epoch 14 | loss: 0.2601  | val_0_auc: 0.91507 |  0:00:21s\n",
      "epoch 15 | loss: 0.26289 | val_0_auc: 0.91736 |  0:00:22s\n",
      "epoch 16 | loss: 0.26125 | val_0_auc: 0.91587 |  0:00:24s\n",
      "epoch 17 | loss: 0.26021 | val_0_auc: 0.91491 |  0:00:25s\n",
      "epoch 18 | loss: 0.25602 | val_0_auc: 0.91447 |  0:00:27s\n",
      "epoch 19 | loss: 0.25516 | val_0_auc: 0.91057 |  0:00:28s\n",
      "epoch 20 | loss: 0.26277 | val_0_auc: 0.90865 |  0:00:29s\n",
      "epoch 21 | loss: 0.26039 | val_0_auc: 0.91065 |  0:00:31s\n",
      "epoch 22 | loss: 0.25295 | val_0_auc: 0.91386 |  0:00:32s\n",
      "epoch 23 | loss: 0.24952 | val_0_auc: 0.91377 |  0:00:34s\n",
      "epoch 24 | loss: 0.24938 | val_0_auc: 0.91328 |  0:00:35s\n",
      "epoch 25 | loss: 0.24626 | val_0_auc: 0.91125 |  0:00:36s\n",
      "epoch 26 | loss: 0.24879 | val_0_auc: 0.91032 |  0:00:38s\n",
      "epoch 27 | loss: 0.24294 | val_0_auc: 0.91119 |  0:00:40s\n",
      "epoch 28 | loss: 0.2415  | val_0_auc: 0.91222 |  0:00:41s\n",
      "epoch 29 | loss: 0.23851 | val_0_auc: 0.90605 |  0:00:43s\n",
      "epoch 30 | loss: 0.23371 | val_0_auc: 0.90942 |  0:00:44s\n",
      "epoch 31 | loss: 0.23134 | val_0_auc: 0.9077  |  0:00:45s\n",
      "epoch 32 | loss: 0.23188 | val_0_auc: 0.90465 |  0:00:47s\n",
      "epoch 33 | loss: 0.23817 | val_0_auc: 0.90872 |  0:00:48s\n",
      "epoch 34 | loss: 0.23561 | val_0_auc: 0.90849 |  0:00:50s\n",
      "epoch 35 | loss: 0.22787 | val_0_auc: 0.90304 |  0:00:51s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.91736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.82628 | val_0_auc: 0.542   |  0:00:05s\n",
      "epoch 1  | loss: 0.63751 | val_0_auc: 0.54516 |  0:00:10s\n",
      "epoch 2  | loss: 0.56838 | val_0_auc: 0.58124 |  0:00:15s\n",
      "epoch 3  | loss: 0.54111 | val_0_auc: 0.61451 |  0:00:20s\n",
      "epoch 4  | loss: 0.54591 | val_0_auc: 0.64631 |  0:00:26s\n",
      "epoch 5  | loss: 0.52442 | val_0_auc: 0.65177 |  0:00:31s\n",
      "epoch 6  | loss: 0.50752 | val_0_auc: 0.70938 |  0:00:37s\n",
      "epoch 7  | loss: 0.50477 | val_0_auc: 0.7312  |  0:00:42s\n",
      "epoch 8  | loss: 0.48693 | val_0_auc: 0.73528 |  0:00:47s\n",
      "epoch 9  | loss: 0.47993 | val_0_auc: 0.75019 |  0:00:52s\n",
      "epoch 10 | loss: 0.46872 | val_0_auc: 0.7725  |  0:00:58s\n",
      "epoch 11 | loss: 0.46325 | val_0_auc: 0.7944  |  0:01:03s\n",
      "epoch 12 | loss: 0.42999 | val_0_auc: 0.81464 |  0:01:08s\n",
      "epoch 13 | loss: 0.40717 | val_0_auc: 0.82537 |  0:01:13s\n",
      "epoch 14 | loss: 0.40934 | val_0_auc: 0.83117 |  0:01:19s\n",
      "epoch 15 | loss: 0.39653 | val_0_auc: 0.81825 |  0:01:24s\n",
      "epoch 16 | loss: 0.39115 | val_0_auc: 0.82856 |  0:01:30s\n",
      "epoch 17 | loss: 0.39559 | val_0_auc: 0.8366  |  0:01:35s\n",
      "epoch 18 | loss: 0.39473 | val_0_auc: 0.83716 |  0:01:40s\n",
      "epoch 19 | loss: 0.38598 | val_0_auc: 0.84579 |  0:01:45s\n",
      "epoch 20 | loss: 0.38433 | val_0_auc: 0.8536  |  0:01:50s\n",
      "epoch 21 | loss: 0.38482 | val_0_auc: 0.85836 |  0:01:56s\n",
      "epoch 22 | loss: 0.374   | val_0_auc: 0.85916 |  0:02:01s\n",
      "epoch 23 | loss: 0.37426 | val_0_auc: 0.85408 |  0:02:06s\n",
      "epoch 24 | loss: 0.37209 | val_0_auc: 0.85547 |  0:02:11s\n",
      "epoch 25 | loss: 0.37027 | val_0_auc: 0.85074 |  0:02:18s\n",
      "epoch 26 | loss: 0.36322 | val_0_auc: 0.85057 |  0:02:23s\n",
      "epoch 27 | loss: 0.35658 | val_0_auc: 0.87025 |  0:02:28s\n",
      "epoch 28 | loss: 0.35773 | val_0_auc: 0.85827 |  0:02:33s\n",
      "epoch 29 | loss: 0.34857 | val_0_auc: 0.87187 |  0:02:38s\n",
      "epoch 30 | loss: 0.34628 | val_0_auc: 0.87504 |  0:02:44s\n",
      "epoch 31 | loss: 0.34786 | val_0_auc: 0.87374 |  0:02:49s\n",
      "epoch 32 | loss: 0.35004 | val_0_auc: 0.87474 |  0:02:54s\n",
      "epoch 33 | loss: 0.33928 | val_0_auc: 0.87468 |  0:02:59s\n",
      "epoch 34 | loss: 0.33749 | val_0_auc: 0.88015 |  0:03:05s\n",
      "epoch 35 | loss: 0.33542 | val_0_auc: 0.88295 |  0:03:10s\n",
      "epoch 36 | loss: 0.33713 | val_0_auc: 0.876   |  0:03:15s\n",
      "epoch 37 | loss: 0.33794 | val_0_auc: 0.8826  |  0:03:20s\n",
      "epoch 38 | loss: 0.33736 | val_0_auc: 0.88246 |  0:03:25s\n",
      "epoch 39 | loss: 0.33135 | val_0_auc: 0.8861  |  0:03:30s\n",
      "epoch 40 | loss: 0.32779 | val_0_auc: 0.89084 |  0:03:36s\n",
      "epoch 41 | loss: 0.32798 | val_0_auc: 0.88093 |  0:03:41s\n",
      "epoch 42 | loss: 0.33715 | val_0_auc: 0.88258 |  0:03:46s\n",
      "epoch 43 | loss: 0.33266 | val_0_auc: 0.88757 |  0:03:51s\n",
      "epoch 44 | loss: 0.32608 | val_0_auc: 0.887   |  0:03:56s\n",
      "epoch 45 | loss: 0.3217  | val_0_auc: 0.88927 |  0:04:01s\n",
      "epoch 46 | loss: 0.32479 | val_0_auc: 0.88896 |  0:04:07s\n",
      "epoch 47 | loss: 0.31836 | val_0_auc: 0.89046 |  0:04:12s\n",
      "epoch 48 | loss: 0.31801 | val_0_auc: 0.89358 |  0:04:17s\n",
      "epoch 49 | loss: 0.3116  | val_0_auc: 0.89097 |  0:04:22s\n",
      "epoch 50 | loss: 0.31486 | val_0_auc: 0.89304 |  0:04:27s\n",
      "epoch 51 | loss: 0.30827 | val_0_auc: 0.89658 |  0:04:32s\n",
      "epoch 52 | loss: 0.30592 | val_0_auc: 0.89674 |  0:04:38s\n",
      "epoch 53 | loss: 0.30754 | val_0_auc: 0.89726 |  0:04:43s\n",
      "epoch 54 | loss: 0.30597 | val_0_auc: 0.90179 |  0:04:48s\n",
      "epoch 55 | loss: 0.29433 | val_0_auc: 0.90626 |  0:04:53s\n",
      "epoch 56 | loss: 0.29107 | val_0_auc: 0.90703 |  0:04:59s\n",
      "epoch 57 | loss: 0.29009 | val_0_auc: 0.90617 |  0:05:04s\n",
      "epoch 58 | loss: 0.28883 | val_0_auc: 0.90922 |  0:05:09s\n",
      "epoch 59 | loss: 0.29068 | val_0_auc: 0.90901 |  0:05:14s\n",
      "epoch 60 | loss: 0.28907 | val_0_auc: 0.90914 |  0:05:19s\n",
      "epoch 61 | loss: 0.29319 | val_0_auc: 0.90214 |  0:05:24s\n",
      "epoch 62 | loss: 0.2927  | val_0_auc: 0.90402 |  0:05:30s\n",
      "epoch 63 | loss: 0.28519 | val_0_auc: 0.90843 |  0:05:35s\n",
      "epoch 64 | loss: 0.2846  | val_0_auc: 0.90839 |  0:05:40s\n",
      "epoch 65 | loss: 0.28083 | val_0_auc: 0.91167 |  0:05:46s\n",
      "epoch 66 | loss: 0.2754  | val_0_auc: 0.90937 |  0:05:51s\n",
      "epoch 67 | loss: 0.2779  | val_0_auc: 0.90735 |  0:05:57s\n",
      "epoch 68 | loss: 0.28576 | val_0_auc: 0.90091 |  0:06:02s\n",
      "epoch 69 | loss: 0.29492 | val_0_auc: 0.906   |  0:06:07s\n",
      "epoch 70 | loss: 0.28632 | val_0_auc: 0.90796 |  0:06:12s\n",
      "epoch 71 | loss: 0.29022 | val_0_auc: 0.9086  |  0:06:17s\n",
      "epoch 72 | loss: 0.28651 | val_0_auc: 0.90966 |  0:06:22s\n",
      "epoch 73 | loss: 0.28234 | val_0_auc: 0.91229 |  0:06:28s\n",
      "epoch 74 | loss: 0.28076 | val_0_auc: 0.90995 |  0:06:33s\n",
      "epoch 75 | loss: 0.28048 | val_0_auc: 0.91112 |  0:06:38s\n",
      "epoch 76 | loss: 0.28456 | val_0_auc: 0.91449 |  0:06:43s\n",
      "epoch 77 | loss: 0.28089 | val_0_auc: 0.91491 |  0:06:48s\n",
      "epoch 78 | loss: 0.28361 | val_0_auc: 0.90652 |  0:06:54s\n",
      "epoch 79 | loss: 0.28888 | val_0_auc: 0.91007 |  0:06:59s\n",
      "epoch 80 | loss: 0.28157 | val_0_auc: 0.91216 |  0:07:04s\n",
      "epoch 81 | loss: 0.27911 | val_0_auc: 0.91344 |  0:07:09s\n",
      "epoch 82 | loss: 0.27729 | val_0_auc: 0.91552 |  0:07:14s\n",
      "epoch 83 | loss: 0.27673 | val_0_auc: 0.91554 |  0:07:19s\n",
      "epoch 84 | loss: 0.2749  | val_0_auc: 0.91612 |  0:07:24s\n",
      "epoch 85 | loss: 0.27547 | val_0_auc: 0.91604 |  0:07:29s\n",
      "epoch 86 | loss: 0.27188 | val_0_auc: 0.91688 |  0:07:34s\n",
      "epoch 87 | loss: 0.27152 | val_0_auc: 0.91546 |  0:07:40s\n",
      "epoch 88 | loss: 0.2689  | val_0_auc: 0.91638 |  0:07:45s\n",
      "epoch 89 | loss: 0.26994 | val_0_auc: 0.91557 |  0:07:50s\n",
      "epoch 90 | loss: 0.27072 | val_0_auc: 0.91643 |  0:07:55s\n",
      "epoch 91 | loss: 0.26746 | val_0_auc: 0.91743 |  0:08:00s\n",
      "epoch 92 | loss: 0.26935 | val_0_auc: 0.91489 |  0:08:05s\n",
      "epoch 93 | loss: 0.27629 | val_0_auc: 0.91146 |  0:08:10s\n",
      "epoch 94 | loss: 0.28144 | val_0_auc: 0.91064 |  0:08:16s\n",
      "epoch 95 | loss: 0.28511 | val_0_auc: 0.90835 |  0:08:21s\n",
      "epoch 96 | loss: 0.28313 | val_0_auc: 0.90929 |  0:08:26s\n",
      "epoch 97 | loss: 0.29096 | val_0_auc: 0.90804 |  0:08:31s\n",
      "epoch 98 | loss: 0.29134 | val_0_auc: 0.90782 |  0:08:36s\n",
      "epoch 99 | loss: 0.28152 | val_0_auc: 0.91399 |  0:08:41s\n",
      "epoch 100| loss: 0.28217 | val_0_auc: 0.91022 |  0:08:47s\n",
      "epoch 101| loss: 0.28847 | val_0_auc: 0.90792 |  0:08:52s\n",
      "epoch 102| loss: 0.28432 | val_0_auc: 0.91179 |  0:08:57s\n",
      "epoch 103| loss: 0.28802 | val_0_auc: 0.91072 |  0:09:02s\n",
      "epoch 104| loss: 0.28857 | val_0_auc: 0.90834 |  0:09:07s\n",
      "epoch 105| loss: 0.28384 | val_0_auc: 0.91293 |  0:09:12s\n",
      "epoch 106| loss: 0.28542 | val_0_auc: 0.91338 |  0:09:18s\n",
      "epoch 107| loss: 0.28243 | val_0_auc: 0.91041 |  0:09:23s\n",
      "epoch 108| loss: 0.28131 | val_0_auc: 0.91211 |  0:09:28s\n",
      "epoch 109| loss: 0.29388 | val_0_auc: 0.90714 |  0:09:33s\n",
      "epoch 110| loss: 0.29413 | val_0_auc: 0.90661 |  0:09:38s\n",
      "epoch 111| loss: 0.29587 | val_0_auc: 0.90551 |  0:09:43s\n",
      "\n",
      "Early stopping occurred at epoch 111 with best_epoch = 91 and best_val_0_auc = 0.91743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.0649  | val_0_auc: 0.49834 |  0:00:06s\n",
      "epoch 1  | loss: 1.02859 | val_0_auc: 0.49757 |  0:00:12s\n",
      "epoch 2  | loss: 1.00862 | val_0_auc: 0.49891 |  0:00:19s\n",
      "epoch 3  | loss: 0.97868 | val_0_auc: 0.51425 |  0:00:25s\n",
      "epoch 4  | loss: 0.96648 | val_0_auc: 0.51705 |  0:00:32s\n",
      "epoch 5  | loss: 0.94018 | val_0_auc: 0.49563 |  0:00:39s\n",
      "epoch 6  | loss: 0.91783 | val_0_auc: 0.51317 |  0:00:45s\n",
      "epoch 7  | loss: 0.90684 | val_0_auc: 0.50705 |  0:00:51s\n",
      "epoch 8  | loss: 0.90066 | val_0_auc: 0.5048  |  0:00:58s\n",
      "epoch 9  | loss: 0.90174 | val_0_auc: 0.50233 |  0:01:04s\n",
      "epoch 10 | loss: 0.87604 | val_0_auc: 0.51363 |  0:01:11s\n",
      "epoch 11 | loss: 0.87616 | val_0_auc: 0.51208 |  0:01:17s\n",
      "epoch 12 | loss: 0.86244 | val_0_auc: 0.52377 |  0:01:23s\n",
      "epoch 13 | loss: 0.84925 | val_0_auc: 0.52757 |  0:01:30s\n",
      "epoch 14 | loss: 0.83567 | val_0_auc: 0.541   |  0:01:36s\n",
      "epoch 15 | loss: 0.81616 | val_0_auc: 0.56517 |  0:01:43s\n",
      "epoch 16 | loss: 0.81077 | val_0_auc: 0.55442 |  0:01:49s\n",
      "epoch 17 | loss: 0.80419 | val_0_auc: 0.57313 |  0:01:55s\n",
      "epoch 18 | loss: 0.79109 | val_0_auc: 0.55964 |  0:02:02s\n",
      "epoch 19 | loss: 0.77783 | val_0_auc: 0.5864  |  0:02:08s\n",
      "epoch 20 | loss: 0.7773  | val_0_auc: 0.58169 |  0:02:15s\n",
      "epoch 21 | loss: 0.7649  | val_0_auc: 0.58498 |  0:02:21s\n",
      "epoch 22 | loss: 0.7585  | val_0_auc: 0.5932  |  0:02:28s\n",
      "epoch 23 | loss: 0.74715 | val_0_auc: 0.59009 |  0:02:35s\n",
      "epoch 24 | loss: 0.74569 | val_0_auc: 0.6033  |  0:02:41s\n",
      "epoch 25 | loss: 0.73945 | val_0_auc: 0.59247 |  0:02:47s\n",
      "epoch 26 | loss: 0.73524 | val_0_auc: 0.59242 |  0:02:54s\n",
      "epoch 27 | loss: 0.72905 | val_0_auc: 0.62528 |  0:03:01s\n",
      "epoch 28 | loss: 0.72832 | val_0_auc: 0.61968 |  0:03:08s\n",
      "epoch 29 | loss: 0.72024 | val_0_auc: 0.63073 |  0:03:14s\n",
      "epoch 30 | loss: 0.72489 | val_0_auc: 0.60578 |  0:03:20s\n",
      "epoch 31 | loss: 0.71772 | val_0_auc: 0.61479 |  0:03:27s\n",
      "epoch 32 | loss: 0.71737 | val_0_auc: 0.63192 |  0:03:33s\n",
      "epoch 33 | loss: 0.71689 | val_0_auc: 0.62649 |  0:03:40s\n",
      "epoch 34 | loss: 0.70872 | val_0_auc: 0.64693 |  0:03:46s\n",
      "epoch 35 | loss: 0.70376 | val_0_auc: 0.65384 |  0:03:52s\n",
      "epoch 36 | loss: 0.70014 | val_0_auc: 0.66688 |  0:03:59s\n",
      "epoch 37 | loss: 0.69857 | val_0_auc: 0.6535  |  0:04:05s\n",
      "epoch 38 | loss: 0.69939 | val_0_auc: 0.65141 |  0:04:12s\n",
      "epoch 39 | loss: 0.70232 | val_0_auc: 0.65997 |  0:04:18s\n",
      "epoch 40 | loss: 0.7007  | val_0_auc: 0.65952 |  0:04:24s\n",
      "epoch 41 | loss: 0.6973  | val_0_auc: 0.65595 |  0:04:31s\n",
      "epoch 42 | loss: 0.69374 | val_0_auc: 0.65534 |  0:04:37s\n",
      "epoch 43 | loss: 0.69823 | val_0_auc: 0.65568 |  0:04:43s\n",
      "epoch 44 | loss: 0.69155 | val_0_auc: 0.65732 |  0:04:50s\n",
      "epoch 45 | loss: 0.69282 | val_0_auc: 0.65923 |  0:04:56s\n",
      "epoch 46 | loss: 0.68458 | val_0_auc: 0.66875 |  0:05:03s\n",
      "epoch 47 | loss: 0.68559 | val_0_auc: 0.6622  |  0:05:09s\n",
      "epoch 48 | loss: 0.68394 | val_0_auc: 0.6588  |  0:05:16s\n",
      "epoch 49 | loss: 0.68533 | val_0_auc: 0.64487 |  0:05:22s\n",
      "epoch 50 | loss: 0.68383 | val_0_auc: 0.65488 |  0:05:29s\n",
      "epoch 51 | loss: 0.6872  | val_0_auc: 0.65801 |  0:05:35s\n",
      "epoch 52 | loss: 0.68352 | val_0_auc: 0.66567 |  0:05:42s\n",
      "epoch 53 | loss: 0.68129 | val_0_auc: 0.66328 |  0:05:48s\n",
      "epoch 54 | loss: 0.67738 | val_0_auc: 0.66941 |  0:05:55s\n",
      "epoch 55 | loss: 0.67837 | val_0_auc: 0.67394 |  0:06:01s\n",
      "epoch 56 | loss: 0.67285 | val_0_auc: 0.66575 |  0:06:07s\n",
      "epoch 57 | loss: 0.67728 | val_0_auc: 0.66135 |  0:06:14s\n",
      "epoch 58 | loss: 0.67274 | val_0_auc: 0.67134 |  0:06:20s\n",
      "epoch 59 | loss: 0.67236 | val_0_auc: 0.68423 |  0:06:26s\n",
      "epoch 60 | loss: 0.67054 | val_0_auc: 0.69271 |  0:06:33s\n",
      "epoch 61 | loss: 0.66492 | val_0_auc: 0.682   |  0:06:39s\n",
      "epoch 62 | loss: 0.66157 | val_0_auc: 0.69245 |  0:06:46s\n",
      "epoch 63 | loss: 0.66006 | val_0_auc: 0.69718 |  0:06:52s\n",
      "epoch 64 | loss: 0.66135 | val_0_auc: 0.68985 |  0:06:58s\n",
      "epoch 65 | loss: 0.65782 | val_0_auc: 0.68218 |  0:07:05s\n",
      "epoch 66 | loss: 0.6591  | val_0_auc: 0.692   |  0:07:11s\n",
      "epoch 67 | loss: 0.65868 | val_0_auc: 0.68954 |  0:07:18s\n",
      "epoch 68 | loss: 0.65576 | val_0_auc: 0.69184 |  0:07:24s\n",
      "epoch 69 | loss: 0.65677 | val_0_auc: 0.69745 |  0:07:31s\n",
      "epoch 70 | loss: 0.65316 | val_0_auc: 0.69886 |  0:07:37s\n",
      "epoch 71 | loss: 0.65409 | val_0_auc: 0.70172 |  0:07:43s\n",
      "epoch 72 | loss: 0.65469 | val_0_auc: 0.7103  |  0:07:50s\n",
      "epoch 73 | loss: 0.64869 | val_0_auc: 0.7173  |  0:07:57s\n",
      "epoch 74 | loss: 0.64679 | val_0_auc: 0.72207 |  0:08:03s\n",
      "epoch 75 | loss: 0.6449  | val_0_auc: 0.72725 |  0:08:09s\n",
      "epoch 76 | loss: 0.63968 | val_0_auc: 0.72814 |  0:08:16s\n",
      "epoch 77 | loss: 0.63992 | val_0_auc: 0.72917 |  0:08:22s\n",
      "epoch 78 | loss: 0.63966 | val_0_auc: 0.73287 |  0:08:29s\n",
      "epoch 79 | loss: 0.63578 | val_0_auc: 0.74046 |  0:08:35s\n",
      "epoch 80 | loss: 0.63046 | val_0_auc: 0.73524 |  0:08:41s\n",
      "epoch 81 | loss: 0.62973 | val_0_auc: 0.73568 |  0:08:48s\n",
      "epoch 82 | loss: 0.62606 | val_0_auc: 0.75088 |  0:08:54s\n",
      "epoch 83 | loss: 0.62221 | val_0_auc: 0.74796 |  0:09:01s\n",
      "epoch 84 | loss: 0.62058 | val_0_auc: 0.75863 |  0:09:07s\n",
      "epoch 85 | loss: 0.61918 | val_0_auc: 0.7577  |  0:09:13s\n",
      "epoch 86 | loss: 0.61537 | val_0_auc: 0.75807 |  0:09:20s\n",
      "epoch 87 | loss: 0.61463 | val_0_auc: 0.76368 |  0:09:26s\n",
      "epoch 88 | loss: 0.61601 | val_0_auc: 0.75029 |  0:09:33s\n",
      "epoch 89 | loss: 0.61099 | val_0_auc: 0.75569 |  0:09:39s\n",
      "epoch 90 | loss: 0.60867 | val_0_auc: 0.74991 |  0:09:45s\n",
      "epoch 91 | loss: 0.61027 | val_0_auc: 0.75324 |  0:09:52s\n",
      "epoch 92 | loss: 0.60665 | val_0_auc: 0.75775 |  0:09:58s\n",
      "epoch 93 | loss: 0.60262 | val_0_auc: 0.75904 |  0:10:05s\n",
      "epoch 94 | loss: 0.60429 | val_0_auc: 0.76989 |  0:10:11s\n",
      "epoch 95 | loss: 0.60396 | val_0_auc: 0.76782 |  0:10:17s\n",
      "epoch 96 | loss: 0.60544 | val_0_auc: 0.76487 |  0:10:24s\n",
      "epoch 97 | loss: 0.60249 | val_0_auc: 0.77282 |  0:10:30s\n",
      "epoch 98 | loss: 0.60287 | val_0_auc: 0.76626 |  0:10:37s\n",
      "epoch 99 | loss: 0.59819 | val_0_auc: 0.77311 |  0:10:43s\n",
      "epoch 100| loss: 0.59931 | val_0_auc: 0.77054 |  0:10:50s\n",
      "epoch 101| loss: 0.59901 | val_0_auc: 0.76908 |  0:10:56s\n",
      "epoch 102| loss: 0.59485 | val_0_auc: 0.77219 |  0:11:03s\n",
      "epoch 103| loss: 0.59475 | val_0_auc: 0.76654 |  0:11:09s\n",
      "epoch 104| loss: 0.59161 | val_0_auc: 0.76999 |  0:11:15s\n",
      "epoch 105| loss: 0.58969 | val_0_auc: 0.77333 |  0:11:21s\n",
      "epoch 106| loss: 0.5853  | val_0_auc: 0.77817 |  0:11:28s\n",
      "epoch 107| loss: 0.58743 | val_0_auc: 0.77306 |  0:11:34s\n",
      "epoch 108| loss: 0.58403 | val_0_auc: 0.77971 |  0:11:41s\n",
      "epoch 109| loss: 0.58309 | val_0_auc: 0.78437 |  0:11:47s\n",
      "epoch 110| loss: 0.58132 | val_0_auc: 0.7816  |  0:11:53s\n",
      "epoch 111| loss: 0.5804  | val_0_auc: 0.78527 |  0:12:00s\n",
      "epoch 112| loss: 0.57826 | val_0_auc: 0.78887 |  0:12:06s\n",
      "epoch 113| loss: 0.57527 | val_0_auc: 0.7862  |  0:12:13s\n",
      "epoch 114| loss: 0.57514 | val_0_auc: 0.78529 |  0:12:19s\n",
      "epoch 115| loss: 0.5735  | val_0_auc: 0.78532 |  0:12:25s\n",
      "epoch 116| loss: 0.57908 | val_0_auc: 0.78435 |  0:12:33s\n",
      "epoch 117| loss: 0.57664 | val_0_auc: 0.78446 |  0:12:40s\n",
      "epoch 118| loss: 0.57764 | val_0_auc: 0.78549 |  0:12:46s\n",
      "epoch 119| loss: 0.57258 | val_0_auc: 0.78446 |  0:12:53s\n",
      "epoch 120| loss: 0.57154 | val_0_auc: 0.78743 |  0:12:59s\n",
      "epoch 121| loss: 0.57431 | val_0_auc: 0.78485 |  0:13:06s\n",
      "epoch 122| loss: 0.56833 | val_0_auc: 0.78699 |  0:13:12s\n",
      "epoch 123| loss: 0.56894 | val_0_auc: 0.78355 |  0:13:18s\n",
      "epoch 124| loss: 0.57125 | val_0_auc: 0.78749 |  0:13:25s\n",
      "epoch 125| loss: 0.5668  | val_0_auc: 0.79111 |  0:13:31s\n",
      "epoch 126| loss: 0.56615 | val_0_auc: 0.78702 |  0:13:37s\n",
      "epoch 127| loss: 0.56747 | val_0_auc: 0.796   |  0:13:44s\n",
      "epoch 128| loss: 0.56572 | val_0_auc: 0.7981  |  0:13:50s\n",
      "epoch 129| loss: 0.56332 | val_0_auc: 0.7953  |  0:13:57s\n",
      "epoch 130| loss: 0.56444 | val_0_auc: 0.7961  |  0:14:03s\n",
      "epoch 131| loss: 0.56279 | val_0_auc: 0.79503 |  0:14:10s\n",
      "epoch 132| loss: 0.56026 | val_0_auc: 0.79368 |  0:14:16s\n",
      "epoch 133| loss: 0.55829 | val_0_auc: 0.79423 |  0:14:22s\n",
      "epoch 134| loss: 0.55994 | val_0_auc: 0.79028 |  0:14:29s\n",
      "epoch 135| loss: 0.55427 | val_0_auc: 0.79658 |  0:14:35s\n",
      "epoch 136| loss: 0.55634 | val_0_auc: 0.79909 |  0:14:41s\n",
      "epoch 137| loss: 0.55423 | val_0_auc: 0.79974 |  0:14:48s\n",
      "epoch 138| loss: 0.5556  | val_0_auc: 0.80217 |  0:14:54s\n",
      "epoch 139| loss: 0.54964 | val_0_auc: 0.80009 |  0:15:01s\n",
      "epoch 140| loss: 0.55317 | val_0_auc: 0.80129 |  0:15:07s\n",
      "epoch 141| loss: 0.55066 | val_0_auc: 0.79681 |  0:15:14s\n",
      "epoch 142| loss: 0.54926 | val_0_auc: 0.79917 |  0:15:20s\n",
      "epoch 143| loss: 0.54665 | val_0_auc: 0.80575 |  0:15:27s\n",
      "epoch 144| loss: 0.54703 | val_0_auc: 0.80877 |  0:15:33s\n",
      "epoch 145| loss: 0.54776 | val_0_auc: 0.80402 |  0:15:40s\n",
      "epoch 146| loss: 0.5465  | val_0_auc: 0.80349 |  0:15:46s\n",
      "epoch 147| loss: 0.54788 | val_0_auc: 0.8063  |  0:15:53s\n",
      "epoch 148| loss: 0.54546 | val_0_auc: 0.80598 |  0:16:00s\n",
      "epoch 149| loss: 0.54446 | val_0_auc: 0.80483 |  0:16:06s\n",
      "epoch 150| loss: 0.5472  | val_0_auc: 0.80341 |  0:16:13s\n",
      "epoch 151| loss: 0.54726 | val_0_auc: 0.80694 |  0:16:19s\n",
      "epoch 152| loss: 0.54916 | val_0_auc: 0.80354 |  0:16:25s\n",
      "epoch 153| loss: 0.54516 | val_0_auc: 0.8033  |  0:16:32s\n",
      "epoch 154| loss: 0.54449 | val_0_auc: 0.80124 |  0:16:38s\n",
      "epoch 155| loss: 0.54603 | val_0_auc: 0.80302 |  0:16:45s\n",
      "epoch 156| loss: 0.54544 | val_0_auc: 0.80505 |  0:16:51s\n",
      "epoch 157| loss: 0.54514 | val_0_auc: 0.80265 |  0:16:57s\n",
      "epoch 158| loss: 0.54296 | val_0_auc: 0.80163 |  0:17:04s\n",
      "epoch 159| loss: 0.5409  | val_0_auc: 0.80493 |  0:17:10s\n",
      "epoch 160| loss: 0.54233 | val_0_auc: 0.80295 |  0:17:16s\n",
      "epoch 161| loss: 0.54239 | val_0_auc: 0.80891 |  0:17:23s\n",
      "epoch 162| loss: 0.53883 | val_0_auc: 0.80541 |  0:17:29s\n",
      "epoch 163| loss: 0.53698 | val_0_auc: 0.8048  |  0:17:36s\n",
      "epoch 164| loss: 0.53648 | val_0_auc: 0.81134 |  0:17:42s\n",
      "epoch 165| loss: 0.53461 | val_0_auc: 0.80693 |  0:17:48s\n",
      "epoch 166| loss: 0.53694 | val_0_auc: 0.80898 |  0:17:55s\n",
      "epoch 167| loss: 0.53346 | val_0_auc: 0.81383 |  0:18:01s\n",
      "epoch 168| loss: 0.53609 | val_0_auc: 0.80844 |  0:18:08s\n",
      "epoch 169| loss: 0.53672 | val_0_auc: 0.80845 |  0:18:15s\n",
      "epoch 170| loss: 0.53391 | val_0_auc: 0.80497 |  0:18:21s\n",
      "epoch 171| loss: 0.53196 | val_0_auc: 0.80916 |  0:18:27s\n",
      "epoch 172| loss: 0.53535 | val_0_auc: 0.80279 |  0:18:34s\n",
      "epoch 173| loss: 0.5314  | val_0_auc: 0.80253 |  0:18:40s\n",
      "epoch 174| loss: 0.52978 | val_0_auc: 0.80807 |  0:18:46s\n",
      "epoch 175| loss: 0.5295  | val_0_auc: 0.80402 |  0:18:53s\n",
      "epoch 176| loss: 0.52666 | val_0_auc: 0.80441 |  0:18:59s\n",
      "epoch 177| loss: 0.52751 | val_0_auc: 0.80731 |  0:19:05s\n",
      "epoch 178| loss: 0.52554 | val_0_auc: 0.80437 |  0:19:12s\n",
      "epoch 179| loss: 0.52583 | val_0_auc: 0.8018  |  0:19:18s\n",
      "epoch 180| loss: 0.52781 | val_0_auc: 0.80723 |  0:19:25s\n",
      "epoch 181| loss: 0.52865 | val_0_auc: 0.81112 |  0:19:31s\n",
      "epoch 182| loss: 0.52568 | val_0_auc: 0.8134  |  0:19:37s\n",
      "epoch 183| loss: 0.52841 | val_0_auc: 0.81355 |  0:19:44s\n",
      "epoch 184| loss: 0.52732 | val_0_auc: 0.80975 |  0:19:50s\n",
      "epoch 185| loss: 0.5256  | val_0_auc: 0.80985 |  0:19:56s\n",
      "epoch 186| loss: 0.52737 | val_0_auc: 0.80957 |  0:20:03s\n",
      "epoch 187| loss: 0.52587 | val_0_auc: 0.81245 |  0:20:09s\n",
      "\n",
      "Early stopping occurred at epoch 187 with best_epoch = 167 and best_val_0_auc = 0.81383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67005 | val_0_auc: 0.68664 |  0:00:01s\n",
      "epoch 1  | loss: 0.52857 | val_0_auc: 0.76829 |  0:00:02s\n",
      "epoch 2  | loss: 0.4631  | val_0_auc: 0.81201 |  0:00:03s\n",
      "epoch 3  | loss: 0.41543 | val_0_auc: 0.83037 |  0:00:04s\n",
      "epoch 4  | loss: 0.38093 | val_0_auc: 0.85159 |  0:00:05s\n",
      "epoch 5  | loss: 0.36228 | val_0_auc: 0.865   |  0:00:06s\n",
      "epoch 6  | loss: 0.34817 | val_0_auc: 0.87089 |  0:00:07s\n",
      "epoch 7  | loss: 0.33531 | val_0_auc: 0.87395 |  0:00:08s\n",
      "epoch 8  | loss: 0.32433 | val_0_auc: 0.88129 |  0:00:09s\n",
      "epoch 9  | loss: 0.31812 | val_0_auc: 0.88656 |  0:00:10s\n",
      "epoch 10 | loss: 0.31268 | val_0_auc: 0.89052 |  0:00:11s\n",
      "epoch 11 | loss: 0.30437 | val_0_auc: 0.89647 |  0:00:12s\n",
      "epoch 12 | loss: 0.30314 | val_0_auc: 0.89672 |  0:00:13s\n",
      "epoch 13 | loss: 0.30078 | val_0_auc: 0.89632 |  0:00:14s\n",
      "epoch 14 | loss: 0.29411 | val_0_auc: 0.89998 |  0:00:15s\n",
      "epoch 15 | loss: 0.2927  | val_0_auc: 0.89979 |  0:00:16s\n",
      "epoch 16 | loss: 0.28924 | val_0_auc: 0.90163 |  0:00:17s\n",
      "epoch 17 | loss: 0.28474 | val_0_auc: 0.89854 |  0:00:18s\n",
      "epoch 18 | loss: 0.28406 | val_0_auc: 0.89809 |  0:00:19s\n",
      "epoch 19 | loss: 0.2846  | val_0_auc: 0.90042 |  0:00:20s\n",
      "epoch 20 | loss: 0.28073 | val_0_auc: 0.89999 |  0:00:21s\n",
      "epoch 21 | loss: 0.27985 | val_0_auc: 0.90198 |  0:00:23s\n",
      "epoch 22 | loss: 0.27251 | val_0_auc: 0.89979 |  0:00:24s\n",
      "epoch 23 | loss: 0.27227 | val_0_auc: 0.89784 |  0:00:25s\n",
      "epoch 24 | loss: 0.27107 | val_0_auc: 0.89791 |  0:00:26s\n",
      "epoch 25 | loss: 0.27034 | val_0_auc: 0.89765 |  0:00:27s\n",
      "epoch 26 | loss: 0.26887 | val_0_auc: 0.89659 |  0:00:28s\n",
      "epoch 27 | loss: 0.26632 | val_0_auc: 0.89616 |  0:00:29s\n",
      "epoch 28 | loss: 0.26065 | val_0_auc: 0.89326 |  0:00:30s\n",
      "epoch 29 | loss: 0.25951 | val_0_auc: 0.89663 |  0:00:31s\n",
      "epoch 30 | loss: 0.26225 | val_0_auc: 0.90065 |  0:00:32s\n",
      "epoch 31 | loss: 0.26102 | val_0_auc: 0.89846 |  0:00:33s\n",
      "epoch 32 | loss: 0.25752 | val_0_auc: 0.90124 |  0:00:34s\n",
      "epoch 33 | loss: 0.25764 | val_0_auc: 0.89579 |  0:00:35s\n",
      "epoch 34 | loss: 0.25807 | val_0_auc: 0.89724 |  0:00:36s\n",
      "epoch 35 | loss: 0.256   | val_0_auc: 0.89699 |  0:00:37s\n",
      "epoch 36 | loss: 0.25438 | val_0_auc: 0.89689 |  0:00:38s\n",
      "epoch 37 | loss: 0.25412 | val_0_auc: 0.89317 |  0:00:39s\n",
      "epoch 38 | loss: 0.25041 | val_0_auc: 0.89859 |  0:00:40s\n",
      "epoch 39 | loss: 0.25002 | val_0_auc: 0.89629 |  0:00:41s\n",
      "epoch 40 | loss: 0.24772 | val_0_auc: 0.89563 |  0:00:43s\n",
      "epoch 41 | loss: 0.24795 | val_0_auc: 0.89234 |  0:00:44s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_auc = 0.90198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68026 | val_0_auc: 0.51806 |  0:00:02s\n",
      "epoch 1  | loss: 0.65368 | val_0_auc: 0.50454 |  0:00:04s\n",
      "epoch 2  | loss: 0.63324 | val_0_auc: 0.52457 |  0:00:06s\n",
      "epoch 3  | loss: 0.61769 | val_0_auc: 0.52802 |  0:00:08s\n",
      "epoch 4  | loss: 0.60065 | val_0_auc: 0.54673 |  0:00:10s\n",
      "epoch 5  | loss: 0.5914  | val_0_auc: 0.55232 |  0:00:12s\n",
      "epoch 6  | loss: 0.5753  | val_0_auc: 0.56729 |  0:00:14s\n",
      "epoch 7  | loss: 0.56157 | val_0_auc: 0.59496 |  0:00:16s\n",
      "epoch 8  | loss: 0.55121 | val_0_auc: 0.61495 |  0:00:18s\n",
      "epoch 9  | loss: 0.53907 | val_0_auc: 0.63642 |  0:00:20s\n",
      "epoch 10 | loss: 0.53181 | val_0_auc: 0.6655  |  0:00:22s\n",
      "epoch 11 | loss: 0.52646 | val_0_auc: 0.68046 |  0:00:24s\n",
      "epoch 12 | loss: 0.5182  | val_0_auc: 0.68408 |  0:00:26s\n",
      "epoch 13 | loss: 0.51338 | val_0_auc: 0.6978  |  0:00:28s\n",
      "epoch 14 | loss: 0.50319 | val_0_auc: 0.72319 |  0:00:30s\n",
      "epoch 15 | loss: 0.49769 | val_0_auc: 0.72585 |  0:00:32s\n",
      "epoch 16 | loss: 0.49175 | val_0_auc: 0.73722 |  0:00:34s\n",
      "epoch 17 | loss: 0.4857  | val_0_auc: 0.74386 |  0:00:36s\n",
      "epoch 18 | loss: 0.48098 | val_0_auc: 0.75315 |  0:00:38s\n",
      "epoch 19 | loss: 0.47586 | val_0_auc: 0.76127 |  0:00:40s\n",
      "epoch 20 | loss: 0.47343 | val_0_auc: 0.7685  |  0:00:42s\n",
      "epoch 21 | loss: 0.46379 | val_0_auc: 0.77718 |  0:00:44s\n",
      "epoch 22 | loss: 0.45992 | val_0_auc: 0.7795  |  0:00:46s\n",
      "epoch 23 | loss: 0.45704 | val_0_auc: 0.78829 |  0:00:48s\n",
      "epoch 24 | loss: 0.45268 | val_0_auc: 0.79643 |  0:00:50s\n",
      "epoch 25 | loss: 0.44235 | val_0_auc: 0.79505 |  0:00:53s\n",
      "epoch 26 | loss: 0.44337 | val_0_auc: 0.795   |  0:00:55s\n",
      "epoch 27 | loss: 0.44078 | val_0_auc: 0.79935 |  0:00:57s\n",
      "epoch 28 | loss: 0.43751 | val_0_auc: 0.80576 |  0:00:59s\n",
      "epoch 29 | loss: 0.4294  | val_0_auc: 0.80294 |  0:01:01s\n",
      "epoch 30 | loss: 0.42708 | val_0_auc: 0.80792 |  0:01:03s\n",
      "epoch 31 | loss: 0.42655 | val_0_auc: 0.80951 |  0:01:05s\n",
      "epoch 32 | loss: 0.42447 | val_0_auc: 0.81182 |  0:01:07s\n",
      "epoch 33 | loss: 0.4213  | val_0_auc: 0.80998 |  0:01:09s\n",
      "epoch 34 | loss: 0.42134 | val_0_auc: 0.81764 |  0:01:11s\n",
      "epoch 35 | loss: 0.41565 | val_0_auc: 0.81917 |  0:01:13s\n",
      "epoch 36 | loss: 0.41679 | val_0_auc: 0.82129 |  0:01:15s\n",
      "epoch 37 | loss: 0.41472 | val_0_auc: 0.82118 |  0:01:17s\n",
      "epoch 38 | loss: 0.41155 | val_0_auc: 0.82318 |  0:01:19s\n",
      "epoch 39 | loss: 0.41097 | val_0_auc: 0.82399 |  0:01:22s\n",
      "epoch 40 | loss: 0.40742 | val_0_auc: 0.82579 |  0:01:24s\n",
      "epoch 41 | loss: 0.40313 | val_0_auc: 0.82528 |  0:01:26s\n",
      "epoch 42 | loss: 0.40253 | val_0_auc: 0.82605 |  0:01:28s\n",
      "epoch 43 | loss: 0.40078 | val_0_auc: 0.82694 |  0:01:31s\n",
      "epoch 44 | loss: 0.39891 | val_0_auc: 0.82857 |  0:01:33s\n",
      "epoch 45 | loss: 0.39889 | val_0_auc: 0.82872 |  0:01:35s\n",
      "epoch 46 | loss: 0.39955 | val_0_auc: 0.83021 |  0:01:37s\n",
      "epoch 47 | loss: 0.39611 | val_0_auc: 0.8318  |  0:01:39s\n",
      "epoch 48 | loss: 0.39565 | val_0_auc: 0.83231 |  0:01:41s\n",
      "epoch 49 | loss: 0.3926  | val_0_auc: 0.83359 |  0:01:43s\n",
      "epoch 50 | loss: 0.3918  | val_0_auc: 0.8351  |  0:01:45s\n",
      "epoch 51 | loss: 0.3923  | val_0_auc: 0.83715 |  0:01:47s\n",
      "epoch 52 | loss: 0.38875 | val_0_auc: 0.83633 |  0:01:49s\n",
      "epoch 53 | loss: 0.38447 | val_0_auc: 0.83635 |  0:01:51s\n",
      "epoch 54 | loss: 0.38536 | val_0_auc: 0.83515 |  0:01:54s\n",
      "epoch 55 | loss: 0.38581 | val_0_auc: 0.83677 |  0:01:56s\n",
      "epoch 56 | loss: 0.38187 | val_0_auc: 0.83565 |  0:01:58s\n",
      "epoch 57 | loss: 0.38192 | val_0_auc: 0.8394  |  0:02:00s\n",
      "epoch 58 | loss: 0.3863  | val_0_auc: 0.83845 |  0:02:02s\n",
      "epoch 59 | loss: 0.38239 | val_0_auc: 0.84118 |  0:02:04s\n",
      "epoch 60 | loss: 0.38096 | val_0_auc: 0.84115 |  0:02:06s\n",
      "epoch 61 | loss: 0.38099 | val_0_auc: 0.84245 |  0:02:08s\n",
      "epoch 62 | loss: 0.37858 | val_0_auc: 0.84379 |  0:02:10s\n",
      "epoch 63 | loss: 0.37952 | val_0_auc: 0.8435  |  0:02:12s\n",
      "epoch 64 | loss: 0.37771 | val_0_auc: 0.84312 |  0:02:14s\n",
      "epoch 65 | loss: 0.37333 | val_0_auc: 0.846   |  0:02:16s\n",
      "epoch 66 | loss: 0.37667 | val_0_auc: 0.84687 |  0:02:18s\n",
      "epoch 67 | loss: 0.37104 | val_0_auc: 0.84475 |  0:02:20s\n",
      "epoch 68 | loss: 0.37173 | val_0_auc: 0.8458  |  0:02:23s\n",
      "epoch 69 | loss: 0.37183 | val_0_auc: 0.8463  |  0:02:25s\n",
      "epoch 70 | loss: 0.37032 | val_0_auc: 0.84607 |  0:02:27s\n",
      "epoch 71 | loss: 0.37095 | val_0_auc: 0.84542 |  0:02:29s\n",
      "epoch 72 | loss: 0.36671 | val_0_auc: 0.84683 |  0:02:31s\n",
      "epoch 73 | loss: 0.36637 | val_0_auc: 0.84824 |  0:02:33s\n",
      "epoch 74 | loss: 0.36639 | val_0_auc: 0.84754 |  0:02:35s\n",
      "epoch 75 | loss: 0.36462 | val_0_auc: 0.84723 |  0:02:37s\n",
      "epoch 76 | loss: 0.36601 | val_0_auc: 0.8488  |  0:02:39s\n",
      "epoch 77 | loss: 0.35925 | val_0_auc: 0.85047 |  0:02:41s\n",
      "epoch 78 | loss: 0.35874 | val_0_auc: 0.85074 |  0:02:43s\n",
      "epoch 79 | loss: 0.35909 | val_0_auc: 0.85208 |  0:02:45s\n",
      "epoch 80 | loss: 0.35738 | val_0_auc: 0.85161 |  0:02:47s\n",
      "epoch 81 | loss: 0.36037 | val_0_auc: 0.85222 |  0:02:49s\n",
      "epoch 82 | loss: 0.35737 | val_0_auc: 0.85361 |  0:02:51s\n",
      "epoch 83 | loss: 0.35509 | val_0_auc: 0.85423 |  0:02:53s\n",
      "epoch 84 | loss: 0.35201 | val_0_auc: 0.85451 |  0:02:55s\n",
      "epoch 85 | loss: 0.35497 | val_0_auc: 0.85284 |  0:02:57s\n",
      "epoch 86 | loss: 0.35268 | val_0_auc: 0.85497 |  0:02:59s\n",
      "epoch 87 | loss: 0.35408 | val_0_auc: 0.85388 |  0:03:01s\n",
      "epoch 88 | loss: 0.34765 | val_0_auc: 0.85451 |  0:03:03s\n",
      "epoch 89 | loss: 0.35016 | val_0_auc: 0.85531 |  0:03:05s\n",
      "epoch 90 | loss: 0.34443 | val_0_auc: 0.85675 |  0:03:07s\n",
      "epoch 91 | loss: 0.34535 | val_0_auc: 0.8578  |  0:03:10s\n",
      "epoch 92 | loss: 0.34329 | val_0_auc: 0.85752 |  0:03:12s\n",
      "epoch 93 | loss: 0.34612 | val_0_auc: 0.85826 |  0:03:14s\n",
      "epoch 94 | loss: 0.34361 | val_0_auc: 0.85727 |  0:03:16s\n",
      "epoch 95 | loss: 0.34151 | val_0_auc: 0.85909 |  0:03:18s\n",
      "epoch 96 | loss: 0.34254 | val_0_auc: 0.85912 |  0:03:20s\n",
      "epoch 97 | loss: 0.34323 | val_0_auc: 0.86026 |  0:03:22s\n",
      "epoch 98 | loss: 0.34108 | val_0_auc: 0.86177 |  0:03:24s\n",
      "epoch 99 | loss: 0.33831 | val_0_auc: 0.8608  |  0:03:26s\n",
      "epoch 100| loss: 0.33978 | val_0_auc: 0.86175 |  0:03:28s\n",
      "epoch 101| loss: 0.33678 | val_0_auc: 0.8625  |  0:03:30s\n",
      "epoch 102| loss: 0.33636 | val_0_auc: 0.86429 |  0:03:32s\n",
      "epoch 103| loss: 0.33523 | val_0_auc: 0.86498 |  0:03:34s\n",
      "epoch 104| loss: 0.33432 | val_0_auc: 0.86526 |  0:03:36s\n",
      "epoch 105| loss: 0.33371 | val_0_auc: 0.86754 |  0:03:38s\n",
      "epoch 106| loss: 0.33125 | val_0_auc: 0.86758 |  0:03:40s\n",
      "epoch 107| loss: 0.32897 | val_0_auc: 0.86949 |  0:03:42s\n",
      "epoch 108| loss: 0.33171 | val_0_auc: 0.87046 |  0:03:44s\n",
      "epoch 109| loss: 0.33198 | val_0_auc: 0.87089 |  0:03:46s\n",
      "epoch 110| loss: 0.33124 | val_0_auc: 0.87184 |  0:03:48s\n",
      "epoch 111| loss: 0.32964 | val_0_auc: 0.87278 |  0:03:50s\n",
      "epoch 112| loss: 0.32536 | val_0_auc: 0.87242 |  0:03:52s\n",
      "epoch 113| loss: 0.32412 | val_0_auc: 0.87371 |  0:03:54s\n",
      "epoch 114| loss: 0.32429 | val_0_auc: 0.87358 |  0:03:57s\n",
      "epoch 115| loss: 0.32443 | val_0_auc: 0.87427 |  0:03:59s\n",
      "epoch 116| loss: 0.32498 | val_0_auc: 0.87427 |  0:04:01s\n",
      "epoch 117| loss: 0.32496 | val_0_auc: 0.87435 |  0:04:03s\n",
      "epoch 118| loss: 0.32135 | val_0_auc: 0.87446 |  0:04:05s\n",
      "epoch 119| loss: 0.31886 | val_0_auc: 0.876   |  0:04:07s\n",
      "epoch 120| loss: 0.31919 | val_0_auc: 0.87608 |  0:04:09s\n",
      "epoch 121| loss: 0.31705 | val_0_auc: 0.87641 |  0:04:11s\n",
      "epoch 122| loss: 0.31844 | val_0_auc: 0.87663 |  0:04:13s\n",
      "epoch 123| loss: 0.31817 | val_0_auc: 0.87667 |  0:04:15s\n",
      "epoch 124| loss: 0.31619 | val_0_auc: 0.87674 |  0:04:17s\n",
      "epoch 125| loss: 0.31632 | val_0_auc: 0.87569 |  0:04:19s\n",
      "epoch 126| loss: 0.31816 | val_0_auc: 0.87748 |  0:04:21s\n",
      "epoch 127| loss: 0.31798 | val_0_auc: 0.87774 |  0:04:23s\n",
      "epoch 128| loss: 0.31355 | val_0_auc: 0.87881 |  0:04:25s\n",
      "epoch 129| loss: 0.31326 | val_0_auc: 0.87872 |  0:04:27s\n",
      "epoch 130| loss: 0.31279 | val_0_auc: 0.87928 |  0:04:29s\n",
      "epoch 131| loss: 0.31094 | val_0_auc: 0.87985 |  0:04:31s\n",
      "epoch 132| loss: 0.31239 | val_0_auc: 0.88007 |  0:04:33s\n",
      "epoch 133| loss: 0.31389 | val_0_auc: 0.88046 |  0:04:35s\n",
      "epoch 134| loss: 0.30831 | val_0_auc: 0.88024 |  0:04:37s\n",
      "epoch 135| loss: 0.31081 | val_0_auc: 0.88069 |  0:04:39s\n",
      "epoch 136| loss: 0.30711 | val_0_auc: 0.8809  |  0:04:41s\n",
      "epoch 137| loss: 0.31012 | val_0_auc: 0.8807  |  0:04:44s\n",
      "epoch 138| loss: 0.3128  | val_0_auc: 0.88034 |  0:04:46s\n",
      "epoch 139| loss: 0.30838 | val_0_auc: 0.88139 |  0:04:48s\n",
      "epoch 140| loss: 0.30738 | val_0_auc: 0.88116 |  0:04:50s\n",
      "epoch 141| loss: 0.30678 | val_0_auc: 0.88056 |  0:04:52s\n",
      "epoch 142| loss: 0.30405 | val_0_auc: 0.88266 |  0:04:54s\n",
      "epoch 143| loss: 0.30535 | val_0_auc: 0.88237 |  0:04:56s\n",
      "epoch 144| loss: 0.30271 | val_0_auc: 0.88286 |  0:04:58s\n",
      "epoch 145| loss: 0.30479 | val_0_auc: 0.88331 |  0:05:00s\n",
      "epoch 146| loss: 0.30533 | val_0_auc: 0.88288 |  0:05:02s\n",
      "epoch 147| loss: 0.30295 | val_0_auc: 0.88276 |  0:05:04s\n",
      "epoch 148| loss: 0.30488 | val_0_auc: 0.88365 |  0:05:06s\n",
      "epoch 149| loss: 0.30144 | val_0_auc: 0.88372 |  0:05:08s\n",
      "epoch 150| loss: 0.29939 | val_0_auc: 0.88378 |  0:05:10s\n",
      "epoch 151| loss: 0.30144 | val_0_auc: 0.88447 |  0:05:12s\n",
      "epoch 152| loss: 0.30042 | val_0_auc: 0.88414 |  0:05:14s\n",
      "epoch 153| loss: 0.29927 | val_0_auc: 0.88464 |  0:05:16s\n",
      "epoch 154| loss: 0.30081 | val_0_auc: 0.88458 |  0:05:18s\n",
      "epoch 155| loss: 0.29765 | val_0_auc: 0.88429 |  0:05:21s\n",
      "epoch 156| loss: 0.29884 | val_0_auc: 0.88373 |  0:05:23s\n",
      "epoch 157| loss: 0.29726 | val_0_auc: 0.88428 |  0:05:25s\n",
      "epoch 158| loss: 0.29724 | val_0_auc: 0.88505 |  0:05:27s\n",
      "epoch 159| loss: 0.29558 | val_0_auc: 0.88405 |  0:05:29s\n",
      "epoch 160| loss: 0.29588 | val_0_auc: 0.88443 |  0:05:31s\n",
      "epoch 161| loss: 0.2957  | val_0_auc: 0.88512 |  0:05:33s\n",
      "epoch 162| loss: 0.29207 | val_0_auc: 0.88506 |  0:05:35s\n",
      "epoch 163| loss: 0.29492 | val_0_auc: 0.88531 |  0:05:37s\n",
      "epoch 164| loss: 0.29221 | val_0_auc: 0.88529 |  0:05:39s\n",
      "epoch 165| loss: 0.2931  | val_0_auc: 0.88472 |  0:05:41s\n",
      "epoch 166| loss: 0.29291 | val_0_auc: 0.88505 |  0:05:43s\n",
      "epoch 167| loss: 0.29104 | val_0_auc: 0.88464 |  0:05:45s\n",
      "epoch 168| loss: 0.29117 | val_0_auc: 0.8855  |  0:05:47s\n",
      "epoch 169| loss: 0.29227 | val_0_auc: 0.88587 |  0:05:49s\n",
      "epoch 170| loss: 0.29192 | val_0_auc: 0.88613 |  0:05:51s\n",
      "epoch 171| loss: 0.29259 | val_0_auc: 0.88545 |  0:05:53s\n",
      "epoch 172| loss: 0.28743 | val_0_auc: 0.88607 |  0:05:55s\n",
      "epoch 173| loss: 0.29031 | val_0_auc: 0.88726 |  0:05:57s\n",
      "epoch 174| loss: 0.28739 | val_0_auc: 0.88627 |  0:05:59s\n",
      "epoch 175| loss: 0.28917 | val_0_auc: 0.8855  |  0:06:01s\n",
      "epoch 176| loss: 0.28584 | val_0_auc: 0.88652 |  0:06:03s\n",
      "epoch 177| loss: 0.28587 | val_0_auc: 0.88658 |  0:06:05s\n",
      "epoch 178| loss: 0.28354 | val_0_auc: 0.88612 |  0:06:07s\n",
      "epoch 179| loss: 0.28718 | val_0_auc: 0.8864  |  0:06:09s\n",
      "epoch 180| loss: 0.28487 | val_0_auc: 0.88789 |  0:06:11s\n",
      "epoch 181| loss: 0.28728 | val_0_auc: 0.88771 |  0:06:14s\n",
      "epoch 182| loss: 0.288   | val_0_auc: 0.887   |  0:06:16s\n",
      "epoch 183| loss: 0.28689 | val_0_auc: 0.88736 |  0:06:18s\n",
      "epoch 184| loss: 0.28323 | val_0_auc: 0.8882  |  0:06:20s\n",
      "epoch 185| loss: 0.28406 | val_0_auc: 0.88836 |  0:06:22s\n",
      "epoch 186| loss: 0.28402 | val_0_auc: 0.88838 |  0:06:24s\n",
      "epoch 187| loss: 0.28078 | val_0_auc: 0.88803 |  0:06:26s\n",
      "epoch 188| loss: 0.28546 | val_0_auc: 0.88878 |  0:06:28s\n",
      "epoch 189| loss: 0.28187 | val_0_auc: 0.88922 |  0:06:30s\n",
      "epoch 190| loss: 0.28147 | val_0_auc: 0.88889 |  0:06:32s\n",
      "epoch 191| loss: 0.27898 | val_0_auc: 0.88895 |  0:06:34s\n",
      "epoch 192| loss: 0.27981 | val_0_auc: 0.8893  |  0:06:36s\n",
      "epoch 193| loss: 0.28098 | val_0_auc: 0.88865 |  0:06:38s\n",
      "epoch 194| loss: 0.27845 | val_0_auc: 0.8872  |  0:06:40s\n",
      "epoch 195| loss: 0.27706 | val_0_auc: 0.88874 |  0:06:42s\n",
      "epoch 196| loss: 0.27776 | val_0_auc: 0.88803 |  0:06:45s\n",
      "epoch 197| loss: 0.27819 | val_0_auc: 0.88907 |  0:06:47s\n",
      "epoch 198| loss: 0.28061 | val_0_auc: 0.88838 |  0:06:49s\n",
      "epoch 199| loss: 0.27788 | val_0_auc: 0.89037 |  0:06:51s\n",
      "epoch 200| loss: 0.27933 | val_0_auc: 0.88985 |  0:06:53s\n",
      "epoch 201| loss: 0.27931 | val_0_auc: 0.89013 |  0:06:55s\n",
      "epoch 202| loss: 0.27928 | val_0_auc: 0.89064 |  0:06:57s\n",
      "epoch 203| loss: 0.27727 | val_0_auc: 0.89022 |  0:06:59s\n",
      "epoch 204| loss: 0.27376 | val_0_auc: 0.89021 |  0:07:01s\n",
      "epoch 205| loss: 0.27186 | val_0_auc: 0.89051 |  0:07:03s\n",
      "epoch 206| loss: 0.27198 | val_0_auc: 0.89109 |  0:07:05s\n",
      "epoch 207| loss: 0.27423 | val_0_auc: 0.89103 |  0:07:07s\n",
      "epoch 208| loss: 0.27372 | val_0_auc: 0.89003 |  0:07:09s\n",
      "epoch 209| loss: 0.27526 | val_0_auc: 0.89033 |  0:07:11s\n",
      "epoch 210| loss: 0.27667 | val_0_auc: 0.89013 |  0:07:13s\n",
      "epoch 211| loss: 0.27204 | val_0_auc: 0.89066 |  0:07:15s\n",
      "epoch 212| loss: 0.27396 | val_0_auc: 0.88971 |  0:07:18s\n",
      "epoch 213| loss: 0.27256 | val_0_auc: 0.88924 |  0:07:20s\n",
      "epoch 214| loss: 0.27298 | val_0_auc: 0.88939 |  0:07:22s\n",
      "epoch 215| loss: 0.27397 | val_0_auc: 0.89035 |  0:07:24s\n",
      "epoch 216| loss: 0.27442 | val_0_auc: 0.8903  |  0:07:26s\n",
      "epoch 217| loss: 0.2713  | val_0_auc: 0.89009 |  0:07:28s\n",
      "epoch 218| loss: 0.26879 | val_0_auc: 0.89055 |  0:07:30s\n",
      "epoch 219| loss: 0.26954 | val_0_auc: 0.89077 |  0:07:32s\n",
      "epoch 220| loss: 0.26852 | val_0_auc: 0.89049 |  0:07:34s\n",
      "epoch 221| loss: 0.26753 | val_0_auc: 0.88962 |  0:07:36s\n",
      "epoch 222| loss: 0.27228 | val_0_auc: 0.88826 |  0:07:38s\n",
      "epoch 223| loss: 0.26863 | val_0_auc: 0.88721 |  0:07:40s\n",
      "epoch 224| loss: 0.27067 | val_0_auc: 0.88689 |  0:07:42s\n",
      "epoch 225| loss: 0.2679  | val_0_auc: 0.88685 |  0:07:44s\n",
      "epoch 226| loss: 0.26807 | val_0_auc: 0.88685 |  0:07:46s\n",
      "\n",
      "Early stopping occurred at epoch 226 with best_epoch = 206 and best_val_0_auc = 0.89109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.86432 | val_0_auc: 0.64958 |  0:00:02s\n",
      "epoch 1  | loss: 0.66792 | val_0_auc: 0.76735 |  0:00:05s\n",
      "epoch 2  | loss: 0.58686 | val_0_auc: 0.80345 |  0:00:07s\n",
      "epoch 3  | loss: 0.53939 | val_0_auc: 0.82691 |  0:00:10s\n",
      "epoch 4  | loss: 0.50934 | val_0_auc: 0.84638 |  0:00:12s\n",
      "epoch 5  | loss: 0.48545 | val_0_auc: 0.84424 |  0:00:15s\n",
      "epoch 6  | loss: 0.46431 | val_0_auc: 0.85234 |  0:00:17s\n",
      "epoch 7  | loss: 0.44653 | val_0_auc: 0.85537 |  0:00:19s\n",
      "epoch 8  | loss: 0.4357  | val_0_auc: 0.85028 |  0:00:22s\n",
      "epoch 9  | loss: 0.42231 | val_0_auc: 0.85846 |  0:00:24s\n",
      "epoch 10 | loss: 0.41366 | val_0_auc: 0.86242 |  0:00:26s\n",
      "epoch 11 | loss: 0.40093 | val_0_auc: 0.86702 |  0:00:29s\n",
      "epoch 12 | loss: 0.39872 | val_0_auc: 0.87201 |  0:00:31s\n",
      "epoch 13 | loss: 0.3872  | val_0_auc: 0.87194 |  0:00:33s\n",
      "epoch 14 | loss: 0.38253 | val_0_auc: 0.87321 |  0:00:36s\n",
      "epoch 15 | loss: 0.37577 | val_0_auc: 0.87552 |  0:00:38s\n",
      "epoch 16 | loss: 0.36983 | val_0_auc: 0.87753 |  0:00:40s\n",
      "epoch 17 | loss: 0.36337 | val_0_auc: 0.87897 |  0:00:43s\n",
      "epoch 18 | loss: 0.35883 | val_0_auc: 0.88365 |  0:00:45s\n",
      "epoch 19 | loss: 0.35765 | val_0_auc: 0.88188 |  0:00:47s\n",
      "epoch 20 | loss: 0.35148 | val_0_auc: 0.88445 |  0:00:50s\n",
      "epoch 21 | loss: 0.35369 | val_0_auc: 0.88502 |  0:00:52s\n",
      "epoch 22 | loss: 0.34976 | val_0_auc: 0.88606 |  0:00:54s\n",
      "epoch 23 | loss: 0.34584 | val_0_auc: 0.88985 |  0:00:56s\n",
      "epoch 24 | loss: 0.34155 | val_0_auc: 0.88854 |  0:00:59s\n",
      "epoch 25 | loss: 0.3416  | val_0_auc: 0.88825 |  0:01:01s\n",
      "epoch 26 | loss: 0.33949 | val_0_auc: 0.88537 |  0:01:03s\n",
      "epoch 27 | loss: 0.33792 | val_0_auc: 0.89087 |  0:01:06s\n",
      "epoch 28 | loss: 0.33396 | val_0_auc: 0.89104 |  0:01:08s\n",
      "epoch 29 | loss: 0.33411 | val_0_auc: 0.89145 |  0:01:10s\n",
      "epoch 30 | loss: 0.3338  | val_0_auc: 0.89172 |  0:01:13s\n",
      "epoch 31 | loss: 0.33408 | val_0_auc: 0.89222 |  0:01:15s\n",
      "epoch 32 | loss: 0.33415 | val_0_auc: 0.89251 |  0:01:17s\n",
      "epoch 33 | loss: 0.33731 | val_0_auc: 0.89127 |  0:01:20s\n",
      "epoch 34 | loss: 0.33527 | val_0_auc: 0.89287 |  0:01:22s\n",
      "epoch 35 | loss: 0.33585 | val_0_auc: 0.89411 |  0:01:24s\n",
      "epoch 36 | loss: 0.33612 | val_0_auc: 0.88995 |  0:01:27s\n",
      "epoch 37 | loss: 0.32968 | val_0_auc: 0.8952  |  0:01:29s\n",
      "epoch 38 | loss: 0.3263  | val_0_auc: 0.89791 |  0:01:31s\n",
      "epoch 39 | loss: 0.32874 | val_0_auc: 0.89596 |  0:01:34s\n",
      "epoch 40 | loss: 0.32673 | val_0_auc: 0.89756 |  0:01:36s\n",
      "epoch 41 | loss: 0.32533 | val_0_auc: 0.89599 |  0:01:38s\n",
      "epoch 42 | loss: 0.32182 | val_0_auc: 0.89481 |  0:01:41s\n",
      "epoch 43 | loss: 0.31963 | val_0_auc: 0.89803 |  0:01:43s\n",
      "epoch 44 | loss: 0.31813 | val_0_auc: 0.89626 |  0:01:45s\n",
      "epoch 45 | loss: 0.3183  | val_0_auc: 0.89704 |  0:01:48s\n",
      "epoch 46 | loss: 0.31933 | val_0_auc: 0.89491 |  0:01:50s\n",
      "epoch 47 | loss: 0.31755 | val_0_auc: 0.89749 |  0:01:52s\n",
      "epoch 48 | loss: 0.31567 | val_0_auc: 0.8987  |  0:01:55s\n",
      "epoch 49 | loss: 0.31467 | val_0_auc: 0.89838 |  0:01:57s\n",
      "epoch 50 | loss: 0.31457 | val_0_auc: 0.8985  |  0:01:59s\n",
      "epoch 51 | loss: 0.31385 | val_0_auc: 0.89953 |  0:02:02s\n",
      "epoch 52 | loss: 0.31068 | val_0_auc: 0.89977 |  0:02:04s\n",
      "epoch 53 | loss: 0.30966 | val_0_auc: 0.89839 |  0:02:06s\n",
      "epoch 54 | loss: 0.30906 | val_0_auc: 0.89864 |  0:02:09s\n",
      "epoch 55 | loss: 0.30983 | val_0_auc: 0.90098 |  0:02:11s\n",
      "epoch 56 | loss: 0.3095  | val_0_auc: 0.89863 |  0:02:13s\n",
      "epoch 57 | loss: 0.30861 | val_0_auc: 0.89903 |  0:02:16s\n",
      "epoch 58 | loss: 0.30881 | val_0_auc: 0.90103 |  0:02:18s\n",
      "epoch 59 | loss: 0.3065  | val_0_auc: 0.90006 |  0:02:20s\n",
      "epoch 60 | loss: 0.30805 | val_0_auc: 0.90242 |  0:02:22s\n",
      "epoch 61 | loss: 0.30479 | val_0_auc: 0.90244 |  0:02:25s\n",
      "epoch 62 | loss: 0.3062  | val_0_auc: 0.90261 |  0:02:27s\n",
      "epoch 63 | loss: 0.30842 | val_0_auc: 0.90286 |  0:02:29s\n",
      "epoch 64 | loss: 0.30747 | val_0_auc: 0.90081 |  0:02:32s\n",
      "epoch 65 | loss: 0.3058  | val_0_auc: 0.89692 |  0:02:34s\n",
      "epoch 66 | loss: 0.30499 | val_0_auc: 0.90116 |  0:02:37s\n",
      "epoch 67 | loss: 0.3052  | val_0_auc: 0.90273 |  0:02:39s\n",
      "epoch 68 | loss: 0.307   | val_0_auc: 0.90239 |  0:02:41s\n",
      "epoch 69 | loss: 0.30489 | val_0_auc: 0.90288 |  0:02:43s\n",
      "epoch 70 | loss: 0.30561 | val_0_auc: 0.90208 |  0:02:46s\n",
      "epoch 71 | loss: 0.30379 | val_0_auc: 0.90001 |  0:02:48s\n",
      "epoch 72 | loss: 0.30377 | val_0_auc: 0.90179 |  0:02:51s\n",
      "epoch 73 | loss: 0.30516 | val_0_auc: 0.90116 |  0:02:53s\n",
      "epoch 74 | loss: 0.30357 | val_0_auc: 0.89948 |  0:02:55s\n",
      "epoch 75 | loss: 0.30941 | val_0_auc: 0.9025  |  0:02:57s\n",
      "epoch 76 | loss: 0.31042 | val_0_auc: 0.90241 |  0:03:00s\n",
      "epoch 77 | loss: 0.30941 | val_0_auc: 0.90464 |  0:03:02s\n",
      "epoch 78 | loss: 0.30591 | val_0_auc: 0.90313 |  0:03:04s\n",
      "epoch 79 | loss: 0.30521 | val_0_auc: 0.90416 |  0:03:07s\n",
      "epoch 80 | loss: 0.30438 | val_0_auc: 0.90345 |  0:03:09s\n",
      "epoch 81 | loss: 0.30355 | val_0_auc: 0.90194 |  0:03:11s\n",
      "epoch 82 | loss: 0.31037 | val_0_auc: 0.90377 |  0:03:14s\n",
      "epoch 83 | loss: 0.30939 | val_0_auc: 0.90363 |  0:03:16s\n",
      "epoch 84 | loss: 0.30771 | val_0_auc: 0.90442 |  0:03:18s\n",
      "epoch 85 | loss: 0.30254 | val_0_auc: 0.90556 |  0:03:21s\n",
      "epoch 86 | loss: 0.30488 | val_0_auc: 0.90667 |  0:03:23s\n",
      "epoch 87 | loss: 0.30535 | val_0_auc: 0.90423 |  0:03:25s\n",
      "epoch 88 | loss: 0.30651 | val_0_auc: 0.90261 |  0:03:28s\n",
      "epoch 89 | loss: 0.30289 | val_0_auc: 0.90167 |  0:03:30s\n",
      "epoch 90 | loss: 0.30285 | val_0_auc: 0.90093 |  0:03:32s\n",
      "epoch 91 | loss: 0.30598 | val_0_auc: 0.90395 |  0:03:34s\n",
      "epoch 92 | loss: 0.30434 | val_0_auc: 0.90541 |  0:03:37s\n",
      "epoch 93 | loss: 0.30157 | val_0_auc: 0.90403 |  0:03:39s\n",
      "epoch 94 | loss: 0.29951 | val_0_auc: 0.90317 |  0:03:42s\n",
      "epoch 95 | loss: 0.30171 | val_0_auc: 0.88457 |  0:03:45s\n",
      "epoch 96 | loss: 0.30155 | val_0_auc: 0.903   |  0:03:47s\n",
      "epoch 97 | loss: 0.30108 | val_0_auc: 0.90683 |  0:03:49s\n",
      "epoch 98 | loss: 0.29935 | val_0_auc: 0.90045 |  0:03:52s\n",
      "epoch 99 | loss: 0.29969 | val_0_auc: 0.90461 |  0:03:54s\n",
      "epoch 100| loss: 0.30128 | val_0_auc: 0.90443 |  0:03:56s\n",
      "epoch 101| loss: 0.30078 | val_0_auc: 0.90421 |  0:03:58s\n",
      "epoch 102| loss: 0.30262 | val_0_auc: 0.90619 |  0:04:01s\n",
      "epoch 103| loss: 0.30411 | val_0_auc: 0.90385 |  0:04:03s\n",
      "epoch 104| loss: 0.2995  | val_0_auc: 0.90579 |  0:04:06s\n",
      "epoch 105| loss: 0.3008  | val_0_auc: 0.90572 |  0:04:08s\n",
      "epoch 106| loss: 0.30026 | val_0_auc: 0.90442 |  0:04:10s\n",
      "epoch 107| loss: 0.29874 | val_0_auc: 0.90457 |  0:04:12s\n",
      "epoch 108| loss: 0.29803 | val_0_auc: 0.90358 |  0:04:15s\n",
      "epoch 109| loss: 0.29681 | val_0_auc: 0.904   |  0:04:17s\n",
      "epoch 110| loss: 0.29723 | val_0_auc: 0.90559 |  0:04:19s\n",
      "epoch 111| loss: 0.29505 | val_0_auc: 0.90529 |  0:04:22s\n",
      "epoch 112| loss: 0.29781 | val_0_auc: 0.90242 |  0:04:24s\n",
      "epoch 113| loss: 0.29963 | val_0_auc: 0.90378 |  0:04:26s\n",
      "epoch 114| loss: 0.2998  | val_0_auc: 0.9029  |  0:04:29s\n",
      "epoch 115| loss: 0.29818 | val_0_auc: 0.90234 |  0:04:31s\n",
      "epoch 116| loss: 0.29698 | val_0_auc: 0.90355 |  0:04:33s\n",
      "epoch 117| loss: 0.29796 | val_0_auc: 0.90634 |  0:04:35s\n",
      "\n",
      "Early stopping occurred at epoch 117 with best_epoch = 97 and best_val_0_auc = 0.90683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.12857 | val_0_auc: 0.54725 |  0:00:03s\n",
      "epoch 1  | loss: 0.62648 | val_0_auc: 0.6066  |  0:00:06s\n",
      "epoch 2  | loss: 0.54043 | val_0_auc: 0.63733 |  0:00:10s\n",
      "epoch 3  | loss: 0.49732 | val_0_auc: 0.7305  |  0:00:13s\n",
      "epoch 4  | loss: 0.47208 | val_0_auc: 0.75684 |  0:00:16s\n",
      "epoch 5  | loss: 0.454   | val_0_auc: 0.77037 |  0:00:20s\n",
      "epoch 6  | loss: 0.44773 | val_0_auc: 0.78633 |  0:00:23s\n",
      "epoch 7  | loss: 0.44315 | val_0_auc: 0.79661 |  0:00:26s\n",
      "epoch 8  | loss: 0.43329 | val_0_auc: 0.79783 |  0:00:30s\n",
      "epoch 9  | loss: 0.42799 | val_0_auc: 0.79182 |  0:00:33s\n",
      "epoch 10 | loss: 0.4385  | val_0_auc: 0.79256 |  0:00:36s\n",
      "epoch 11 | loss: 0.4237  | val_0_auc: 0.80037 |  0:00:39s\n",
      "epoch 12 | loss: 0.42446 | val_0_auc: 0.79874 |  0:00:43s\n",
      "epoch 13 | loss: 0.42377 | val_0_auc: 0.80909 |  0:00:46s\n",
      "epoch 14 | loss: 0.41446 | val_0_auc: 0.81932 |  0:00:49s\n",
      "epoch 15 | loss: 0.41609 | val_0_auc: 0.80491 |  0:00:53s\n",
      "epoch 16 | loss: 0.42163 | val_0_auc: 0.79801 |  0:00:56s\n",
      "epoch 17 | loss: 0.42199 | val_0_auc: 0.80381 |  0:00:59s\n",
      "epoch 18 | loss: 0.41812 | val_0_auc: 0.79532 |  0:01:02s\n",
      "epoch 19 | loss: 0.41158 | val_0_auc: 0.79553 |  0:01:06s\n",
      "epoch 20 | loss: 0.40617 | val_0_auc: 0.80817 |  0:01:09s\n",
      "epoch 21 | loss: 0.40506 | val_0_auc: 0.81398 |  0:01:12s\n",
      "epoch 22 | loss: 0.40333 | val_0_auc: 0.8128  |  0:01:16s\n",
      "epoch 23 | loss: 0.40649 | val_0_auc: 0.81415 |  0:01:19s\n",
      "epoch 24 | loss: 0.39763 | val_0_auc: 0.80778 |  0:01:22s\n",
      "epoch 25 | loss: 0.39881 | val_0_auc: 0.81725 |  0:01:25s\n",
      "epoch 26 | loss: 0.39653 | val_0_auc: 0.81431 |  0:01:29s\n",
      "epoch 27 | loss: 0.39437 | val_0_auc: 0.8234  |  0:01:32s\n",
      "epoch 28 | loss: 0.38773 | val_0_auc: 0.83009 |  0:01:35s\n",
      "epoch 29 | loss: 0.38717 | val_0_auc: 0.82661 |  0:01:39s\n",
      "epoch 30 | loss: 0.38625 | val_0_auc: 0.83266 |  0:01:42s\n",
      "epoch 31 | loss: 0.38514 | val_0_auc: 0.83443 |  0:01:46s\n",
      "epoch 32 | loss: 0.38142 | val_0_auc: 0.83275 |  0:01:49s\n",
      "epoch 33 | loss: 0.37873 | val_0_auc: 0.83653 |  0:01:52s\n",
      "epoch 34 | loss: 0.3784  | val_0_auc: 0.84039 |  0:01:56s\n",
      "epoch 35 | loss: 0.37685 | val_0_auc: 0.83966 |  0:01:59s\n",
      "epoch 36 | loss: 0.37441 | val_0_auc: 0.84488 |  0:02:02s\n",
      "epoch 37 | loss: 0.37077 | val_0_auc: 0.84917 |  0:02:05s\n",
      "epoch 38 | loss: 0.36797 | val_0_auc: 0.84743 |  0:02:09s\n",
      "epoch 39 | loss: 0.36947 | val_0_auc: 0.84934 |  0:02:12s\n",
      "epoch 40 | loss: 0.36897 | val_0_auc: 0.85214 |  0:02:15s\n",
      "epoch 41 | loss: 0.36469 | val_0_auc: 0.85959 |  0:02:18s\n",
      "epoch 42 | loss: 0.36164 | val_0_auc: 0.85373 |  0:02:22s\n",
      "epoch 43 | loss: 0.36399 | val_0_auc: 0.85883 |  0:02:25s\n",
      "epoch 44 | loss: 0.3621  | val_0_auc: 0.86201 |  0:02:28s\n",
      "epoch 45 | loss: 0.3577  | val_0_auc: 0.86135 |  0:02:32s\n",
      "epoch 46 | loss: 0.35738 | val_0_auc: 0.86198 |  0:02:35s\n",
      "epoch 47 | loss: 0.35972 | val_0_auc: 0.86575 |  0:02:38s\n",
      "epoch 48 | loss: 0.35882 | val_0_auc: 0.86572 |  0:02:42s\n",
      "epoch 49 | loss: 0.35547 | val_0_auc: 0.86722 |  0:02:45s\n",
      "epoch 50 | loss: 0.34929 | val_0_auc: 0.8724  |  0:02:48s\n",
      "epoch 51 | loss: 0.34293 | val_0_auc: 0.8741  |  0:02:51s\n",
      "epoch 52 | loss: 0.34049 | val_0_auc: 0.87482 |  0:02:55s\n",
      "epoch 53 | loss: 0.33466 | val_0_auc: 0.87606 |  0:02:58s\n",
      "epoch 54 | loss: 0.33374 | val_0_auc: 0.87899 |  0:03:01s\n",
      "epoch 55 | loss: 0.33077 | val_0_auc: 0.87705 |  0:03:04s\n",
      "epoch 56 | loss: 0.33633 | val_0_auc: 0.8789  |  0:03:08s\n",
      "epoch 57 | loss: 0.33387 | val_0_auc: 0.87952 |  0:03:11s\n",
      "epoch 58 | loss: 0.33098 | val_0_auc: 0.87926 |  0:03:14s\n",
      "epoch 59 | loss: 0.33006 | val_0_auc: 0.88499 |  0:03:18s\n",
      "epoch 60 | loss: 0.32745 | val_0_auc: 0.88561 |  0:03:21s\n",
      "epoch 61 | loss: 0.3224  | val_0_auc: 0.8875  |  0:03:24s\n",
      "epoch 62 | loss: 0.31992 | val_0_auc: 0.88973 |  0:03:27s\n",
      "epoch 63 | loss: 0.31818 | val_0_auc: 0.89007 |  0:03:30s\n",
      "epoch 64 | loss: 0.31672 | val_0_auc: 0.89002 |  0:03:34s\n",
      "epoch 65 | loss: 0.31627 | val_0_auc: 0.88707 |  0:03:37s\n",
      "epoch 66 | loss: 0.31385 | val_0_auc: 0.88669 |  0:03:40s\n",
      "epoch 67 | loss: 0.31541 | val_0_auc: 0.88797 |  0:03:43s\n",
      "epoch 68 | loss: 0.31536 | val_0_auc: 0.89028 |  0:03:47s\n",
      "epoch 69 | loss: 0.3144  | val_0_auc: 0.89185 |  0:03:50s\n",
      "epoch 70 | loss: 0.31155 | val_0_auc: 0.89317 |  0:03:53s\n",
      "epoch 71 | loss: 0.30845 | val_0_auc: 0.89297 |  0:03:57s\n",
      "epoch 72 | loss: 0.30726 | val_0_auc: 0.89471 |  0:04:00s\n",
      "epoch 73 | loss: 0.30473 | val_0_auc: 0.89554 |  0:04:03s\n",
      "epoch 74 | loss: 0.30753 | val_0_auc: 0.8909  |  0:04:06s\n",
      "epoch 75 | loss: 0.31365 | val_0_auc: 0.89253 |  0:04:10s\n",
      "epoch 76 | loss: 0.30959 | val_0_auc: 0.89581 |  0:04:13s\n",
      "epoch 77 | loss: 0.30874 | val_0_auc: 0.89628 |  0:04:16s\n",
      "epoch 78 | loss: 0.30636 | val_0_auc: 0.89793 |  0:04:19s\n",
      "epoch 79 | loss: 0.30366 | val_0_auc: 0.89911 |  0:04:23s\n",
      "epoch 80 | loss: 0.30378 | val_0_auc: 0.90019 |  0:04:26s\n",
      "epoch 81 | loss: 0.30167 | val_0_auc: 0.90025 |  0:04:29s\n",
      "epoch 82 | loss: 0.30133 | val_0_auc: 0.9004  |  0:04:33s\n",
      "epoch 83 | loss: 0.30087 | val_0_auc: 0.90033 |  0:04:36s\n",
      "epoch 84 | loss: 0.30075 | val_0_auc: 0.90168 |  0:04:39s\n",
      "epoch 85 | loss: 0.29846 | val_0_auc: 0.90227 |  0:04:42s\n",
      "epoch 86 | loss: 0.29863 | val_0_auc: 0.90257 |  0:04:46s\n",
      "epoch 87 | loss: 0.30047 | val_0_auc: 0.90316 |  0:04:49s\n",
      "epoch 88 | loss: 0.29831 | val_0_auc: 0.90302 |  0:04:52s\n",
      "epoch 89 | loss: 0.29847 | val_0_auc: 0.90286 |  0:04:55s\n",
      "epoch 90 | loss: 0.30075 | val_0_auc: 0.90095 |  0:04:59s\n",
      "epoch 91 | loss: 0.29819 | val_0_auc: 0.90163 |  0:05:02s\n",
      "epoch 92 | loss: 0.29718 | val_0_auc: 0.90253 |  0:05:05s\n",
      "epoch 93 | loss: 0.29579 | val_0_auc: 0.90395 |  0:05:08s\n",
      "epoch 94 | loss: 0.29479 | val_0_auc: 0.90379 |  0:05:12s\n",
      "epoch 95 | loss: 0.29381 | val_0_auc: 0.90394 |  0:05:15s\n",
      "epoch 96 | loss: 0.29417 | val_0_auc: 0.90442 |  0:05:18s\n",
      "epoch 97 | loss: 0.29379 | val_0_auc: 0.90393 |  0:05:21s\n",
      "epoch 98 | loss: 0.29239 | val_0_auc: 0.90475 |  0:05:25s\n",
      "epoch 99 | loss: 0.29165 | val_0_auc: 0.90507 |  0:05:28s\n",
      "epoch 100| loss: 0.29213 | val_0_auc: 0.90487 |  0:05:32s\n",
      "epoch 101| loss: 0.29142 | val_0_auc: 0.90488 |  0:05:35s\n",
      "epoch 102| loss: 0.28979 | val_0_auc: 0.90581 |  0:05:38s\n",
      "epoch 103| loss: 0.29092 | val_0_auc: 0.90395 |  0:05:41s\n",
      "epoch 104| loss: 0.28857 | val_0_auc: 0.90462 |  0:05:45s\n",
      "epoch 105| loss: 0.28726 | val_0_auc: 0.90553 |  0:05:48s\n",
      "epoch 106| loss: 0.28911 | val_0_auc: 0.90332 |  0:05:51s\n",
      "epoch 107| loss: 0.29086 | val_0_auc: 0.90411 |  0:05:54s\n",
      "epoch 108| loss: 0.2925  | val_0_auc: 0.90303 |  0:05:58s\n",
      "epoch 109| loss: 0.29307 | val_0_auc: 0.9027  |  0:06:01s\n",
      "epoch 110| loss: 0.29479 | val_0_auc: 0.90166 |  0:06:04s\n",
      "epoch 111| loss: 0.29403 | val_0_auc: 0.90119 |  0:06:07s\n",
      "epoch 112| loss: 0.29165 | val_0_auc: 0.90258 |  0:06:11s\n",
      "epoch 113| loss: 0.29194 | val_0_auc: 0.90374 |  0:06:14s\n",
      "epoch 114| loss: 0.29023 | val_0_auc: 0.90433 |  0:06:17s\n",
      "epoch 115| loss: 0.29002 | val_0_auc: 0.90268 |  0:06:20s\n",
      "epoch 116| loss: 0.2861  | val_0_auc: 0.9048  |  0:06:24s\n",
      "epoch 117| loss: 0.28715 | val_0_auc: 0.90422 |  0:06:27s\n",
      "epoch 118| loss: 0.29079 | val_0_auc: 0.90306 |  0:06:30s\n",
      "epoch 119| loss: 0.28894 | val_0_auc: 0.90465 |  0:06:33s\n",
      "epoch 120| loss: 0.29066 | val_0_auc: 0.90519 |  0:06:37s\n",
      "epoch 121| loss: 0.28882 | val_0_auc: 0.90668 |  0:06:40s\n",
      "epoch 122| loss: 0.28764 | val_0_auc: 0.90698 |  0:06:43s\n",
      "epoch 123| loss: 0.28462 | val_0_auc: 0.90754 |  0:06:47s\n",
      "epoch 124| loss: 0.28474 | val_0_auc: 0.90635 |  0:06:50s\n",
      "epoch 125| loss: 0.28363 | val_0_auc: 0.90657 |  0:06:53s\n",
      "epoch 126| loss: 0.28655 | val_0_auc: 0.90618 |  0:06:56s\n",
      "epoch 127| loss: 0.28476 | val_0_auc: 0.90629 |  0:06:59s\n",
      "epoch 128| loss: 0.2841  | val_0_auc: 0.90845 |  0:07:03s\n",
      "epoch 129| loss: 0.28232 | val_0_auc: 0.9095  |  0:07:06s\n",
      "epoch 130| loss: 0.27984 | val_0_auc: 0.91005 |  0:07:09s\n",
      "epoch 131| loss: 0.28044 | val_0_auc: 0.9104  |  0:07:13s\n",
      "epoch 132| loss: 0.28004 | val_0_auc: 0.90976 |  0:07:16s\n",
      "epoch 133| loss: 0.28089 | val_0_auc: 0.91039 |  0:07:19s\n",
      "epoch 134| loss: 0.27798 | val_0_auc: 0.90934 |  0:07:22s\n",
      "epoch 135| loss: 0.2784  | val_0_auc: 0.91013 |  0:07:26s\n",
      "epoch 136| loss: 0.27822 | val_0_auc: 0.91125 |  0:07:29s\n",
      "epoch 137| loss: 0.28017 | val_0_auc: 0.91148 |  0:07:32s\n",
      "epoch 138| loss: 0.27924 | val_0_auc: 0.90906 |  0:07:36s\n",
      "epoch 139| loss: 0.27931 | val_0_auc: 0.9105  |  0:07:39s\n",
      "epoch 140| loss: 0.2775  | val_0_auc: 0.91119 |  0:07:42s\n",
      "epoch 141| loss: 0.2772  | val_0_auc: 0.91104 |  0:07:46s\n",
      "epoch 142| loss: 0.27729 | val_0_auc: 0.91141 |  0:07:49s\n",
      "epoch 143| loss: 0.27758 | val_0_auc: 0.90818 |  0:07:52s\n",
      "epoch 144| loss: 0.27771 | val_0_auc: 0.90687 |  0:07:55s\n",
      "epoch 145| loss: 0.27498 | val_0_auc: 0.90702 |  0:07:58s\n",
      "epoch 146| loss: 0.2762  | val_0_auc: 0.91037 |  0:08:02s\n",
      "epoch 147| loss: 0.27601 | val_0_auc: 0.90724 |  0:08:05s\n",
      "epoch 148| loss: 0.27485 | val_0_auc: 0.90786 |  0:08:08s\n",
      "epoch 149| loss: 0.27598 | val_0_auc: 0.9065  |  0:08:11s\n",
      "epoch 150| loss: 0.27913 | val_0_auc: 0.90692 |  0:08:15s\n",
      "epoch 151| loss: 0.27748 | val_0_auc: 0.90869 |  0:08:18s\n",
      "epoch 152| loss: 0.28225 | val_0_auc: 0.90924 |  0:08:21s\n",
      "epoch 153| loss: 0.28281 | val_0_auc: 0.90889 |  0:08:24s\n",
      "epoch 154| loss: 0.27932 | val_0_auc: 0.9109  |  0:08:28s\n",
      "epoch 155| loss: 0.27921 | val_0_auc: 0.9109  |  0:08:31s\n",
      "epoch 156| loss: 0.2763  | val_0_auc: 0.91142 |  0:08:34s\n",
      "epoch 157| loss: 0.27572 | val_0_auc: 0.91155 |  0:08:38s\n",
      "epoch 158| loss: 0.27277 | val_0_auc: 0.91164 |  0:08:41s\n",
      "epoch 159| loss: 0.27115 | val_0_auc: 0.91206 |  0:08:44s\n",
      "epoch 160| loss: 0.27226 | val_0_auc: 0.91179 |  0:08:48s\n",
      "epoch 161| loss: 0.27183 | val_0_auc: 0.91212 |  0:08:51s\n",
      "epoch 162| loss: 0.27124 | val_0_auc: 0.91236 |  0:08:54s\n",
      "epoch 163| loss: 0.27234 | val_0_auc: 0.91214 |  0:08:58s\n",
      "epoch 164| loss: 0.27027 | val_0_auc: 0.91181 |  0:09:01s\n",
      "epoch 165| loss: 0.27275 | val_0_auc: 0.91234 |  0:09:06s\n",
      "epoch 166| loss: 0.27124 | val_0_auc: 0.91356 |  0:09:10s\n",
      "epoch 167| loss: 0.2699  | val_0_auc: 0.91373 |  0:09:13s\n",
      "epoch 168| loss: 0.27164 | val_0_auc: 0.91416 |  0:09:17s\n",
      "epoch 169| loss: 0.26662 | val_0_auc: 0.914   |  0:09:20s\n",
      "epoch 170| loss: 0.26791 | val_0_auc: 0.91235 |  0:09:24s\n",
      "epoch 171| loss: 0.26703 | val_0_auc: 0.91388 |  0:09:28s\n",
      "epoch 172| loss: 0.26943 | val_0_auc: 0.91363 |  0:09:32s\n",
      "epoch 173| loss: 0.26888 | val_0_auc: 0.91379 |  0:09:35s\n",
      "epoch 174| loss: 0.27018 | val_0_auc: 0.91345 |  0:09:39s\n",
      "epoch 175| loss: 0.27015 | val_0_auc: 0.91266 |  0:09:43s\n",
      "epoch 176| loss: 0.267   | val_0_auc: 0.91278 |  0:09:46s\n",
      "epoch 177| loss: 0.26647 | val_0_auc: 0.91243 |  0:09:49s\n",
      "epoch 178| loss: 0.26858 | val_0_auc: 0.9126  |  0:09:53s\n",
      "epoch 179| loss: 0.26721 | val_0_auc: 0.91417 |  0:09:56s\n",
      "epoch 180| loss: 0.26973 | val_0_auc: 0.91316 |  0:09:59s\n",
      "epoch 181| loss: 0.26722 | val_0_auc: 0.91292 |  0:10:02s\n",
      "epoch 182| loss: 0.26566 | val_0_auc: 0.9129  |  0:10:06s\n",
      "epoch 183| loss: 0.26785 | val_0_auc: 0.91401 |  0:10:09s\n",
      "epoch 184| loss: 0.26619 | val_0_auc: 0.9137  |  0:10:12s\n",
      "epoch 185| loss: 0.26677 | val_0_auc: 0.91419 |  0:10:15s\n",
      "epoch 186| loss: 0.26595 | val_0_auc: 0.91352 |  0:10:19s\n",
      "epoch 187| loss: 0.26781 | val_0_auc: 0.91275 |  0:10:22s\n",
      "epoch 188| loss: 0.26552 | val_0_auc: 0.91356 |  0:10:25s\n",
      "epoch 189| loss: 0.26558 | val_0_auc: 0.91289 |  0:10:28s\n",
      "epoch 190| loss: 0.2658  | val_0_auc: 0.91097 |  0:10:32s\n",
      "epoch 191| loss: 0.26714 | val_0_auc: 0.91288 |  0:10:35s\n",
      "epoch 192| loss: 0.26719 | val_0_auc: 0.91376 |  0:10:38s\n",
      "epoch 193| loss: 0.2652  | val_0_auc: 0.91434 |  0:10:42s\n",
      "epoch 194| loss: 0.26666 | val_0_auc: 0.91335 |  0:10:45s\n",
      "epoch 195| loss: 0.26608 | val_0_auc: 0.91431 |  0:10:48s\n",
      "epoch 196| loss: 0.2648  | val_0_auc: 0.91323 |  0:10:51s\n",
      "epoch 197| loss: 0.26652 | val_0_auc: 0.91331 |  0:10:55s\n",
      "epoch 198| loss: 0.26642 | val_0_auc: 0.91438 |  0:10:58s\n",
      "epoch 199| loss: 0.26902 | val_0_auc: 0.91435 |  0:11:01s\n",
      "epoch 200| loss: 0.26601 | val_0_auc: 0.91422 |  0:11:05s\n",
      "epoch 201| loss: 0.2658  | val_0_auc: 0.91516 |  0:11:08s\n",
      "epoch 202| loss: 0.26594 | val_0_auc: 0.91509 |  0:11:11s\n",
      "epoch 203| loss: 0.26422 | val_0_auc: 0.91626 |  0:11:14s\n",
      "epoch 204| loss: 0.26405 | val_0_auc: 0.91493 |  0:11:18s\n",
      "epoch 205| loss: 0.26185 | val_0_auc: 0.91574 |  0:11:21s\n",
      "epoch 206| loss: 0.26223 | val_0_auc: 0.91554 |  0:11:24s\n",
      "epoch 207| loss: 0.2613  | val_0_auc: 0.91515 |  0:11:28s\n",
      "epoch 208| loss: 0.26284 | val_0_auc: 0.91503 |  0:11:31s\n",
      "epoch 209| loss: 0.26151 | val_0_auc: 0.91498 |  0:11:34s\n",
      "epoch 210| loss: 0.25973 | val_0_auc: 0.91519 |  0:11:37s\n",
      "epoch 211| loss: 0.2583  | val_0_auc: 0.91446 |  0:11:41s\n",
      "epoch 212| loss: 0.26134 | val_0_auc: 0.91529 |  0:11:44s\n",
      "epoch 213| loss: 0.25835 | val_0_auc: 0.91588 |  0:11:47s\n",
      "epoch 214| loss: 0.2597  | val_0_auc: 0.91672 |  0:11:50s\n",
      "epoch 215| loss: 0.25945 | val_0_auc: 0.91682 |  0:11:54s\n",
      "epoch 216| loss: 0.25932 | val_0_auc: 0.91419 |  0:11:57s\n",
      "epoch 217| loss: 0.26135 | val_0_auc: 0.91404 |  0:12:00s\n",
      "epoch 218| loss: 0.26158 | val_0_auc: 0.91443 |  0:12:04s\n",
      "epoch 219| loss: 0.26037 | val_0_auc: 0.91383 |  0:12:07s\n",
      "epoch 220| loss: 0.25987 | val_0_auc: 0.91427 |  0:12:10s\n",
      "epoch 221| loss: 0.25977 | val_0_auc: 0.91404 |  0:12:14s\n",
      "epoch 222| loss: 0.26039 | val_0_auc: 0.91358 |  0:12:17s\n",
      "epoch 223| loss: 0.25917 | val_0_auc: 0.91213 |  0:12:20s\n",
      "epoch 224| loss: 0.25973 | val_0_auc: 0.913   |  0:12:24s\n",
      "epoch 225| loss: 0.25831 | val_0_auc: 0.91291 |  0:12:27s\n",
      "epoch 226| loss: 0.2587  | val_0_auc: 0.91306 |  0:12:30s\n",
      "epoch 227| loss: 0.25956 | val_0_auc: 0.9134  |  0:12:33s\n",
      "epoch 228| loss: 0.25647 | val_0_auc: 0.91396 |  0:12:37s\n",
      "epoch 229| loss: 0.25895 | val_0_auc: 0.91437 |  0:12:40s\n",
      "epoch 230| loss: 0.25768 | val_0_auc: 0.91493 |  0:12:43s\n",
      "epoch 231| loss: 0.25845 | val_0_auc: 0.91427 |  0:12:46s\n",
      "epoch 232| loss: 0.25745 | val_0_auc: 0.91461 |  0:12:50s\n",
      "epoch 233| loss: 0.25578 | val_0_auc: 0.91436 |  0:12:53s\n",
      "epoch 234| loss: 0.25447 | val_0_auc: 0.91588 |  0:12:56s\n",
      "epoch 235| loss: 0.25361 | val_0_auc: 0.91503 |  0:13:00s\n",
      "\n",
      "Early stopping occurred at epoch 235 with best_epoch = 215 and best_val_0_auc = 0.91682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.90113 | val_0_auc: 0.63951 |  0:00:01s\n",
      "epoch 1  | loss: 0.69731 | val_0_auc: 0.77876 |  0:00:03s\n",
      "epoch 2  | loss: 0.62132 | val_0_auc: 0.7986  |  0:00:05s\n",
      "epoch 3  | loss: 0.57145 | val_0_auc: 0.82037 |  0:00:07s\n",
      "epoch 4  | loss: 0.52619 | val_0_auc: 0.82953 |  0:00:09s\n",
      "epoch 5  | loss: 0.50315 | val_0_auc: 0.83951 |  0:00:11s\n",
      "epoch 6  | loss: 0.48044 | val_0_auc: 0.84708 |  0:00:13s\n",
      "epoch 7  | loss: 0.46274 | val_0_auc: 0.85472 |  0:00:15s\n",
      "epoch 8  | loss: 0.4456  | val_0_auc: 0.85891 |  0:00:17s\n",
      "epoch 9  | loss: 0.43177 | val_0_auc: 0.86166 |  0:00:19s\n",
      "epoch 10 | loss: 0.42338 | val_0_auc: 0.86434 |  0:00:21s\n",
      "epoch 11 | loss: 0.40961 | val_0_auc: 0.86836 |  0:00:23s\n",
      "epoch 12 | loss: 0.40325 | val_0_auc: 0.87248 |  0:00:25s\n",
      "epoch 13 | loss: 0.39683 | val_0_auc: 0.87168 |  0:00:27s\n",
      "epoch 14 | loss: 0.38749 | val_0_auc: 0.87472 |  0:00:29s\n",
      "epoch 15 | loss: 0.38217 | val_0_auc: 0.87745 |  0:00:31s\n",
      "epoch 16 | loss: 0.37637 | val_0_auc: 0.87915 |  0:00:33s\n",
      "epoch 17 | loss: 0.36791 | val_0_auc: 0.87963 |  0:00:35s\n",
      "epoch 18 | loss: 0.36474 | val_0_auc: 0.8819  |  0:00:37s\n",
      "epoch 19 | loss: 0.3572  | val_0_auc: 0.88422 |  0:00:39s\n",
      "epoch 20 | loss: 0.35209 | val_0_auc: 0.88352 |  0:00:41s\n",
      "epoch 21 | loss: 0.34907 | val_0_auc: 0.88376 |  0:00:42s\n",
      "epoch 22 | loss: 0.34976 | val_0_auc: 0.88592 |  0:00:44s\n",
      "epoch 23 | loss: 0.34291 | val_0_auc: 0.88404 |  0:00:46s\n",
      "epoch 24 | loss: 0.34089 | val_0_auc: 0.88628 |  0:00:48s\n",
      "epoch 25 | loss: 0.33795 | val_0_auc: 0.88454 |  0:00:50s\n",
      "epoch 26 | loss: 0.33274 | val_0_auc: 0.88839 |  0:00:52s\n",
      "epoch 27 | loss: 0.33402 | val_0_auc: 0.89154 |  0:00:54s\n",
      "epoch 28 | loss: 0.33571 | val_0_auc: 0.89083 |  0:00:56s\n",
      "epoch 29 | loss: 0.33033 | val_0_auc: 0.89065 |  0:00:58s\n",
      "epoch 30 | loss: 0.32674 | val_0_auc: 0.89207 |  0:01:00s\n",
      "epoch 31 | loss: 0.32442 | val_0_auc: 0.8907  |  0:01:02s\n",
      "epoch 32 | loss: 0.32927 | val_0_auc: 0.89044 |  0:01:04s\n",
      "epoch 33 | loss: 0.33082 | val_0_auc: 0.89162 |  0:01:06s\n",
      "epoch 34 | loss: 0.32649 | val_0_auc: 0.88808 |  0:01:07s\n",
      "epoch 35 | loss: 0.32507 | val_0_auc: 0.89244 |  0:01:09s\n",
      "epoch 36 | loss: 0.32264 | val_0_auc: 0.89341 |  0:01:11s\n",
      "epoch 37 | loss: 0.32021 | val_0_auc: 0.89449 |  0:01:13s\n",
      "epoch 38 | loss: 0.32611 | val_0_auc: 0.89226 |  0:01:15s\n",
      "epoch 39 | loss: 0.32453 | val_0_auc: 0.89206 |  0:01:17s\n",
      "epoch 40 | loss: 0.32396 | val_0_auc: 0.89085 |  0:01:19s\n",
      "epoch 41 | loss: 0.31889 | val_0_auc: 0.89177 |  0:01:21s\n",
      "epoch 42 | loss: 0.31766 | val_0_auc: 0.88897 |  0:01:23s\n",
      "epoch 43 | loss: 0.31951 | val_0_auc: 0.88961 |  0:01:25s\n",
      "epoch 44 | loss: 0.31894 | val_0_auc: 0.89345 |  0:01:27s\n",
      "epoch 45 | loss: 0.31591 | val_0_auc: 0.88931 |  0:01:29s\n",
      "epoch 46 | loss: 0.31599 | val_0_auc: 0.89286 |  0:01:30s\n",
      "epoch 47 | loss: 0.3132  | val_0_auc: 0.89448 |  0:01:32s\n",
      "epoch 48 | loss: 0.3113  | val_0_auc: 0.89197 |  0:01:34s\n",
      "epoch 49 | loss: 0.31173 | val_0_auc: 0.89298 |  0:01:36s\n",
      "epoch 50 | loss: 0.31349 | val_0_auc: 0.89324 |  0:01:38s\n",
      "epoch 51 | loss: 0.31066 | val_0_auc: 0.89384 |  0:01:40s\n",
      "epoch 52 | loss: 0.31143 | val_0_auc: 0.89472 |  0:01:42s\n",
      "epoch 53 | loss: 0.31255 | val_0_auc: 0.89351 |  0:01:44s\n",
      "epoch 54 | loss: 0.3109  | val_0_auc: 0.89358 |  0:01:46s\n",
      "epoch 55 | loss: 0.30919 | val_0_auc: 0.89319 |  0:01:48s\n",
      "epoch 56 | loss: 0.30913 | val_0_auc: 0.88941 |  0:01:50s\n",
      "epoch 57 | loss: 0.30672 | val_0_auc: 0.89493 |  0:01:51s\n",
      "epoch 58 | loss: 0.30701 | val_0_auc: 0.89275 |  0:01:53s\n",
      "epoch 59 | loss: 0.30962 | val_0_auc: 0.89399 |  0:01:55s\n",
      "epoch 60 | loss: 0.30638 | val_0_auc: 0.89443 |  0:01:57s\n",
      "epoch 61 | loss: 0.30457 | val_0_auc: 0.89842 |  0:01:59s\n",
      "epoch 62 | loss: 0.30436 | val_0_auc: 0.89747 |  0:02:01s\n",
      "epoch 63 | loss: 0.30828 | val_0_auc: 0.89542 |  0:02:03s\n",
      "epoch 64 | loss: 0.30452 | val_0_auc: 0.89829 |  0:02:05s\n",
      "epoch 65 | loss: 0.30276 | val_0_auc: 0.89762 |  0:02:07s\n",
      "epoch 66 | loss: 0.30382 | val_0_auc: 0.89778 |  0:02:09s\n",
      "epoch 67 | loss: 0.30376 | val_0_auc: 0.89711 |  0:02:11s\n",
      "epoch 68 | loss: 0.30502 | val_0_auc: 0.89523 |  0:02:12s\n",
      "epoch 69 | loss: 0.30198 | val_0_auc: 0.89701 |  0:02:14s\n",
      "epoch 70 | loss: 0.30165 | val_0_auc: 0.89806 |  0:02:16s\n",
      "epoch 71 | loss: 0.30122 | val_0_auc: 0.89283 |  0:02:18s\n",
      "epoch 72 | loss: 0.30351 | val_0_auc: 0.89571 |  0:02:20s\n",
      "epoch 73 | loss: 0.30258 | val_0_auc: 0.89109 |  0:02:22s\n",
      "epoch 74 | loss: 0.30168 | val_0_auc: 0.89565 |  0:02:24s\n",
      "epoch 75 | loss: 0.30238 | val_0_auc: 0.89669 |  0:02:26s\n",
      "epoch 76 | loss: 0.30129 | val_0_auc: 0.8979  |  0:02:28s\n",
      "epoch 77 | loss: 0.30029 | val_0_auc: 0.89817 |  0:02:30s\n",
      "epoch 78 | loss: 0.30081 | val_0_auc: 0.89749 |  0:02:31s\n",
      "epoch 79 | loss: 0.30353 | val_0_auc: 0.89638 |  0:02:33s\n",
      "epoch 80 | loss: 0.30487 | val_0_auc: 0.89631 |  0:02:35s\n",
      "epoch 81 | loss: 0.30499 | val_0_auc: 0.89294 |  0:02:37s\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_auc = 0.89842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6436  | val_0_auc: 0.53124 |  0:00:00s\n",
      "epoch 1  | loss: 0.5853  | val_0_auc: 0.54108 |  0:00:01s\n",
      "epoch 2  | loss: 0.54934 | val_0_auc: 0.58092 |  0:00:02s\n",
      "epoch 3  | loss: 0.52265 | val_0_auc: 0.61405 |  0:00:02s\n",
      "epoch 4  | loss: 0.50749 | val_0_auc: 0.64915 |  0:00:03s\n",
      "epoch 5  | loss: 0.49081 | val_0_auc: 0.68366 |  0:00:04s\n",
      "epoch 6  | loss: 0.48077 | val_0_auc: 0.71573 |  0:00:05s\n",
      "epoch 7  | loss: 0.4716  | val_0_auc: 0.738   |  0:00:05s\n",
      "epoch 8  | loss: 0.46315 | val_0_auc: 0.75095 |  0:00:06s\n",
      "epoch 9  | loss: 0.45742 | val_0_auc: 0.76472 |  0:00:07s\n",
      "epoch 10 | loss: 0.45033 | val_0_auc: 0.77635 |  0:00:08s\n",
      "epoch 11 | loss: 0.44597 | val_0_auc: 0.78416 |  0:00:08s\n",
      "epoch 12 | loss: 0.44044 | val_0_auc: 0.79122 |  0:00:09s\n",
      "epoch 13 | loss: 0.43616 | val_0_auc: 0.79897 |  0:00:10s\n",
      "epoch 14 | loss: 0.43177 | val_0_auc: 0.80422 |  0:00:10s\n",
      "epoch 15 | loss: 0.42849 | val_0_auc: 0.80999 |  0:00:11s\n",
      "epoch 16 | loss: 0.42421 | val_0_auc: 0.81432 |  0:00:12s\n",
      "epoch 17 | loss: 0.42242 | val_0_auc: 0.81749 |  0:00:13s\n",
      "epoch 18 | loss: 0.4193  | val_0_auc: 0.82147 |  0:00:13s\n",
      "epoch 19 | loss: 0.4178  | val_0_auc: 0.82573 |  0:00:14s\n",
      "epoch 20 | loss: 0.4139  | val_0_auc: 0.8281  |  0:00:15s\n",
      "epoch 21 | loss: 0.40806 | val_0_auc: 0.83119 |  0:00:15s\n",
      "epoch 22 | loss: 0.40684 | val_0_auc: 0.83316 |  0:00:16s\n",
      "epoch 23 | loss: 0.40506 | val_0_auc: 0.83606 |  0:00:17s\n",
      "epoch 24 | loss: 0.40269 | val_0_auc: 0.83844 |  0:00:18s\n",
      "epoch 25 | loss: 0.39975 | val_0_auc: 0.83986 |  0:00:18s\n",
      "epoch 26 | loss: 0.39872 | val_0_auc: 0.84217 |  0:00:19s\n",
      "epoch 27 | loss: 0.39695 | val_0_auc: 0.84382 |  0:00:20s\n",
      "epoch 28 | loss: 0.39652 | val_0_auc: 0.84617 |  0:00:20s\n",
      "epoch 29 | loss: 0.39205 | val_0_auc: 0.84871 |  0:00:21s\n",
      "epoch 30 | loss: 0.39197 | val_0_auc: 0.84978 |  0:00:22s\n",
      "epoch 31 | loss: 0.39014 | val_0_auc: 0.85069 |  0:00:23s\n",
      "epoch 32 | loss: 0.38873 | val_0_auc: 0.85156 |  0:00:24s\n",
      "epoch 33 | loss: 0.38509 | val_0_auc: 0.85323 |  0:00:24s\n",
      "epoch 34 | loss: 0.38402 | val_0_auc: 0.85322 |  0:00:25s\n",
      "epoch 35 | loss: 0.38105 | val_0_auc: 0.85398 |  0:00:26s\n",
      "epoch 36 | loss: 0.38144 | val_0_auc: 0.8541  |  0:00:27s\n",
      "epoch 37 | loss: 0.38105 | val_0_auc: 0.8558  |  0:00:27s\n",
      "epoch 38 | loss: 0.3781  | val_0_auc: 0.85726 |  0:00:28s\n",
      "epoch 39 | loss: 0.37747 | val_0_auc: 0.85782 |  0:00:29s\n",
      "epoch 40 | loss: 0.37394 | val_0_auc: 0.85854 |  0:00:29s\n",
      "epoch 41 | loss: 0.37373 | val_0_auc: 0.85894 |  0:00:30s\n",
      "epoch 42 | loss: 0.3736  | val_0_auc: 0.85958 |  0:00:31s\n",
      "epoch 43 | loss: 0.37308 | val_0_auc: 0.86083 |  0:00:32s\n",
      "epoch 44 | loss: 0.36999 | val_0_auc: 0.86174 |  0:00:32s\n",
      "epoch 45 | loss: 0.36904 | val_0_auc: 0.86226 |  0:00:33s\n",
      "epoch 46 | loss: 0.36888 | val_0_auc: 0.86284 |  0:00:34s\n",
      "epoch 47 | loss: 0.36778 | val_0_auc: 0.86319 |  0:00:34s\n",
      "epoch 48 | loss: 0.36469 | val_0_auc: 0.8639  |  0:00:35s\n",
      "epoch 49 | loss: 0.36463 | val_0_auc: 0.865   |  0:00:36s\n",
      "epoch 50 | loss: 0.36401 | val_0_auc: 0.86599 |  0:00:37s\n",
      "epoch 51 | loss: 0.36335 | val_0_auc: 0.86683 |  0:00:37s\n",
      "epoch 52 | loss: 0.36438 | val_0_auc: 0.86754 |  0:00:38s\n",
      "epoch 53 | loss: 0.36205 | val_0_auc: 0.86817 |  0:00:39s\n",
      "epoch 54 | loss: 0.36061 | val_0_auc: 0.86863 |  0:00:39s\n",
      "epoch 55 | loss: 0.35973 | val_0_auc: 0.86915 |  0:00:40s\n",
      "epoch 56 | loss: 0.36001 | val_0_auc: 0.86984 |  0:00:41s\n",
      "epoch 57 | loss: 0.35931 | val_0_auc: 0.87015 |  0:00:42s\n",
      "epoch 58 | loss: 0.35772 | val_0_auc: 0.8709  |  0:00:43s\n",
      "epoch 59 | loss: 0.35541 | val_0_auc: 0.87126 |  0:00:43s\n",
      "epoch 60 | loss: 0.356   | val_0_auc: 0.87164 |  0:00:44s\n",
      "epoch 61 | loss: 0.35095 | val_0_auc: 0.87148 |  0:00:45s\n",
      "epoch 62 | loss: 0.35284 | val_0_auc: 0.87229 |  0:00:45s\n",
      "epoch 63 | loss: 0.35217 | val_0_auc: 0.8725  |  0:00:46s\n",
      "epoch 64 | loss: 0.34955 | val_0_auc: 0.87298 |  0:00:47s\n",
      "epoch 65 | loss: 0.35172 | val_0_auc: 0.8729  |  0:00:48s\n",
      "epoch 66 | loss: 0.3509  | val_0_auc: 0.87362 |  0:00:48s\n",
      "epoch 67 | loss: 0.35036 | val_0_auc: 0.87382 |  0:00:49s\n",
      "epoch 68 | loss: 0.34978 | val_0_auc: 0.87384 |  0:00:50s\n",
      "epoch 69 | loss: 0.34715 | val_0_auc: 0.87458 |  0:00:50s\n",
      "epoch 70 | loss: 0.34676 | val_0_auc: 0.87483 |  0:00:51s\n",
      "epoch 71 | loss: 0.34679 | val_0_auc: 0.87491 |  0:00:52s\n",
      "epoch 72 | loss: 0.34506 | val_0_auc: 0.8755  |  0:00:53s\n",
      "epoch 73 | loss: 0.3462  | val_0_auc: 0.87595 |  0:00:53s\n",
      "epoch 74 | loss: 0.34532 | val_0_auc: 0.87675 |  0:00:54s\n",
      "epoch 75 | loss: 0.34439 | val_0_auc: 0.87732 |  0:00:55s\n",
      "epoch 76 | loss: 0.34248 | val_0_auc: 0.8771  |  0:00:55s\n",
      "epoch 77 | loss: 0.3421  | val_0_auc: 0.87748 |  0:00:56s\n",
      "epoch 78 | loss: 0.34206 | val_0_auc: 0.87805 |  0:00:57s\n",
      "epoch 79 | loss: 0.34164 | val_0_auc: 0.87828 |  0:00:58s\n",
      "epoch 80 | loss: 0.34069 | val_0_auc: 0.87855 |  0:00:59s\n",
      "epoch 81 | loss: 0.33989 | val_0_auc: 0.87869 |  0:00:59s\n",
      "epoch 82 | loss: 0.34047 | val_0_auc: 0.87939 |  0:01:00s\n",
      "epoch 83 | loss: 0.34024 | val_0_auc: 0.87975 |  0:01:01s\n",
      "epoch 84 | loss: 0.33834 | val_0_auc: 0.87975 |  0:01:02s\n",
      "epoch 85 | loss: 0.33811 | val_0_auc: 0.88    |  0:01:02s\n",
      "epoch 86 | loss: 0.33616 | val_0_auc: 0.88012 |  0:01:03s\n",
      "epoch 87 | loss: 0.33528 | val_0_auc: 0.88096 |  0:01:04s\n",
      "epoch 88 | loss: 0.33566 | val_0_auc: 0.88114 |  0:01:05s\n",
      "epoch 89 | loss: 0.3339  | val_0_auc: 0.8816  |  0:01:05s\n",
      "epoch 90 | loss: 0.33408 | val_0_auc: 0.8819  |  0:01:06s\n",
      "epoch 91 | loss: 0.33225 | val_0_auc: 0.88188 |  0:01:07s\n",
      "epoch 92 | loss: 0.33556 | val_0_auc: 0.88235 |  0:01:08s\n",
      "epoch 93 | loss: 0.33192 | val_0_auc: 0.88261 |  0:01:08s\n",
      "epoch 94 | loss: 0.33277 | val_0_auc: 0.88233 |  0:01:09s\n",
      "epoch 95 | loss: 0.33221 | val_0_auc: 0.88323 |  0:01:10s\n",
      "epoch 96 | loss: 0.33234 | val_0_auc: 0.88409 |  0:01:10s\n",
      "epoch 97 | loss: 0.32968 | val_0_auc: 0.88421 |  0:01:11s\n",
      "epoch 98 | loss: 0.32855 | val_0_auc: 0.88427 |  0:01:12s\n",
      "epoch 99 | loss: 0.32855 | val_0_auc: 0.88527 |  0:01:13s\n",
      "epoch 100| loss: 0.32627 | val_0_auc: 0.88536 |  0:01:13s\n",
      "epoch 101| loss: 0.32501 | val_0_auc: 0.88614 |  0:01:14s\n",
      "epoch 102| loss: 0.32582 | val_0_auc: 0.88636 |  0:01:15s\n",
      "epoch 103| loss: 0.32616 | val_0_auc: 0.88683 |  0:01:15s\n",
      "epoch 104| loss: 0.3227  | val_0_auc: 0.88713 |  0:01:16s\n",
      "epoch 105| loss: 0.32187 | val_0_auc: 0.88764 |  0:01:17s\n",
      "epoch 106| loss: 0.32187 | val_0_auc: 0.88809 |  0:01:18s\n",
      "epoch 107| loss: 0.32062 | val_0_auc: 0.88785 |  0:01:18s\n",
      "epoch 108| loss: 0.32208 | val_0_auc: 0.88818 |  0:01:19s\n",
      "epoch 109| loss: 0.32062 | val_0_auc: 0.88895 |  0:01:20s\n",
      "epoch 110| loss: 0.32156 | val_0_auc: 0.88907 |  0:01:20s\n",
      "epoch 111| loss: 0.3183  | val_0_auc: 0.88952 |  0:01:21s\n",
      "epoch 112| loss: 0.31716 | val_0_auc: 0.89001 |  0:01:22s\n",
      "epoch 113| loss: 0.31884 | val_0_auc: 0.89055 |  0:01:23s\n",
      "epoch 114| loss: 0.31624 | val_0_auc: 0.89095 |  0:01:23s\n",
      "epoch 115| loss: 0.31489 | val_0_auc: 0.89117 |  0:01:24s\n",
      "epoch 116| loss: 0.31573 | val_0_auc: 0.89138 |  0:01:25s\n",
      "epoch 117| loss: 0.31488 | val_0_auc: 0.8914  |  0:01:25s\n",
      "epoch 118| loss: 0.31272 | val_0_auc: 0.89223 |  0:01:26s\n",
      "epoch 119| loss: 0.31157 | val_0_auc: 0.89236 |  0:01:27s\n",
      "epoch 120| loss: 0.31182 | val_0_auc: 0.89237 |  0:01:28s\n",
      "epoch 121| loss: 0.3105  | val_0_auc: 0.89254 |  0:01:28s\n",
      "epoch 122| loss: 0.31016 | val_0_auc: 0.89345 |  0:01:29s\n",
      "epoch 123| loss: 0.31148 | val_0_auc: 0.89391 |  0:01:30s\n",
      "epoch 124| loss: 0.3115  | val_0_auc: 0.89375 |  0:01:30s\n",
      "epoch 125| loss: 0.30965 | val_0_auc: 0.89384 |  0:01:31s\n",
      "epoch 126| loss: 0.3084  | val_0_auc: 0.89438 |  0:01:32s\n",
      "epoch 127| loss: 0.30979 | val_0_auc: 0.89484 |  0:01:33s\n",
      "epoch 128| loss: 0.30929 | val_0_auc: 0.89475 |  0:01:33s\n",
      "epoch 129| loss: 0.30739 | val_0_auc: 0.89483 |  0:01:34s\n",
      "epoch 130| loss: 0.30653 | val_0_auc: 0.89548 |  0:01:35s\n",
      "epoch 131| loss: 0.30587 | val_0_auc: 0.8954  |  0:01:35s\n",
      "epoch 132| loss: 0.30668 | val_0_auc: 0.8959  |  0:01:36s\n",
      "epoch 133| loss: 0.30462 | val_0_auc: 0.89597 |  0:01:37s\n",
      "epoch 134| loss: 0.30647 | val_0_auc: 0.89633 |  0:01:38s\n",
      "epoch 135| loss: 0.30594 | val_0_auc: 0.89608 |  0:01:38s\n",
      "epoch 136| loss: 0.30402 | val_0_auc: 0.89643 |  0:01:39s\n",
      "epoch 137| loss: 0.30459 | val_0_auc: 0.89658 |  0:01:40s\n",
      "epoch 138| loss: 0.30333 | val_0_auc: 0.89668 |  0:01:40s\n",
      "epoch 139| loss: 0.30512 | val_0_auc: 0.89665 |  0:01:41s\n",
      "epoch 140| loss: 0.30466 | val_0_auc: 0.89664 |  0:01:42s\n",
      "epoch 141| loss: 0.30232 | val_0_auc: 0.89685 |  0:01:43s\n",
      "epoch 142| loss: 0.30205 | val_0_auc: 0.89648 |  0:01:43s\n",
      "epoch 143| loss: 0.29973 | val_0_auc: 0.89702 |  0:01:44s\n",
      "epoch 144| loss: 0.30285 | val_0_auc: 0.89715 |  0:01:45s\n",
      "epoch 145| loss: 0.30172 | val_0_auc: 0.89729 |  0:01:45s\n",
      "epoch 146| loss: 0.30017 | val_0_auc: 0.8975  |  0:01:46s\n",
      "epoch 147| loss: 0.30018 | val_0_auc: 0.89769 |  0:01:47s\n",
      "epoch 148| loss: 0.30295 | val_0_auc: 0.89781 |  0:01:48s\n",
      "epoch 149| loss: 0.30065 | val_0_auc: 0.89763 |  0:01:48s\n",
      "epoch 150| loss: 0.30062 | val_0_auc: 0.89789 |  0:01:49s\n",
      "epoch 151| loss: 0.29916 | val_0_auc: 0.89808 |  0:01:50s\n",
      "epoch 152| loss: 0.29768 | val_0_auc: 0.89801 |  0:01:51s\n",
      "epoch 153| loss: 0.30029 | val_0_auc: 0.89848 |  0:01:51s\n",
      "epoch 154| loss: 0.29906 | val_0_auc: 0.89749 |  0:01:52s\n",
      "epoch 155| loss: 0.29956 | val_0_auc: 0.89844 |  0:01:53s\n",
      "epoch 156| loss: 0.29841 | val_0_auc: 0.89907 |  0:01:53s\n",
      "epoch 157| loss: 0.29781 | val_0_auc: 0.89891 |  0:01:54s\n",
      "epoch 158| loss: 0.29908 | val_0_auc: 0.899   |  0:01:55s\n",
      "epoch 159| loss: 0.29799 | val_0_auc: 0.89857 |  0:01:56s\n",
      "epoch 160| loss: 0.29975 | val_0_auc: 0.89858 |  0:01:56s\n",
      "epoch 161| loss: 0.29695 | val_0_auc: 0.89914 |  0:01:57s\n",
      "epoch 162| loss: 0.29643 | val_0_auc: 0.89934 |  0:01:58s\n",
      "epoch 163| loss: 0.29544 | val_0_auc: 0.89892 |  0:01:58s\n",
      "epoch 164| loss: 0.29646 | val_0_auc: 0.89885 |  0:01:59s\n",
      "epoch 165| loss: 0.29463 | val_0_auc: 0.899   |  0:02:00s\n",
      "epoch 166| loss: 0.29499 | val_0_auc: 0.89913 |  0:02:01s\n",
      "epoch 167| loss: 0.29355 | val_0_auc: 0.89943 |  0:02:01s\n",
      "epoch 168| loss: 0.29377 | val_0_auc: 0.8996  |  0:02:02s\n",
      "epoch 169| loss: 0.29398 | val_0_auc: 0.89929 |  0:02:03s\n",
      "epoch 170| loss: 0.29571 | val_0_auc: 0.89939 |  0:02:03s\n",
      "epoch 171| loss: 0.29314 | val_0_auc: 0.89943 |  0:02:04s\n",
      "epoch 172| loss: 0.29447 | val_0_auc: 0.89969 |  0:02:05s\n",
      "epoch 173| loss: 0.29531 | val_0_auc: 0.89984 |  0:02:06s\n",
      "epoch 174| loss: 0.2938  | val_0_auc: 0.89985 |  0:02:06s\n",
      "epoch 175| loss: 0.29473 | val_0_auc: 0.90031 |  0:02:07s\n",
      "epoch 176| loss: 0.29371 | val_0_auc: 0.90004 |  0:02:08s\n",
      "epoch 177| loss: 0.29203 | val_0_auc: 0.89978 |  0:02:08s\n",
      "epoch 178| loss: 0.29451 | val_0_auc: 0.90043 |  0:02:09s\n",
      "epoch 179| loss: 0.29066 | val_0_auc: 0.90006 |  0:02:10s\n",
      "epoch 180| loss: 0.292   | val_0_auc: 0.89958 |  0:02:11s\n",
      "epoch 181| loss: 0.29273 | val_0_auc: 0.90055 |  0:02:11s\n",
      "epoch 182| loss: 0.2942  | val_0_auc: 0.90059 |  0:02:12s\n",
      "epoch 183| loss: 0.29218 | val_0_auc: 0.90069 |  0:02:13s\n",
      "epoch 184| loss: 0.29156 | val_0_auc: 0.90066 |  0:02:13s\n",
      "epoch 185| loss: 0.29199 | val_0_auc: 0.90067 |  0:02:14s\n",
      "epoch 186| loss: 0.28741 | val_0_auc: 0.90045 |  0:02:15s\n",
      "epoch 187| loss: 0.2905  | val_0_auc: 0.90049 |  0:02:16s\n",
      "epoch 188| loss: 0.29305 | val_0_auc: 0.90059 |  0:02:16s\n",
      "epoch 189| loss: 0.29136 | val_0_auc: 0.90137 |  0:02:17s\n",
      "epoch 190| loss: 0.29307 | val_0_auc: 0.90148 |  0:02:18s\n",
      "epoch 191| loss: 0.2908  | val_0_auc: 0.90148 |  0:02:18s\n",
      "epoch 192| loss: 0.28855 | val_0_auc: 0.90148 |  0:02:19s\n",
      "epoch 193| loss: 0.28987 | val_0_auc: 0.90148 |  0:02:20s\n",
      "epoch 194| loss: 0.28932 | val_0_auc: 0.90109 |  0:02:21s\n",
      "epoch 195| loss: 0.28946 | val_0_auc: 0.90146 |  0:02:21s\n",
      "epoch 196| loss: 0.29057 | val_0_auc: 0.90164 |  0:02:22s\n",
      "epoch 197| loss: 0.28868 | val_0_auc: 0.90157 |  0:02:23s\n",
      "epoch 198| loss: 0.29049 | val_0_auc: 0.90161 |  0:02:24s\n",
      "epoch 199| loss: 0.29117 | val_0_auc: 0.90139 |  0:02:24s\n",
      "epoch 200| loss: 0.28862 | val_0_auc: 0.9019  |  0:02:25s\n",
      "epoch 201| loss: 0.28664 | val_0_auc: 0.90152 |  0:02:26s\n",
      "epoch 202| loss: 0.28876 | val_0_auc: 0.90115 |  0:02:26s\n",
      "epoch 203| loss: 0.28872 | val_0_auc: 0.90136 |  0:02:27s\n",
      "epoch 204| loss: 0.28864 | val_0_auc: 0.90226 |  0:02:28s\n",
      "epoch 205| loss: 0.28767 | val_0_auc: 0.90174 |  0:02:29s\n",
      "epoch 206| loss: 0.28817 | val_0_auc: 0.90178 |  0:02:29s\n",
      "epoch 207| loss: 0.28682 | val_0_auc: 0.90199 |  0:02:30s\n",
      "epoch 208| loss: 0.28777 | val_0_auc: 0.9021  |  0:02:31s\n",
      "epoch 209| loss: 0.28609 | val_0_auc: 0.90254 |  0:02:31s\n",
      "epoch 210| loss: 0.28525 | val_0_auc: 0.9025  |  0:02:32s\n",
      "epoch 211| loss: 0.28503 | val_0_auc: 0.90243 |  0:02:33s\n",
      "epoch 212| loss: 0.28597 | val_0_auc: 0.90223 |  0:02:33s\n",
      "epoch 213| loss: 0.28523 | val_0_auc: 0.90206 |  0:02:34s\n",
      "epoch 214| loss: 0.2868  | val_0_auc: 0.90222 |  0:02:35s\n",
      "epoch 215| loss: 0.28481 | val_0_auc: 0.90228 |  0:02:36s\n",
      "epoch 216| loss: 0.28615 | val_0_auc: 0.90268 |  0:02:36s\n",
      "epoch 217| loss: 0.28638 | val_0_auc: 0.90219 |  0:02:37s\n",
      "epoch 218| loss: 0.28728 | val_0_auc: 0.90217 |  0:02:38s\n",
      "epoch 219| loss: 0.28622 | val_0_auc: 0.90238 |  0:02:38s\n",
      "epoch 220| loss: 0.28478 | val_0_auc: 0.90198 |  0:02:39s\n",
      "epoch 221| loss: 0.28292 | val_0_auc: 0.90251 |  0:02:40s\n",
      "epoch 222| loss: 0.28525 | val_0_auc: 0.90271 |  0:02:41s\n",
      "epoch 223| loss: 0.28444 | val_0_auc: 0.90261 |  0:02:41s\n",
      "epoch 224| loss: 0.28355 | val_0_auc: 0.9025  |  0:02:42s\n",
      "epoch 225| loss: 0.28108 | val_0_auc: 0.90241 |  0:02:43s\n",
      "epoch 226| loss: 0.28516 | val_0_auc: 0.90266 |  0:02:43s\n",
      "epoch 227| loss: 0.28293 | val_0_auc: 0.90285 |  0:02:44s\n",
      "epoch 228| loss: 0.28307 | val_0_auc: 0.90274 |  0:02:45s\n",
      "epoch 229| loss: 0.28308 | val_0_auc: 0.90285 |  0:02:46s\n",
      "epoch 230| loss: 0.28244 | val_0_auc: 0.90282 |  0:02:46s\n",
      "epoch 231| loss: 0.2823  | val_0_auc: 0.90264 |  0:02:47s\n",
      "epoch 232| loss: 0.28309 | val_0_auc: 0.90262 |  0:02:48s\n",
      "epoch 233| loss: 0.28352 | val_0_auc: 0.9031  |  0:02:49s\n",
      "epoch 234| loss: 0.28106 | val_0_auc: 0.90303 |  0:02:49s\n",
      "epoch 235| loss: 0.28337 | val_0_auc: 0.90375 |  0:02:50s\n",
      "epoch 236| loss: 0.28193 | val_0_auc: 0.9034  |  0:02:51s\n",
      "epoch 237| loss: 0.28378 | val_0_auc: 0.90354 |  0:02:52s\n",
      "epoch 238| loss: 0.28295 | val_0_auc: 0.9038  |  0:02:52s\n",
      "epoch 239| loss: 0.28376 | val_0_auc: 0.90394 |  0:02:53s\n",
      "epoch 240| loss: 0.28103 | val_0_auc: 0.90366 |  0:02:54s\n",
      "epoch 241| loss: 0.2794  | val_0_auc: 0.9034  |  0:02:54s\n",
      "epoch 242| loss: 0.282   | val_0_auc: 0.90376 |  0:02:55s\n",
      "epoch 243| loss: 0.28266 | val_0_auc: 0.90312 |  0:02:56s\n",
      "epoch 244| loss: 0.28154 | val_0_auc: 0.9035  |  0:02:57s\n",
      "epoch 245| loss: 0.28167 | val_0_auc: 0.90435 |  0:02:57s\n",
      "epoch 246| loss: 0.28101 | val_0_auc: 0.90346 |  0:02:58s\n",
      "epoch 247| loss: 0.27981 | val_0_auc: 0.90414 |  0:02:59s\n",
      "epoch 248| loss: 0.28066 | val_0_auc: 0.90386 |  0:03:00s\n",
      "epoch 249| loss: 0.28049 | val_0_auc: 0.90448 |  0:03:00s\n",
      "epoch 250| loss: 0.28019 | val_0_auc: 0.90444 |  0:03:01s\n",
      "epoch 251| loss: 0.28036 | val_0_auc: 0.90469 |  0:03:02s\n",
      "epoch 252| loss: 0.27955 | val_0_auc: 0.90474 |  0:03:02s\n",
      "epoch 253| loss: 0.27947 | val_0_auc: 0.9043  |  0:03:03s\n",
      "epoch 254| loss: 0.27874 | val_0_auc: 0.90463 |  0:03:04s\n",
      "epoch 255| loss: 0.27796 | val_0_auc: 0.90507 |  0:03:04s\n",
      "epoch 256| loss: 0.27945 | val_0_auc: 0.90492 |  0:03:05s\n",
      "epoch 257| loss: 0.2811  | val_0_auc: 0.90492 |  0:03:06s\n",
      "epoch 258| loss: 0.27806 | val_0_auc: 0.90486 |  0:03:07s\n",
      "epoch 259| loss: 0.27959 | val_0_auc: 0.90444 |  0:03:07s\n",
      "epoch 260| loss: 0.27882 | val_0_auc: 0.90472 |  0:03:08s\n",
      "epoch 261| loss: 0.27873 | val_0_auc: 0.90474 |  0:03:09s\n",
      "epoch 262| loss: 0.27911 | val_0_auc: 0.90532 |  0:03:10s\n",
      "epoch 263| loss: 0.27804 | val_0_auc: 0.90496 |  0:03:10s\n",
      "epoch 264| loss: 0.27922 | val_0_auc: 0.90549 |  0:03:11s\n",
      "epoch 265| loss: 0.27722 | val_0_auc: 0.90547 |  0:03:12s\n",
      "epoch 266| loss: 0.2779  | val_0_auc: 0.90528 |  0:03:12s\n",
      "epoch 267| loss: 0.27824 | val_0_auc: 0.90526 |  0:03:13s\n",
      "epoch 268| loss: 0.27774 | val_0_auc: 0.90558 |  0:03:14s\n",
      "epoch 269| loss: 0.27825 | val_0_auc: 0.90493 |  0:03:15s\n",
      "epoch 270| loss: 0.27871 | val_0_auc: 0.90505 |  0:03:15s\n",
      "epoch 271| loss: 0.27711 | val_0_auc: 0.90543 |  0:03:16s\n",
      "epoch 272| loss: 0.27726 | val_0_auc: 0.90547 |  0:03:17s\n",
      "epoch 273| loss: 0.27779 | val_0_auc: 0.90582 |  0:03:17s\n",
      "epoch 274| loss: 0.27755 | val_0_auc: 0.90539 |  0:03:18s\n",
      "epoch 275| loss: 0.27644 | val_0_auc: 0.9058  |  0:03:19s\n",
      "epoch 276| loss: 0.27682 | val_0_auc: 0.90549 |  0:03:20s\n",
      "epoch 277| loss: 0.27743 | val_0_auc: 0.90569 |  0:03:20s\n",
      "epoch 278| loss: 0.27837 | val_0_auc: 0.90642 |  0:03:21s\n",
      "epoch 279| loss: 0.27544 | val_0_auc: 0.90637 |  0:03:22s\n",
      "epoch 280| loss: 0.27694 | val_0_auc: 0.90571 |  0:03:23s\n",
      "epoch 281| loss: 0.27712 | val_0_auc: 0.90596 |  0:03:24s\n",
      "epoch 282| loss: 0.27698 | val_0_auc: 0.90636 |  0:03:25s\n",
      "epoch 283| loss: 0.27553 | val_0_auc: 0.90594 |  0:03:26s\n",
      "epoch 284| loss: 0.27593 | val_0_auc: 0.90631 |  0:03:26s\n",
      "epoch 285| loss: 0.27416 | val_0_auc: 0.90626 |  0:03:27s\n",
      "epoch 286| loss: 0.27803 | val_0_auc: 0.90618 |  0:03:28s\n",
      "epoch 287| loss: 0.27466 | val_0_auc: 0.90622 |  0:03:28s\n",
      "epoch 288| loss: 0.27641 | val_0_auc: 0.90662 |  0:03:29s\n",
      "epoch 289| loss: 0.27839 | val_0_auc: 0.90626 |  0:03:30s\n",
      "epoch 290| loss: 0.27631 | val_0_auc: 0.90637 |  0:03:31s\n",
      "epoch 291| loss: 0.27476 | val_0_auc: 0.90666 |  0:03:31s\n",
      "epoch 292| loss: 0.27591 | val_0_auc: 0.9067  |  0:03:32s\n",
      "epoch 293| loss: 0.27556 | val_0_auc: 0.90632 |  0:03:33s\n",
      "epoch 294| loss: 0.27523 | val_0_auc: 0.90655 |  0:03:33s\n",
      "epoch 295| loss: 0.27273 | val_0_auc: 0.90665 |  0:03:34s\n",
      "epoch 296| loss: 0.2741  | val_0_auc: 0.9072  |  0:03:35s\n",
      "epoch 297| loss: 0.27449 | val_0_auc: 0.90718 |  0:03:36s\n",
      "epoch 298| loss: 0.2757  | val_0_auc: 0.90688 |  0:03:36s\n",
      "epoch 299| loss: 0.27564 | val_0_auc: 0.90698 |  0:03:37s\n",
      "epoch 300| loss: 0.27046 | val_0_auc: 0.90685 |  0:03:38s\n",
      "epoch 301| loss: 0.27538 | val_0_auc: 0.90717 |  0:03:39s\n",
      "epoch 302| loss: 0.27359 | val_0_auc: 0.90727 |  0:03:39s\n",
      "epoch 303| loss: 0.274   | val_0_auc: 0.90801 |  0:03:40s\n",
      "epoch 304| loss: 0.27314 | val_0_auc: 0.90736 |  0:03:41s\n",
      "epoch 305| loss: 0.27517 | val_0_auc: 0.90748 |  0:03:41s\n",
      "epoch 306| loss: 0.27488 | val_0_auc: 0.90736 |  0:03:42s\n",
      "epoch 307| loss: 0.27354 | val_0_auc: 0.90749 |  0:03:43s\n",
      "epoch 308| loss: 0.27461 | val_0_auc: 0.90727 |  0:03:44s\n",
      "epoch 309| loss: 0.27467 | val_0_auc: 0.90717 |  0:03:44s\n",
      "epoch 310| loss: 0.27386 | val_0_auc: 0.90716 |  0:03:45s\n",
      "epoch 311| loss: 0.27308 | val_0_auc: 0.90825 |  0:03:46s\n",
      "epoch 312| loss: 0.27306 | val_0_auc: 0.90798 |  0:03:47s\n",
      "epoch 313| loss: 0.27234 | val_0_auc: 0.90767 |  0:03:47s\n",
      "epoch 314| loss: 0.27278 | val_0_auc: 0.90755 |  0:03:48s\n",
      "epoch 315| loss: 0.27484 | val_0_auc: 0.90711 |  0:03:49s\n",
      "epoch 316| loss: 0.27222 | val_0_auc: 0.90708 |  0:03:49s\n",
      "epoch 317| loss: 0.27357 | val_0_auc: 0.90729 |  0:03:50s\n",
      "epoch 318| loss: 0.27415 | val_0_auc: 0.90775 |  0:03:51s\n",
      "epoch 319| loss: 0.27352 | val_0_auc: 0.90821 |  0:03:52s\n",
      "epoch 320| loss: 0.27469 | val_0_auc: 0.90794 |  0:03:52s\n",
      "epoch 321| loss: 0.27286 | val_0_auc: 0.90787 |  0:03:53s\n",
      "epoch 322| loss: 0.2726  | val_0_auc: 0.90766 |  0:03:54s\n",
      "epoch 323| loss: 0.27161 | val_0_auc: 0.90763 |  0:03:54s\n",
      "epoch 324| loss: 0.2721  | val_0_auc: 0.90782 |  0:03:55s\n",
      "epoch 325| loss: 0.27382 | val_0_auc: 0.90803 |  0:03:56s\n",
      "epoch 326| loss: 0.26956 | val_0_auc: 0.90791 |  0:03:57s\n",
      "epoch 327| loss: 0.27055 | val_0_auc: 0.90853 |  0:03:57s\n",
      "epoch 328| loss: 0.27291 | val_0_auc: 0.90791 |  0:03:58s\n",
      "epoch 329| loss: 0.27154 | val_0_auc: 0.90847 |  0:03:59s\n",
      "epoch 330| loss: 0.27337 | val_0_auc: 0.90821 |  0:03:59s\n",
      "epoch 331| loss: 0.2715  | val_0_auc: 0.90798 |  0:04:00s\n",
      "epoch 332| loss: 0.27286 | val_0_auc: 0.90827 |  0:04:01s\n",
      "epoch 333| loss: 0.27079 | val_0_auc: 0.90857 |  0:04:02s\n",
      "epoch 334| loss: 0.27212 | val_0_auc: 0.90849 |  0:04:02s\n",
      "epoch 335| loss: 0.27366 | val_0_auc: 0.90832 |  0:04:03s\n",
      "epoch 336| loss: 0.27134 | val_0_auc: 0.90814 |  0:04:04s\n",
      "epoch 337| loss: 0.2711  | val_0_auc: 0.90836 |  0:04:04s\n",
      "epoch 338| loss: 0.27184 | val_0_auc: 0.90857 |  0:04:05s\n",
      "epoch 339| loss: 0.2722  | val_0_auc: 0.90834 |  0:04:06s\n",
      "epoch 340| loss: 0.27218 | val_0_auc: 0.90854 |  0:04:07s\n",
      "epoch 341| loss: 0.26972 | val_0_auc: 0.90829 |  0:04:07s\n",
      "epoch 342| loss: 0.26959 | val_0_auc: 0.90847 |  0:04:08s\n",
      "epoch 343| loss: 0.27059 | val_0_auc: 0.90918 |  0:04:09s\n",
      "epoch 344| loss: 0.26954 | val_0_auc: 0.90857 |  0:04:09s\n",
      "epoch 345| loss: 0.27004 | val_0_auc: 0.90837 |  0:04:10s\n",
      "epoch 346| loss: 0.27111 | val_0_auc: 0.90884 |  0:04:11s\n",
      "epoch 347| loss: 0.27203 | val_0_auc: 0.90857 |  0:04:11s\n",
      "epoch 348| loss: 0.27139 | val_0_auc: 0.90877 |  0:04:12s\n",
      "epoch 349| loss: 0.2697  | val_0_auc: 0.90884 |  0:04:13s\n",
      "epoch 350| loss: 0.27067 | val_0_auc: 0.90839 |  0:04:14s\n",
      "epoch 351| loss: 0.26977 | val_0_auc: 0.90866 |  0:04:14s\n",
      "epoch 352| loss: 0.26866 | val_0_auc: 0.90889 |  0:04:15s\n",
      "epoch 353| loss: 0.26791 | val_0_auc: 0.90928 |  0:04:16s\n",
      "epoch 354| loss: 0.27041 | val_0_auc: 0.90863 |  0:04:16s\n",
      "epoch 355| loss: 0.27232 | val_0_auc: 0.90919 |  0:04:17s\n",
      "epoch 356| loss: 0.27305 | val_0_auc: 0.90877 |  0:04:18s\n",
      "epoch 357| loss: 0.26982 | val_0_auc: 0.90848 |  0:04:19s\n",
      "epoch 358| loss: 0.26945 | val_0_auc: 0.90855 |  0:04:19s\n",
      "epoch 359| loss: 0.26903 | val_0_auc: 0.90843 |  0:04:20s\n",
      "epoch 360| loss: 0.27014 | val_0_auc: 0.90843 |  0:04:21s\n",
      "epoch 361| loss: 0.2692  | val_0_auc: 0.90871 |  0:04:21s\n",
      "epoch 362| loss: 0.26995 | val_0_auc: 0.90846 |  0:04:22s\n",
      "epoch 363| loss: 0.26971 | val_0_auc: 0.90865 |  0:04:23s\n",
      "epoch 364| loss: 0.27125 | val_0_auc: 0.90878 |  0:04:24s\n",
      "epoch 365| loss: 0.26949 | val_0_auc: 0.90917 |  0:04:24s\n",
      "epoch 366| loss: 0.27035 | val_0_auc: 0.9085  |  0:04:25s\n",
      "epoch 367| loss: 0.26968 | val_0_auc: 0.90956 |  0:04:26s\n",
      "epoch 368| loss: 0.26994 | val_0_auc: 0.90973 |  0:04:26s\n",
      "epoch 369| loss: 0.26963 | val_0_auc: 0.90947 |  0:04:27s\n",
      "epoch 370| loss: 0.26702 | val_0_auc: 0.9093  |  0:04:28s\n",
      "epoch 371| loss: 0.2691  | val_0_auc: 0.90947 |  0:04:29s\n",
      "epoch 372| loss: 0.26962 | val_0_auc: 0.909   |  0:04:29s\n",
      "epoch 373| loss: 0.27047 | val_0_auc: 0.90924 |  0:04:30s\n",
      "epoch 374| loss: 0.26749 | val_0_auc: 0.90967 |  0:04:31s\n",
      "epoch 375| loss: 0.2683  | val_0_auc: 0.90986 |  0:04:32s\n",
      "epoch 376| loss: 0.26857 | val_0_auc: 0.909   |  0:04:32s\n",
      "epoch 377| loss: 0.26749 | val_0_auc: 0.90905 |  0:04:33s\n",
      "epoch 378| loss: 0.26883 | val_0_auc: 0.90909 |  0:04:34s\n",
      "epoch 379| loss: 0.26766 | val_0_auc: 0.90908 |  0:04:34s\n",
      "epoch 380| loss: 0.26952 | val_0_auc: 0.90955 |  0:04:35s\n",
      "epoch 381| loss: 0.2679  | val_0_auc: 0.90926 |  0:04:36s\n",
      "epoch 382| loss: 0.26793 | val_0_auc: 0.90938 |  0:04:37s\n",
      "epoch 383| loss: 0.26883 | val_0_auc: 0.91016 |  0:04:37s\n",
      "epoch 384| loss: 0.26822 | val_0_auc: 0.90919 |  0:04:38s\n",
      "epoch 385| loss: 0.26979 | val_0_auc: 0.90898 |  0:04:39s\n",
      "epoch 386| loss: 0.26741 | val_0_auc: 0.90954 |  0:04:39s\n",
      "epoch 387| loss: 0.26898 | val_0_auc: 0.9095  |  0:04:40s\n",
      "epoch 388| loss: 0.26714 | val_0_auc: 0.90979 |  0:04:41s\n",
      "epoch 389| loss: 0.26937 | val_0_auc: 0.9094  |  0:04:42s\n",
      "epoch 390| loss: 0.26651 | val_0_auc: 0.90942 |  0:04:42s\n",
      "epoch 391| loss: 0.2687  | val_0_auc: 0.90979 |  0:04:43s\n",
      "epoch 392| loss: 0.26679 | val_0_auc: 0.90989 |  0:04:44s\n",
      "epoch 393| loss: 0.26724 | val_0_auc: 0.90954 |  0:04:45s\n",
      "epoch 394| loss: 0.26742 | val_0_auc: 0.90942 |  0:04:45s\n",
      "epoch 395| loss: 0.26831 | val_0_auc: 0.9093  |  0:04:46s\n",
      "epoch 396| loss: 0.26724 | val_0_auc: 0.90922 |  0:04:47s\n",
      "epoch 397| loss: 0.26732 | val_0_auc: 0.90927 |  0:04:47s\n",
      "epoch 398| loss: 0.2674  | val_0_auc: 0.90917 |  0:04:48s\n",
      "epoch 399| loss: 0.26718 | val_0_auc: 0.90905 |  0:04:49s\n",
      "epoch 400| loss: 0.26718 | val_0_auc: 0.90936 |  0:04:49s\n",
      "epoch 401| loss: 0.26782 | val_0_auc: 0.9093  |  0:04:50s\n",
      "epoch 402| loss: 0.2664  | val_0_auc: 0.90949 |  0:04:51s\n",
      "epoch 403| loss: 0.26907 | val_0_auc: 0.90937 |  0:04:52s\n",
      "\n",
      "Early stopping occurred at epoch 403 with best_epoch = 383 and best_val_0_auc = 0.91016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "8 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"<ipython-input-29-dd7f35387bf5>\", line 8, in fit\n",
      "    return super().fit(\n",
      "  File \"/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\", line 223, in fit\n",
      "    self._set_network()\n",
      "  File \"/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\", line 570, in _set_network\n",
      "    self.network = tab_network.TabNet(\n",
      "  File \"/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py\", line 564, in __init__\n",
      "    raise ValueError(\"n_shared and n_independent can't be both zero.\")\n",
      "ValueError: n_shared and n_independent can't be both zero.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.89917433 0.90783035 0.91997184 0.91249073 0.90628205 0.92346047\n",
      " 0.72547509        nan 0.89878105 0.66342981 0.90829189 0.9168045\n",
      " 0.9116346  0.90920524 0.8998921  0.90850191 0.86477793 0.9029923\n",
      " 0.90429066 0.91977068 0.90494352 0.86249335 0.91934268 0.91438452\n",
      " 0.91551709 0.90177486 0.92055592 0.90173772 0.9124608  0.91322537\n",
      "        nan 0.87581546 0.9193797  0.90998405 0.83272924 0.89136072\n",
      " 0.89997224 0.88791777 0.91929582 0.90551276        nan 0.92036236\n",
      " 0.91537178 0.91708344 0.53489817 0.90399938 0.75466432 0.92079574\n",
      " 0.92363971 0.91732176 0.7399425  0.89879004 0.89913068 0.91082186\n",
      " 0.85069939 0.85314004 0.92091644 0.91912425 0.90742283 0.91323882\n",
      " 0.72318408 0.89697902 0.81434632 0.91343277 0.7994429  0.87757917\n",
      " 0.91115574 0.90633325 0.80375798 0.90602293        nan 0.91085068\n",
      " 0.8894301  0.88082738        nan 0.91402209 0.91863581 0.91118698\n",
      " 0.91686344 0.92141427        nan 0.81863784 0.90337101        nan\n",
      " 0.88969421 0.91156523        nan 0.92049035 0.90622088 0.91541615]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'verbose': 1,\n 'optimizer_params': {'lr': 0.02},\n 'n_steps': 8,\n 'n_shared': 2,\n 'n_independent': 5,\n 'n_a': 3,\n 'momentum': 0.1,\n 'lambda_sparse': 0.01,\n 'gamma': 0.5,\n 'clip_value': 1,\n 'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'mean_fit_time': array([4.08061368e+01, 1.85347041e+02, 5.74600625e+01, 1.08301079e+02,\n        2.51711654e+02, 2.32989799e+02, 5.21421972e+02, 2.84569263e-02,\n        6.26589324e+02, 6.11330811e+02, 4.41757250e+01, 1.12380654e+02,\n        2.24576004e+02, 5.22203013e+02, 1.21972117e+02, 4.06275187e+01,\n        3.96675159e+02, 6.02129016e+01, 2.42230824e+02, 7.27539158e+01,\n        1.33457010e+02, 1.24621843e+03, 1.85400095e+02, 5.61502819e+02,\n        5.57822475e+01, 1.18428343e+03, 9.87606510e+02, 4.01647903e+02,\n        1.14188959e+03, 5.24649794e+01, 2.75726318e-02, 4.84483572e+02,\n        1.62669059e+02, 6.93285742e+01, 3.46412943e+02, 3.69374297e+02,\n        8.03735483e+01, 8.09059507e+02, 5.10247952e+02, 7.30384941e+01,\n        2.68254280e-02, 1.87851882e+02, 4.45718486e+01, 4.26955628e+01,\n        1.39901430e+02, 3.72311754e+02, 4.55973170e+02, 1.98191815e+02,\n        3.63457852e+02, 7.83383770e+02, 3.11688058e+02, 4.05035396e+02,\n        3.91424727e+01, 5.72492851e+02, 5.16334677e+02, 2.35991440e+02,\n        3.40880412e+02, 2.57350253e+02, 5.98351341e+02, 3.24200817e+02,\n        7.28636259e+02, 3.44757751e+02, 2.33956869e+02, 5.18612545e+02,\n        1.74820002e+02, 4.77423132e+02, 1.31205453e+02, 5.30780983e+01,\n        5.09071715e+02, 3.70976714e+02, 2.61247158e-02, 6.49918637e+01,\n        2.72094726e+02, 2.79595156e+02, 2.58674622e-02, 4.73850313e+02,\n        1.81008251e+02, 4.66372225e+01, 5.21388032e+01, 5.86037887e+02,\n        2.59335041e-02, 1.21284614e+03, 4.45603368e+01, 2.77142525e-02,\n        4.67860957e+02, 2.76795756e+02, 2.60155201e-02, 7.81765330e+02,\n        1.58462449e+02, 2.92394124e+02]),\n 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]),\n 'mean_score_time': array([0.09297633, 0.20074677, 0.1132288 , 0.32483339, 0.24613047,\n        0.1140306 , 0.49730206, 0.        , 0.14744663, 0.7531476 ,\n        0.11899137, 0.21072698, 0.43400407, 0.61550903, 0.09259367,\n        0.07618189, 0.20920181, 0.08794332, 0.16566634, 0.08098602,\n        0.3991828 , 0.27718472, 0.6078372 , 0.60385704, 0.18420768,\n        0.27788329, 0.89350152, 0.21459413, 0.24753642, 0.15729809,\n        0.        , 0.22938228, 0.28988099, 0.08841801, 0.36681843,\n        0.20204186, 0.14597607, 0.32068491, 0.29802346, 0.06181717,\n        0.        , 0.17357469, 0.13848042, 0.08752418, 0.77762413,\n        0.09257746, 0.50896955, 0.28946519, 0.83753133, 0.46996784,\n        0.64134002, 0.23699999, 0.09314942, 0.35210109, 0.74006295,\n        0.27781248, 0.59133935, 0.15470171, 0.55101967, 0.48295808,\n        0.74124026, 0.57994699, 0.18966269, 0.28346157, 0.15026283,\n        0.48076034, 0.32535148, 0.16124964, 0.45495439, 0.29326606,\n        0.        , 0.12371159, 0.11335015, 0.20774889, 0.        ,\n        0.35211134, 0.23079777, 0.09038997, 0.16023874, 0.6150198 ,\n        0.        , 0.86720872, 0.10537744, 0.        , 0.30183458,\n        0.21634531, 0.        , 0.4353776 , 0.21499777, 0.07357407]),\n 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]),\n 'param_verbose': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_optimizer_params': masked_array(data=[{'lr': 0.01}, {'lr': 0.02}, {'lr': 0.02}, {'lr': 0.02},\n                    {'lr': 0.01}, {'lr': 0.01}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.01}, {'lr': 0.02},\n                    {'lr': 0.001}, {'lr': 0.001}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.01}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.02}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.01}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.01}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.01}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.01}, {'lr': 0.01},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.02},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.001},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.01},\n                    {'lr': 0.001}, {'lr': 0.001}, {'lr': 0.01},\n                    {'lr': 0.01}, {'lr': 0.001}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.001},\n                    {'lr': 0.001}, {'lr': 0.02}, {'lr': 0.01},\n                    {'lr': 0.01}, {'lr': 0.01}, {'lr': 0.02}, {'lr': 0.01},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.01},\n                    {'lr': 0.02}, {'lr': 0.001}, {'lr': 0.01},\n                    {'lr': 0.01}, {'lr': 0.01}, {'lr': 0.01},\n                    {'lr': 0.001}],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_n_steps': masked_array(data=[1, 5, 1, 3, 3, 3, 8, 5, 3, 8, 1, 5, 3, 8, 1, 1, 5, 1,\n                    1, 1, 8, 8, 5, 5, 1, 3, 8, 3, 3, 1, 8, 5, 5, 1, 8, 3,\n                    1, 8, 5, 1, 1, 3, 1, 1, 8, 3, 8, 5, 8, 5, 5, 3, 1, 3,\n                    8, 5, 5, 5, 5, 5, 8, 8, 8, 5, 8, 5, 5, 1, 5, 5, 3, 1,\n                    1, 5, 3, 8, 3, 1, 1, 5, 8, 8, 1, 1, 3, 3, 8, 8, 3, 1],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_n_shared': masked_array(data=[1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 1, 2,\n                    1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2,\n                    1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 2, 0, 2,\n                    0, 2, 1, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2,\n                    2, 2, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 1, 2, 0],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_n_independent': masked_array(data=[0, 2, 5, 0, 2, 2, 0, 0, 2, 0, 1, 0, 2, 5, 1, 2, 1, 1,\n                    5, 2, 5, 5, 1, 5, 5, 5, 0, 2, 5, 5, 0, 1, 0, 1, 0, 2,\n                    0, 5, 5, 1, 0, 1, 2, 2, 5, 2, 5, 1, 5, 2, 0, 1, 2, 5,\n                    1, 0, 5, 1, 5, 1, 0, 5, 1, 5, 0, 1, 5, 2, 5, 5, 0, 5,\n                    5, 2, 0, 2, 5, 2, 5, 5, 0, 5, 2, 0, 0, 5, 0, 1, 2, 1],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_n_a': masked_array(data=[21, 8, 3, 21, 13, 8, 5, 13, 5, 5, 21, 8, 3, 13, 8, 8,\n                    21, 3, 5, 13, 13, 5, 21, 5, 8, 8, 3, 8, 3, 3, 13, 21,\n                    3, 3, 5, 21, 13, 8, 8, 8, 13, 5, 13, 5, 8, 3, 3, 8, 3,\n                    13, 13, 13, 21, 8, 21, 13, 8, 5, 21, 21, 8, 5, 5, 21,\n                    8, 21, 13, 21, 3, 5, 21, 5, 13, 21, 5, 13, 3, 3, 8, 21,\n                    13, 3, 13, 21, 5, 13, 5, 3, 5, 21],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_momentum': masked_array(data=[0.02, 0.1, 0.005, 0.005, 0.005, 0.05, 0.05, 0.005,\n                    0.02, 0.02, 0.005, 0.1, 0.1, 0.005, 0.1, 0.005, 0.02,\n                    0.05, 0.005, 0.005, 0.1, 0.02, 0.02, 0.02, 0.1, 0.1,\n                    0.05, 0.02, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.1,\n                    0.02, 0.1, 0.02, 0.05, 0.005, 0.02, 0.02, 0.02, 0.1,\n                    0.05, 0.05, 0.05, 0.005, 0.1, 0.05, 0.05, 0.05, 0.1,\n                    0.05, 0.05, 0.05, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02,\n                    0.1, 0.1, 0.1, 0.005, 0.1, 0.02, 0.1, 0.02, 0.005,\n                    0.005, 0.005, 0.1, 0.02, 0.05, 0.02, 0.005, 0.05, 0.02,\n                    0.1, 0.05, 0.1, 0.1, 0.02, 0.005, 0.05, 0.1, 0.005,\n                    0.1],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_lambda_sparse': masked_array(data=[0.001, 0.1, 0.001, 0.01, 0.1, 0.01, 0.1, 0.1, 0.001,\n                    0.1, 0.001, 0.01, 0.1, 0.1, 0.01, 0.01, 0.01, 0.1, 0.1,\n                    0.001, 0.001, 0.1, 0.01, 0.01, 0.01, 0.001, 0.01, 0.01,\n                    0.001, 0.1, 0.001, 0.01, 0.001, 0.1, 0.01, 0.001,\n                    0.001, 0.1, 0.01, 0.1, 0.01, 0.001, 0.01, 0.001, 0.01,\n                    0.1, 0.001, 0.001, 0.01, 0.01, 0.1, 0.01, 0.1, 0.001,\n                    0.001, 0.001, 0.1, 0.001, 0.001, 0.1, 0.01, 0.1, 0.01,\n                    0.1, 0.1, 0.001, 0.001, 0.01, 0.001, 0.001, 0.1, 0.1,\n                    0.1, 0.01, 0.01, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.1,\n                    0.1, 0.01, 0.001, 0.1, 0.1, 0.001, 0.1, 0.01],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_gamma': masked_array(data=[1.3, 3, 1.3, 1.3, 3, 3, 3, 3, 1.3, 3, 0.5, 0.5, 1.3,\n                    0.5, 0.5, 3, 1.3, 0.5, 0.5, 1.3, 0.5, 3, 0.5, 1.3, 1.3,\n                    3, 3, 0.5, 1.3, 0.5, 0.5, 1.3, 1.3, 3, 1.3, 0.5, 0.5,\n                    0.5, 3, 0.5, 3, 3, 0.5, 0.5, 3, 0.5, 3, 1.3, 0.5, 1.3,\n                    3, 0.5, 0.5, 0.5, 1.3, 1.3, 3, 3, 0.5, 3, 3, 3, 1.3, 3,\n                    3, 1.3, 0.5, 0.5, 1.3, 0.5, 1.3, 3, 3, 0.5, 1.3, 1.3,\n                    3, 3, 1.3, 3, 3, 1.3, 1.3, 3, 3, 0.5, 1.3, 1.3, 0.5,\n                    1.3],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_clip_value': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'param_cat_emb_dim': masked_array(data=[list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n                    list([50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]),\n                    list([5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]),\n                    list([10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]),\n                    list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.005,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 21,\n   'momentum': 0.005,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 8,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 2,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.005,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 3,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 8,\n   'momentum': 0.005,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 1,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 13,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 2,\n   'n_a': 8,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 3,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 3,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 8,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 5,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 1,\n   'n_a': 8,\n   'momentum': 0.005,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 1,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 5,\n   'momentum': 0.05,\n   'lambda_sparse': 0.001,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 0,\n   'n_a': 8,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 5,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 8,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.005,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 13,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 21,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 5,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 0,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.02,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 2,\n   'n_a': 3,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 1,\n   'n_independent': 5,\n   'n_a': 8,\n   'momentum': 0.05,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 5,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 21,\n   'momentum': 0.02,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 13,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 8,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 3,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 2,\n   'n_a': 13,\n   'momentum': 0.1,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.02},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 3,\n   'n_shared': 1,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.02,\n   'lambda_sparse': 0.001,\n   'gamma': 3,\n   'clip_value': 1,\n   'cat_emb_dim': [50, 21, 46, 3, 8, 47, 8, 37, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 5,\n   'n_a': 13,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 8,\n   'n_shared': 0,\n   'n_independent': 0,\n   'n_a': 5,\n   'momentum': 0.05,\n   'lambda_sparse': 0.1,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [10, 10, 10, 3, 8, 10, 8, 10, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 8,\n   'n_shared': 1,\n   'n_independent': 1,\n   'n_a': 3,\n   'momentum': 0.1,\n   'lambda_sparse': 0.001,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.01},\n   'n_steps': 3,\n   'n_shared': 2,\n   'n_independent': 2,\n   'n_a': 5,\n   'momentum': 0.005,\n   'lambda_sparse': 0.1,\n   'gamma': 0.5,\n   'clip_value': 1,\n   'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]},\n  {'verbose': 1,\n   'optimizer_params': {'lr': 0.001},\n   'n_steps': 1,\n   'n_shared': 0,\n   'n_independent': 1,\n   'n_a': 21,\n   'momentum': 0.1,\n   'lambda_sparse': 0.01,\n   'gamma': 1.3,\n   'clip_value': 1,\n   'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}],\n 'split0_test_score': array([0.89917433, 0.90783035, 0.91997184, 0.91249073, 0.90628205,\n        0.92346047, 0.72547509,        nan, 0.89878105, 0.66342981,\n        0.90829189, 0.9168045 , 0.9116346 , 0.90920524, 0.8998921 ,\n        0.90850191, 0.86477793, 0.9029923 , 0.90429066, 0.91977068,\n        0.90494352, 0.86249335, 0.91934268, 0.91438452, 0.91551709,\n        0.90177486, 0.92055592, 0.90173772, 0.9124608 , 0.91322537,\n               nan, 0.87581546, 0.9193797 , 0.90998405, 0.83272924,\n        0.89136072, 0.89997224, 0.88791777, 0.91929582, 0.90551276,\n               nan, 0.92036236, 0.91537178, 0.91708344, 0.53489817,\n        0.90399938, 0.75466432, 0.92079574, 0.92363971, 0.91732176,\n        0.7399425 , 0.89879004, 0.89913068, 0.91082186, 0.85069939,\n        0.85314004, 0.92091644, 0.91912425, 0.90742283, 0.91323882,\n        0.72318408, 0.89697902, 0.81434632, 0.91343277, 0.7994429 ,\n        0.87757917, 0.91115574, 0.90633325, 0.80375798, 0.90602293,\n               nan, 0.91085068, 0.8894301 , 0.88082738,        nan,\n        0.91402209, 0.91863581, 0.91118698, 0.91686344, 0.92141427,\n               nan, 0.81863784, 0.90337101,        nan, 0.88969421,\n        0.91156523,        nan, 0.92049035, 0.90622088, 0.91541615]),\n 'mean_test_score': array([0.89917433, 0.90783035, 0.91997184, 0.91249073, 0.90628205,\n        0.92346047, 0.72547509,        nan, 0.89878105, 0.66342981,\n        0.90829189, 0.9168045 , 0.9116346 , 0.90920524, 0.8998921 ,\n        0.90850191, 0.86477793, 0.9029923 , 0.90429066, 0.91977068,\n        0.90494352, 0.86249335, 0.91934268, 0.91438452, 0.91551709,\n        0.90177486, 0.92055592, 0.90173772, 0.9124608 , 0.91322537,\n               nan, 0.87581546, 0.9193797 , 0.90998405, 0.83272924,\n        0.89136072, 0.89997224, 0.88791777, 0.91929582, 0.90551276,\n               nan, 0.92036236, 0.91537178, 0.91708344, 0.53489817,\n        0.90399938, 0.75466432, 0.92079574, 0.92363971, 0.91732176,\n        0.7399425 , 0.89879004, 0.89913068, 0.91082186, 0.85069939,\n        0.85314004, 0.92091644, 0.91912425, 0.90742283, 0.91323882,\n        0.72318408, 0.89697902, 0.81434632, 0.91343277, 0.7994429 ,\n        0.87757917, 0.91115574, 0.90633325, 0.80375798, 0.90602293,\n               nan, 0.91085068, 0.8894301 , 0.88082738,        nan,\n        0.91402209, 0.91863581, 0.91118698, 0.91686344, 0.92141427,\n               nan, 0.81863784, 0.90337101,        nan, 0.88969421,\n        0.91156523,        nan, 0.92049035, 0.90622088, 0.91541615]),\n 'std_test_score': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., nan,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0., nan,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0., nan,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n         0.,  0.,  0.,  0.,  0., nan,  0.,  0.,  0., nan,  0.,  0.,  0.,\n         0.,  0., nan,  0.,  0., nan,  0.,  0., nan,  0.,  0.,  0.]),\n 'rank_test_score': array([56, 40,  9, 28, 43,  2, 79, 86, 59, 81, 39, 19, 30, 37, 55, 38, 68,\n        51, 48, 10, 47, 69, 12, 23, 20, 52,  6, 53, 29, 27, 83, 67, 11, 36,\n        72, 61, 54, 64, 13, 46, 88,  8, 22, 17, 82, 49, 77,  5,  1, 16, 78,\n        58, 57, 35, 71, 70,  4, 14, 41, 26, 80, 60, 74, 25, 76, 66, 33, 42,\n        75, 45, 89, 34, 63, 65, 85, 24, 15, 32, 18,  3, 90, 73, 50, 84, 62,\n        31, 87,  7, 44, 21], dtype=int32)}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'verbose': 1,\n 'optimizer_params': {'lr': 0.02},\n 'n_steps': 8,\n 'n_shared': 2,\n 'n_independent': 5,\n 'n_a': 3,\n 'momentum': 0.1,\n 'lambda_sparse': 0.01,\n 'gamma': 0.5,\n 'clip_value': 1,\n 'cat_emb_dim': [20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4]}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After searching for best params, we need to retrain a model, we chose a make that on 5 folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60107 | val_0_auc: 0.77976 |  0:00:07s\n",
      "epoch 1  | loss: 0.43793 | val_0_auc: 0.84023 |  0:00:14s\n",
      "epoch 2  | loss: 0.4     | val_0_auc: 0.85954 |  0:00:20s\n",
      "epoch 3  | loss: 0.37987 | val_0_auc: 0.87103 |  0:00:26s\n",
      "epoch 4  | loss: 0.36643 | val_0_auc: 0.88439 |  0:00:33s\n",
      "epoch 5  | loss: 0.34755 | val_0_auc: 0.89597 |  0:00:41s\n",
      "epoch 6  | loss: 0.33228 | val_0_auc: 0.89988 |  0:00:47s\n",
      "epoch 7  | loss: 0.32411 | val_0_auc: 0.90841 |  0:00:53s\n",
      "epoch 8  | loss: 0.31601 | val_0_auc: 0.91378 |  0:01:00s\n",
      "epoch 9  | loss: 0.30695 | val_0_auc: 0.91423 |  0:01:06s\n",
      "epoch 10 | loss: 0.3037  | val_0_auc: 0.91635 |  0:01:12s\n",
      "epoch 11 | loss: 0.30079 | val_0_auc: 0.92064 |  0:01:18s\n",
      "epoch 12 | loss: 0.29784 | val_0_auc: 0.91981 |  0:01:25s\n",
      "epoch 13 | loss: 0.29549 | val_0_auc: 0.92228 |  0:01:31s\n",
      "epoch 14 | loss: 0.29373 | val_0_auc: 0.92194 |  0:01:38s\n",
      "epoch 15 | loss: 0.28967 | val_0_auc: 0.9221  |  0:01:44s\n",
      "epoch 16 | loss: 0.2872  | val_0_auc: 0.92047 |  0:01:50s\n",
      "epoch 17 | loss: 0.28905 | val_0_auc: 0.919   |  0:01:56s\n",
      "epoch 18 | loss: 0.28594 | val_0_auc: 0.92061 |  0:02:02s\n",
      "epoch 19 | loss: 0.28315 | val_0_auc: 0.92083 |  0:02:09s\n",
      "epoch 20 | loss: 0.28117 | val_0_auc: 0.92015 |  0:02:15s\n",
      "epoch 21 | loss: 0.27847 | val_0_auc: 0.92106 |  0:02:21s\n",
      "epoch 22 | loss: 0.27698 | val_0_auc: 0.92215 |  0:02:27s\n",
      "epoch 23 | loss: 0.27591 | val_0_auc: 0.92095 |  0:02:34s\n",
      "epoch 24 | loss: 0.27517 | val_0_auc: 0.91881 |  0:02:40s\n",
      "epoch 25 | loss: 0.27138 | val_0_auc: 0.92014 |  0:02:46s\n",
      "epoch 26 | loss: 0.26972 | val_0_auc: 0.91827 |  0:02:52s\n",
      "epoch 27 | loss: 0.27211 | val_0_auc: 0.91883 |  0:02:58s\n",
      "epoch 28 | loss: 0.27012 | val_0_auc: 0.91879 |  0:03:04s\n",
      "epoch 29 | loss: 0.26824 | val_0_auc: 0.92001 |  0:03:10s\n",
      "epoch 30 | loss: 0.26526 | val_0_auc: 0.91696 |  0:03:16s\n",
      "epoch 31 | loss: 0.2627  | val_0_auc: 0.9182  |  0:03:22s\n",
      "epoch 32 | loss: 0.2622  | val_0_auc: 0.91414 |  0:03:29s\n",
      "epoch 33 | loss: 0.26228 | val_0_auc: 0.91707 |  0:03:35s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_0_auc = 0.92228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60315 | val_0_auc: 0.79925 |  0:00:05s\n",
      "epoch 1  | loss: 0.45187 | val_0_auc: 0.85005 |  0:00:13s\n",
      "epoch 2  | loss: 0.41627 | val_0_auc: 0.87023 |  0:00:19s\n",
      "epoch 3  | loss: 0.38748 | val_0_auc: 0.88243 |  0:00:26s\n",
      "epoch 4  | loss: 0.37249 | val_0_auc: 0.88876 |  0:00:32s\n",
      "epoch 5  | loss: 0.3645  | val_0_auc: 0.90044 |  0:00:38s\n",
      "epoch 6  | loss: 0.34853 | val_0_auc: 0.90784 |  0:00:45s\n",
      "epoch 7  | loss: 0.33398 | val_0_auc: 0.91265 |  0:00:51s\n",
      "epoch 8  | loss: 0.32491 | val_0_auc: 0.91579 |  0:00:58s\n",
      "epoch 9  | loss: 0.31792 | val_0_auc: 0.9202  |  0:01:03s\n",
      "epoch 10 | loss: 0.31434 | val_0_auc: 0.9227  |  0:01:10s\n",
      "epoch 11 | loss: 0.30767 | val_0_auc: 0.92377 |  0:01:16s\n",
      "epoch 12 | loss: 0.30246 | val_0_auc: 0.92575 |  0:01:22s\n",
      "epoch 13 | loss: 0.30031 | val_0_auc: 0.92541 |  0:01:28s\n",
      "epoch 14 | loss: 0.29791 | val_0_auc: 0.92627 |  0:01:34s\n",
      "epoch 15 | loss: 0.29608 | val_0_auc: 0.92776 |  0:01:40s\n",
      "epoch 16 | loss: 0.29192 | val_0_auc: 0.92696 |  0:01:46s\n",
      "epoch 17 | loss: 0.29145 | val_0_auc: 0.92634 |  0:01:53s\n",
      "epoch 18 | loss: 0.28941 | val_0_auc: 0.92798 |  0:01:59s\n",
      "epoch 19 | loss: 0.28654 | val_0_auc: 0.92784 |  0:02:05s\n",
      "epoch 20 | loss: 0.28682 | val_0_auc: 0.92692 |  0:02:11s\n",
      "epoch 21 | loss: 0.28814 | val_0_auc: 0.92756 |  0:02:17s\n",
      "epoch 22 | loss: 0.28574 | val_0_auc: 0.92875 |  0:02:23s\n",
      "epoch 23 | loss: 0.28485 | val_0_auc: 0.92882 |  0:02:30s\n",
      "epoch 24 | loss: 0.28381 | val_0_auc: 0.92681 |  0:02:38s\n",
      "epoch 25 | loss: 0.28259 | val_0_auc: 0.92757 |  0:02:44s\n",
      "epoch 26 | loss: 0.27984 | val_0_auc: 0.92827 |  0:02:50s\n",
      "epoch 27 | loss: 0.27791 | val_0_auc: 0.92819 |  0:02:57s\n",
      "epoch 28 | loss: 0.27554 | val_0_auc: 0.92658 |  0:03:03s\n",
      "epoch 29 | loss: 0.27641 | val_0_auc: 0.92607 |  0:03:09s\n",
      "epoch 30 | loss: 0.27612 | val_0_auc: 0.92601 |  0:03:15s\n",
      "epoch 31 | loss: 0.27319 | val_0_auc: 0.92526 |  0:03:21s\n",
      "epoch 32 | loss: 0.272   | val_0_auc: 0.92558 |  0:03:27s\n",
      "epoch 33 | loss: 0.27341 | val_0_auc: 0.92597 |  0:03:33s\n",
      "epoch 34 | loss: 0.27002 | val_0_auc: 0.92455 |  0:03:39s\n",
      "epoch 35 | loss: 0.26854 | val_0_auc: 0.92499 |  0:03:44s\n",
      "epoch 36 | loss: 0.26875 | val_0_auc: 0.92467 |  0:03:50s\n",
      "epoch 37 | loss: 0.26937 | val_0_auc: 0.92452 |  0:03:56s\n",
      "epoch 38 | loss: 0.26451 | val_0_auc: 0.92233 |  0:04:02s\n",
      "epoch 39 | loss: 0.26614 | val_0_auc: 0.92157 |  0:04:08s\n",
      "epoch 40 | loss: 0.26452 | val_0_auc: 0.92231 |  0:04:14s\n",
      "epoch 41 | loss: 0.26003 | val_0_auc: 0.92217 |  0:04:19s\n",
      "epoch 42 | loss: 0.26245 | val_0_auc: 0.92179 |  0:04:25s\n",
      "epoch 43 | loss: 0.2605  | val_0_auc: 0.92065 |  0:04:32s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 23 and best_val_0_auc = 0.92882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60328 | val_0_auc: 0.80316 |  0:00:06s\n",
      "epoch 1  | loss: 0.44383 | val_0_auc: 0.82537 |  0:00:19s\n",
      "epoch 2  | loss: 0.41207 | val_0_auc: 0.86845 |  0:00:27s\n",
      "epoch 3  | loss: 0.39141 | val_0_auc: 0.88581 |  0:00:35s\n",
      "epoch 4  | loss: 0.36904 | val_0_auc: 0.8881  |  0:00:44s\n",
      "epoch 5  | loss: 0.36055 | val_0_auc: 0.89573 |  0:00:52s\n",
      "epoch 6  | loss: 0.34529 | val_0_auc: 0.90571 |  0:01:00s\n",
      "epoch 7  | loss: 0.33158 | val_0_auc: 0.91145 |  0:01:07s\n",
      "epoch 8  | loss: 0.32529 | val_0_auc: 0.91583 |  0:01:14s\n",
      "epoch 9  | loss: 0.31954 | val_0_auc: 0.91802 |  0:01:21s\n",
      "epoch 10 | loss: 0.3159  | val_0_auc: 0.92045 |  0:01:29s\n",
      "epoch 11 | loss: 0.30887 | val_0_auc: 0.9237  |  0:01:35s\n",
      "epoch 12 | loss: 0.30555 | val_0_auc: 0.92711 |  0:01:44s\n",
      "epoch 13 | loss: 0.29961 | val_0_auc: 0.92757 |  0:01:50s\n",
      "epoch 14 | loss: 0.29604 | val_0_auc: 0.92824 |  0:01:56s\n",
      "epoch 15 | loss: 0.29761 | val_0_auc: 0.92722 |  0:02:02s\n",
      "epoch 16 | loss: 0.29354 | val_0_auc: 0.92751 |  0:02:08s\n",
      "epoch 17 | loss: 0.29145 | val_0_auc: 0.92809 |  0:02:14s\n",
      "epoch 18 | loss: 0.28857 | val_0_auc: 0.92729 |  0:02:21s\n",
      "epoch 19 | loss: 0.28755 | val_0_auc: 0.92686 |  0:02:29s\n",
      "epoch 20 | loss: 0.2866  | val_0_auc: 0.92711 |  0:02:37s\n",
      "epoch 21 | loss: 0.28516 | val_0_auc: 0.92479 |  0:02:44s\n",
      "epoch 22 | loss: 0.28372 | val_0_auc: 0.92556 |  0:02:54s\n",
      "epoch 23 | loss: 0.28369 | val_0_auc: 0.92595 |  0:03:02s\n",
      "epoch 24 | loss: 0.28262 | val_0_auc: 0.92545 |  0:03:11s\n",
      "epoch 25 | loss: 0.28018 | val_0_auc: 0.92375 |  0:03:17s\n",
      "epoch 26 | loss: 0.27887 | val_0_auc: 0.92558 |  0:03:25s\n",
      "epoch 27 | loss: 0.27484 | val_0_auc: 0.9221  |  0:03:32s\n",
      "epoch 28 | loss: 0.27221 | val_0_auc: 0.92245 |  0:03:38s\n",
      "epoch 29 | loss: 0.2747  | val_0_auc: 0.92203 |  0:03:44s\n",
      "epoch 30 | loss: 0.27356 | val_0_auc: 0.92123 |  0:03:52s\n",
      "epoch 31 | loss: 0.2724  | val_0_auc: 0.92273 |  0:04:00s\n",
      "epoch 32 | loss: 0.27128 | val_0_auc: 0.92009 |  0:04:08s\n",
      "epoch 33 | loss: 0.27168 | val_0_auc: 0.92061 |  0:04:18s\n",
      "epoch 34 | loss: 0.27201 | val_0_auc: 0.91878 |  0:04:27s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_auc = 0.92824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.58411 | val_0_auc: 0.80499 |  0:00:08s\n",
      "epoch 1  | loss: 0.4322  | val_0_auc: 0.84588 |  0:00:14s\n",
      "epoch 2  | loss: 0.40246 | val_0_auc: 0.86718 |  0:00:21s\n",
      "epoch 3  | loss: 0.38725 | val_0_auc: 0.87852 |  0:00:28s\n",
      "epoch 4  | loss: 0.36543 | val_0_auc: 0.89157 |  0:00:35s\n",
      "epoch 5  | loss: 0.34746 | val_0_auc: 0.90021 |  0:00:42s\n",
      "epoch 6  | loss: 0.33701 | val_0_auc: 0.90422 |  0:00:49s\n",
      "epoch 7  | loss: 0.32389 | val_0_auc: 0.91324 |  0:00:55s\n",
      "epoch 8  | loss: 0.31817 | val_0_auc: 0.91593 |  0:01:01s\n",
      "epoch 9  | loss: 0.31084 | val_0_auc: 0.91876 |  0:01:08s\n",
      "epoch 10 | loss: 0.30809 | val_0_auc: 0.92087 |  0:01:16s\n",
      "epoch 11 | loss: 0.30871 | val_0_auc: 0.92232 |  0:01:22s\n",
      "epoch 12 | loss: 0.30047 | val_0_auc: 0.92257 |  0:01:29s\n",
      "epoch 13 | loss: 0.29914 | val_0_auc: 0.92217 |  0:01:36s\n",
      "epoch 14 | loss: 0.29511 | val_0_auc: 0.92442 |  0:01:46s\n",
      "epoch 15 | loss: 0.29253 | val_0_auc: 0.92438 |  0:01:55s\n",
      "epoch 16 | loss: 0.29044 | val_0_auc: 0.92412 |  0:02:01s\n",
      "epoch 17 | loss: 0.2911  | val_0_auc: 0.92482 |  0:02:08s\n",
      "epoch 18 | loss: 0.28794 | val_0_auc: 0.92513 |  0:02:14s\n",
      "epoch 19 | loss: 0.28566 | val_0_auc: 0.92642 |  0:02:20s\n",
      "epoch 20 | loss: 0.28539 | val_0_auc: 0.92433 |  0:02:27s\n",
      "epoch 21 | loss: 0.28158 | val_0_auc: 0.92412 |  0:02:34s\n",
      "epoch 22 | loss: 0.28403 | val_0_auc: 0.9242  |  0:02:40s\n",
      "epoch 23 | loss: 0.28413 | val_0_auc: 0.92105 |  0:02:46s\n",
      "epoch 24 | loss: 0.2845  | val_0_auc: 0.92313 |  0:02:52s\n",
      "epoch 25 | loss: 0.28125 | val_0_auc: 0.92274 |  0:02:58s\n",
      "epoch 26 | loss: 0.27809 | val_0_auc: 0.92389 |  0:03:04s\n",
      "epoch 27 | loss: 0.27992 | val_0_auc: 0.92231 |  0:03:10s\n",
      "epoch 28 | loss: 0.27604 | val_0_auc: 0.92028 |  0:03:16s\n",
      "epoch 29 | loss: 0.27437 | val_0_auc: 0.92228 |  0:03:22s\n",
      "epoch 30 | loss: 0.27303 | val_0_auc: 0.91949 |  0:03:28s\n",
      "epoch 31 | loss: 0.27317 | val_0_auc: 0.92071 |  0:03:34s\n",
      "epoch 32 | loss: 0.27383 | val_0_auc: 0.91997 |  0:03:40s\n",
      "epoch 33 | loss: 0.27162 | val_0_auc: 0.92108 |  0:03:46s\n",
      "epoch 34 | loss: 0.26925 | val_0_auc: 0.91804 |  0:03:53s\n",
      "epoch 35 | loss: 0.26867 | val_0_auc: 0.92072 |  0:03:59s\n",
      "epoch 36 | loss: 0.26829 | val_0_auc: 0.91706 |  0:04:05s\n",
      "epoch 37 | loss: 0.27024 | val_0_auc: 0.91865 |  0:04:11s\n",
      "epoch 38 | loss: 0.26976 | val_0_auc: 0.91845 |  0:04:17s\n",
      "epoch 39 | loss: 0.27201 | val_0_auc: 0.92105 |  0:04:23s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_auc = 0.92642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60508 | val_0_auc: 0.83027 |  0:00:06s\n",
      "epoch 1  | loss: 0.45802 | val_0_auc: 0.86351 |  0:00:12s\n",
      "epoch 2  | loss: 0.42288 | val_0_auc: 0.87235 |  0:00:18s\n",
      "epoch 3  | loss: 0.39977 | val_0_auc: 0.88249 |  0:00:24s\n",
      "epoch 4  | loss: 0.39076 | val_0_auc: 0.88858 |  0:00:30s\n",
      "epoch 5  | loss: 0.3688  | val_0_auc: 0.89615 |  0:00:37s\n",
      "epoch 6  | loss: 0.34539 | val_0_auc: 0.90579 |  0:00:43s\n",
      "epoch 7  | loss: 0.32624 | val_0_auc: 0.91331 |  0:00:49s\n",
      "epoch 8  | loss: 0.31671 | val_0_auc: 0.91834 |  0:00:55s\n",
      "epoch 9  | loss: 0.30941 | val_0_auc: 0.91996 |  0:01:02s\n",
      "epoch 10 | loss: 0.30416 | val_0_auc: 0.92375 |  0:01:08s\n",
      "epoch 11 | loss: 0.30084 | val_0_auc: 0.92666 |  0:01:18s\n",
      "epoch 12 | loss: 0.29745 | val_0_auc: 0.92759 |  0:01:25s\n",
      "epoch 13 | loss: 0.29197 | val_0_auc: 0.92792 |  0:01:33s\n",
      "epoch 14 | loss: 0.29118 | val_0_auc: 0.92816 |  0:01:42s\n",
      "epoch 15 | loss: 0.28835 | val_0_auc: 0.92909 |  0:01:49s\n",
      "epoch 16 | loss: 0.28657 | val_0_auc: 0.92831 |  0:01:55s\n",
      "epoch 17 | loss: 0.28431 | val_0_auc: 0.92732 |  0:02:01s\n",
      "epoch 18 | loss: 0.28349 | val_0_auc: 0.92676 |  0:02:07s\n",
      "epoch 19 | loss: 0.28223 | val_0_auc: 0.92782 |  0:02:14s\n",
      "epoch 20 | loss: 0.28152 | val_0_auc: 0.92687 |  0:02:20s\n",
      "epoch 21 | loss: 0.28093 | val_0_auc: 0.92792 |  0:02:26s\n",
      "epoch 22 | loss: 0.27834 | val_0_auc: 0.9285  |  0:02:32s\n",
      "epoch 23 | loss: 0.27815 | val_0_auc: 0.92756 |  0:02:39s\n",
      "epoch 24 | loss: 0.27383 | val_0_auc: 0.92764 |  0:02:45s\n",
      "epoch 25 | loss: 0.27109 | val_0_auc: 0.92804 |  0:02:51s\n",
      "epoch 26 | loss: 0.27253 | val_0_auc: 0.9278  |  0:02:57s\n",
      "epoch 27 | loss: 0.27067 | val_0_auc: 0.92507 |  0:03:04s\n",
      "epoch 28 | loss: 0.27051 | val_0_auc: 0.92551 |  0:03:10s\n",
      "epoch 29 | loss: 0.26825 | val_0_auc: 0.9262  |  0:03:16s\n",
      "epoch 30 | loss: 0.26646 | val_0_auc: 0.92389 |  0:03:22s\n",
      "epoch 31 | loss: 0.26467 | val_0_auc: 0.92372 |  0:03:29s\n",
      "epoch 32 | loss: 0.26372 | val_0_auc: 0.92067 |  0:03:36s\n",
      "epoch 33 | loss: 0.26203 | val_0_auc: 0.92022 |  0:03:42s\n",
      "epoch 34 | loss: 0.26008 | val_0_auc: 0.92116 |  0:03:49s\n",
      "epoch 35 | loss: 0.26111 | val_0_auc: 0.91943 |  0:03:55s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_auc = 0.92909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/Mestrado_Computacao_Aplicada_UFMS/1_semestre/Inteligencia_Artificial/algoritmos-ia/venv/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    clf = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, device_name='cpu', **search.best_params_)\n",
    "    clf.fit(\n",
    "        X[train_index],\n",
    "        y[train_index],\n",
    "        patience=20,\n",
    "        # X_valid=X[valid_index],\n",
    "        # y_valid=y[valid_index],\n",
    "        eval_set=[(X[valid_index], y[valid_index])],\n",
    "    )\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[TabNetClassifier(n_d=8, n_a=3, n_steps=8, gamma=0.5, cat_idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13], cat_dims=[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8], cat_emb_dim=[20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4], n_independent=5, n_shared=2, epsilon=1e-15, momentum=0.1, lambda_sparse=0.01, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=14, output_dim=2, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1),\n TabNetClassifier(n_d=8, n_a=3, n_steps=8, gamma=0.5, cat_idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13], cat_dims=[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8], cat_emb_dim=[20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4], n_independent=5, n_shared=2, epsilon=1e-15, momentum=0.1, lambda_sparse=0.01, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=14, output_dim=2, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1),\n TabNetClassifier(n_d=8, n_a=3, n_steps=8, gamma=0.5, cat_idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13], cat_dims=[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8], cat_emb_dim=[20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4], n_independent=5, n_shared=2, epsilon=1e-15, momentum=0.1, lambda_sparse=0.01, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=14, output_dim=2, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1),\n TabNetClassifier(n_d=8, n_a=3, n_steps=8, gamma=0.5, cat_idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13], cat_dims=[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8], cat_emb_dim=[20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4], n_independent=5, n_shared=2, epsilon=1e-15, momentum=0.1, lambda_sparse=0.01, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=14, output_dim=2, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1),\n TabNetClassifier(n_d=8, n_a=3, n_steps=8, gamma=0.5, cat_idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13], cat_dims=[120, 43, 93, 7, 17, 95, 17, 74, 3, 10, 6, 16, 8], cat_emb_dim=[20, 20, 20, 3, 8, 20, 8, 20, 1, 5, 3, 8, 4], n_independent=5, n_shared=2, epsilon=1e-15, momentum=0.1, lambda_sparse=0.01, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.02}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=14, output_dim=2, device_name='cpu', n_shared_decoder=1, n_indep_decoder=1)]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros(shape=y_test.shape)\n",
    "for model in models:\n",
    "    preds += clf.predict_proba(X_test)[:, 1]\n",
    "preds = preds / len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST SCORE FOR census-income : 0.9244184959613279\n"
     ]
    }
   ],
   "source": [
    "test_auc = roc_auc_score(y_score=preds, y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
